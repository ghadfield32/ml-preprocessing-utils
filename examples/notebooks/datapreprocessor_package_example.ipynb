{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "        \n",
    "# main.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "# from data_preprocessor import DataPreprocessor\n",
    "# from clustering_module import ClusteringModule  # Ensure this is implemented\n",
    "# from feature_manager import FeatureManager  # Ensure this is implemented\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and parse the YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the preprocessor_config.yaml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed configuration.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def construct_filepath(output_dir: str, identifier: str, dataset_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Utility function to construct file paths for saving models and preprocessors.\n",
    "\n",
    "    Args:\n",
    "        identifier (str): Identifier for the file (e.g., 'trained_model', 'preprocessor').\n",
    "        dataset_key (str): Key representing the dataset type.\n",
    "\n",
    "    Returns:\n",
    "        str: Constructed file path.\n",
    "    \"\"\"\n",
    "    return os.path.join(output_dir, f\"{dataset_key}_{identifier}.pkl\")\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = '../../dataset/test/preprocessor_config/preprocessor_config.yaml'\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "        logger_config = config.get('logging', {})\n",
    "        logger_level = logger_config.get('level', 'INFO').upper()\n",
    "        logger_format = logger_config.get('format', '%(asctime)s [%(levelname)s] %(message)s')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Configure Logging\n",
    "    # ----------------------------\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else getattr(logging, logger_level, logging.INFO),\n",
    "        format=logger_format,\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Extract Feature Assets\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Extract Execution Parameters\n",
    "    # ----------------------------\n",
    "    execution = config.get('execution', {})\n",
    "    shared_execution = execution.get('shared', {})\n",
    "    mode_execution = execution.get('train', {})  # Default to train mode\n",
    "    current_mode = mode_execution.get('mode', 'train').lower()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Get List of Model Types\n",
    "    # ----------------------------\n",
    "    model_types = config.get('model_types', ['Tree Based Classifier'])  # Default to one model if not specified\n",
    "\n",
    "    for current_model_type in model_types:\n",
    "        logger.info(f\"---\\nProcessing Model: {current_model_type}\\n---\")\n",
    "\n",
    "        # Step 6: Extract Mode for the Current Model\n",
    "        model_config = config.get('models', {}).get(current_model_type, {})\n",
    "        if not model_config:\n",
    "            logger.error(f\"No configuration found for model '{current_model_type}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Determine mode based on the model type\n",
    "        # For example, if the model is a clustering model, set mode to 'clustering'\n",
    "        if current_model_type in ['K-Means', 'Hierarchical Clustering', 'DBSCAN', 'KModes', 'KPrototypes']:\n",
    "            current_mode = 'clustering'\n",
    "        elif current_model_type in ['Logistic Regression', 'Tree Based Classifier', 'Support Vector Machine']:\n",
    "            current_mode = 'train'\n",
    "        elif current_model_type in ['Linear Regression', 'Tree Based Regressor']:\n",
    "            current_mode = 'train'\n",
    "        else:\n",
    "            current_mode = 'train'  # Default to train\n",
    "\n",
    "        # ----------------------------\n",
    "        # Step 7: Handle Modes for Each Model\n",
    "        # ----------------------------\n",
    "        if current_mode == 'train':\n",
    "            # Adjust output directories to prevent overwriting\n",
    "            execution_train = execution.get('train', {})\n",
    "            train_mode = 'train'\n",
    "\n",
    "            train_input_path = execution_train.get('input_path', '')\n",
    "            base_output_dir = execution_train.get('output_dir', './processed_data')\n",
    "            model_output_dir = os.path.join(base_output_dir, current_model_type.replace(\" \", \"_\"))\n",
    "            transformers_dir = execution_train.get('save_transformers_path', './transformers')  # Changed: Remove model name\n",
    "            normalize_debug = execution_train.get('normalize_debug', False)\n",
    "            normalize_graphs_output = execution_train.get('normalize_graphs_output', False)\n",
    "\n",
    "            # Validate essential paths\n",
    "            if not train_input_path:\n",
    "                logger.error(\"❌ 'input_path' for training mode is not specified in the configuration.\")\n",
    "                continue\n",
    "            if not os.path.exists(train_input_path):\n",
    "                logger.error(f\"❌ Training input dataset not found at {train_input_path}.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=current_model_type,\n",
    "                y_variable=column_assets.get('y_variable', []),\n",
    "                ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "                nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "                numericals=column_assets.get('numericals', []), \n",
    "                mode=train_mode,\n",
    "                options=model_config,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=shared_execution.get('plot_output_dir', './plots'),\n",
    "                transformers_dir=transformers_dir  # Now a directory\n",
    "            )\n",
    "\n",
    "            # Initialize FeatureManager\n",
    "            save_path = config.get('execution', {}).get('shared', {}).get('features_metadata_path', '../../dataset/test/features_info/features_metadata.pkl')\n",
    "            feature_manager = FeatureManager(save_path=save_path)  # Ensure FeatureManager is correctly implemented\n",
    "\n",
    "            # Load Training Dataset via FeatureManager\n",
    "            try:\n",
    "                filtered_df, column_assets = feature_manager.load_features_and_dataset(\n",
    "                    debug=True  # Set to False to reduce verbosity\n",
    "                )\n",
    "                logger.info(\"✅ Features loaded and dataset filtered successfully.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to load features and dataset: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Execute Preprocessing\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(filtered_df)\n",
    "                logger.info(\"✅ Preprocessing completed successfully in train mode.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Preprocessing failed in train mode: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Save Preprocessed Data\n",
    "            try:\n",
    "                os.makedirs(model_output_dir, exist_ok=True)\n",
    "                X_train.to_csv(os.path.join(model_output_dir, 'X_train.csv'), index=False)\n",
    "                y_train.to_csv(os.path.join(model_output_dir, 'y_train.csv'), index=False)\n",
    "                X_test.to_csv(os.path.join(model_output_dir, 'X_test.csv'), index=False)\n",
    "                y_test.to_csv(os.path.join(model_output_dir, 'y_test.csv'), index=False)\n",
    "                recommendations.to_csv(os.path.join(model_output_dir, 'preprocessing_recommendations.csv'), index=False)\n",
    "                logger.info(f\"✅ Preprocessed data saved to '{model_output_dir}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to save preprocessed data: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Optional: Visualize Inverse Transformations\n",
    "            try:\n",
    "                if X_test_inverse is not None:\n",
    "                    print(f\"Inverse Transformed Test Data for {current_model_type}:\")\n",
    "                    print(X_test_inverse.head())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Error during visualization: {e}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"✅ All preprocessing tasks completed successfully for model '{current_model_type}'.\")\n",
    "\n",
    "        elif current_mode == 'predict':\n",
    "            # Adjust paths accordingly\n",
    "            execution_predict = execution.get('predict', {})\n",
    "            predict_mode = 'predict'\n",
    "\n",
    "            predict_input_path = execution_predict.get('prediction_input_path', '')\n",
    "            predictions_output_path = execution_predict.get('predictions_output_path', './predictions')\n",
    "            transformers_dir = execution_predict.get('load_transformers_path', './transformers')  # Correct directory\n",
    "            trained_model_path = execution_predict.get('trained_model_path', './models/trained_model.pkl')  # Path to load model\n",
    "            normalize_debug = execution_predict.get('normalize_debug', False)\n",
    "            normalize_graphs_output = execution_predict.get('normalize_graphs_output', False)\n",
    "\n",
    "            # Validate essential paths\n",
    "            if not predict_input_path:\n",
    "                logger.error(\"❌ 'prediction_input_path' for predict mode is not specified in the configuration.\")\n",
    "                continue\n",
    "            if not os.path.exists(predict_input_path):\n",
    "                logger.error(f\"❌ Prediction input dataset not found at {predict_input_path}.\")\n",
    "                continue\n",
    "            if not os.path.exists(trained_model_path):\n",
    "                logger.error(f\"❌ Trained model not found at {trained_model_path}.\")\n",
    "                continue\n",
    "            if not os.path.exists(transformers_dir):\n",
    "                logger.error(f\"❌ Transformers directory not found at {transformers_dir}.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=current_model_type,\n",
    "                y_variable=column_assets.get('y_variable', []),\n",
    "                ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "                nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "                numericals=column_assets.get('numericals', []),\n",
    "                mode=predict_mode,\n",
    "                options=model_config,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=shared_execution.get('plot_output_dir', './plots'),\n",
    "                transformers_dir=transformers_dir  # Directory path\n",
    "            )\n",
    "\n",
    "            # Load Prediction Dataset\n",
    "            try:\n",
    "                df_predict = load_dataset(predict_input_path)\n",
    "                logger.info(f\"✅ Prediction input data loaded from '{predict_input_path}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Execute Preprocessing for Prediction\n",
    "            try:\n",
    "                X_preprocessed, recommendations, X_inversed = preprocessor.preprocess_predict(X=df_predict)\n",
    "                logger.info(\"✅ Preprocessing completed successfully in predict mode.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Preprocessing failed in predict mode: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Load Trained Model\n",
    "            # try:\n",
    "            #     trained_model = joblib.load(trained_model_path)\n",
    "            #     logger.info(f\"✅ Trained model loaded from '{trained_model_path}'.\")\n",
    "            # except Exception as e:\n",
    "            #     logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "            #     continue\n",
    "\n",
    "            # # Make Predictions\n",
    "            # try:\n",
    "            #     # Ensure X_preprocessed is a DataFrame with appropriate feature names\n",
    "            #     if isinstance(X_preprocessed, np.ndarray):\n",
    "            #         X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=preprocessor.final_feature_order)\n",
    "            #     else:\n",
    "            #         X_preprocessed_df = X_preprocessed\n",
    "\n",
    "            #     # Make predictions\n",
    "            #     predictions = trained_model.predict(X_preprocessed_df)\n",
    "            #     logger.info(\"✅ Predictions made successfully.\")\n",
    "            # except Exception as e:\n",
    "            #     logger.error(f\"❌ Prediction failed: {e}\")\n",
    "            #     continue\n",
    "            y_new_pred = np.random.choice(['1', '0'], size=X_inversed.shape[0])  # Example for binary predictions\n",
    "            \n",
    "            # Attach Predictions to Inversed Data\n",
    "            if X_inversed is not None:\n",
    "                # Ensure predictions length matches the number of rows in X_inversed\n",
    "                if len(y_new_pred) == len(X_inversed):\n",
    "                    # Add predictions column\n",
    "                    X_inversed['predictions'] = y_new_pred\n",
    "                    logger.info(\"✅ Predictions attached to inversed data successfully.\")\n",
    "\n",
    "                    # Debugging Output AFTER attaching predictions\n",
    "                    print(f\"\\nUpdated INVERSED DATA with Predictions for {current_model_type}:\")\n",
    "                    print(X_inversed.head())  # Shows predictions column included\n",
    "                else:\n",
    "                    logger.error(\"❌ Predictions length does not match inversed data length.\")\n",
    "                    continue\n",
    "            else:\n",
    "                logger.error(\"❌ Inversed data is None. Cannot attach predictions.\")\n",
    "                continue\n",
    "\n",
    "            # Save Predictions\n",
    "            try:\n",
    "                os.makedirs(predictions_output_path, exist_ok=True)\n",
    "                predictions_filename = os.path.join(predictions_output_path, f'predictions_{current_model_type.replace(\" \", \"_\")}.csv')\n",
    "                if X_inversed is not None:\n",
    "                    X_inversed.to_csv(predictions_filename, index=False)\n",
    "                else:\n",
    "                    logger.error(\"❌ Inversed data is None. Predictions not saved.\")\n",
    "                    continue\n",
    "                logger.info(f\"✅ Predictions saved to '{predictions_filename}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"✅ All prediction tasks completed successfully for model '{current_model_type}'.\")\n",
    "\n",
    "        elif current_mode == 'clustering':\n",
    "            # Adjust paths accordingly\n",
    "            execution_clustering = execution.get('clustering', {})\n",
    "            clustering_mode = 'clustering'\n",
    "\n",
    "            clustering_input_path = execution_clustering.get('clustering_input_path', '')\n",
    "            clustering_output_dir = execution_clustering.get('clustering_output_dir', './clustering_output')\n",
    "            normalize_debug = execution_clustering.get('normalize_debug', False)\n",
    "            normalize_graphs_output = execution_clustering.get('normalize_graphs_output', False)\n",
    "\n",
    "            # Validate essential paths\n",
    "            if not clustering_input_path:\n",
    "                logger.error(\"❌ 'clustering_input_path' for clustering mode is not specified in the configuration.\")\n",
    "                continue\n",
    "            if not os.path.exists(clustering_input_path):\n",
    "                logger.error(f\"❌ Clustering input dataset not found at {clustering_input_path}.\")\n",
    "                continue\n",
    "\n",
    "            # Initialize DataPreprocessor\n",
    "            preprocessor = DataPreprocessor(\n",
    "                model_type=current_model_type,\n",
    "                y_variable=column_assets.get('y_variable', []),\n",
    "                ordinal_categoricals=column_assets.get('ordinal_categoricals', []),\n",
    "                nominal_categoricals=column_assets.get('nominal_categoricals', []),\n",
    "                numericals=column_assets.get('numericals', []),\n",
    "                mode=clustering_mode,\n",
    "                options=model_config,\n",
    "                debug=debug_flag,\n",
    "                normalize_debug=normalize_debug,\n",
    "                normalize_graphs_output=normalize_graphs_output,\n",
    "                graphs_output_dir=shared_execution.get('plot_output_dir', './plots'),\n",
    "                transformers_dir=execution_clustering.get('save_transformers_path', './transformers')  # Changed: Remove model name\n",
    "            )\n",
    "\n",
    "            # Load Clustering Dataset\n",
    "            try:\n",
    "                df_clustering = load_dataset(clustering_input_path)\n",
    "                logger.info(f\"✅ Clustering input data loaded from '{clustering_input_path}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to load clustering input data: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Execute Preprocessing for Clustering\n",
    "            try:\n",
    "                X_processed, recommendations = preprocessor.final_preprocessing(df_clustering)\n",
    "                logger.info(\"✅ Preprocessing completed successfully in clustering mode.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Preprocessing failed in clustering mode: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Initialize and Train Clustering Model\n",
    "            try:\n",
    "                # Load clustering model parameters from config\n",
    "                clustering_model_config = model_config.get('clustering_model_params', {})\n",
    "\n",
    "                clustering_module = ClusteringModule(\n",
    "                    model_type=current_model_type,\n",
    "                    model_params=clustering_model_config,\n",
    "                    debug=debug_flag\n",
    "                )\n",
    "\n",
    "                clustering_module.fit(X_processed)\n",
    "                clustering_module.evaluate(X_processed)\n",
    "                # Plot clusters if applicable\n",
    "                clustering_module.plot_clusters(X_processed, clustering_output_dir)\n",
    "                # Save the clustering model\n",
    "                os.makedirs(clustering_output_dir, exist_ok=True)\n",
    "                clustering_model_path = os.path.join(clustering_output_dir, f\"{current_model_type.replace(' ', '_')}_model.pkl\")\n",
    "                clustering_module.save_model(clustering_model_path)\n",
    "                logger.info(f\"✅ Clustering model saved to '{clustering_model_path}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Clustering tasks failed: {e}\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"✅ All clustering tasks completed successfully for model '{current_model_type}'.\")\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"❌ Unsupported mode '{current_mode}'. Supported modes are 'train', 'predict', and 'clustering'.\")\n",
    "            continue\n",
    "\n",
    "    logger.info(\"✅ All model processing completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
