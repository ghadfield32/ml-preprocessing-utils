{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# data_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # Added for saving/loading transformers\n",
    "from inspect import signature  # Needed for parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        column_assets: Dict[str, List[str]],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        perform_split: bool = True,\n",
    "        debug: bool = False,\n",
    "        debug_split_dataset: bool = False,\n",
    "        debug_handle_missing_values: bool = False,\n",
    "        debug_test_normality: bool = False,\n",
    "        debug_handle_outliers: bool = False,\n",
    "        debug_choose_transformations: bool = False,\n",
    "        debug_encode_categoricals: bool = False,\n",
    "        debug_apply_scaling: bool = False,\n",
    "        debug_implement_smote: bool = False,\n",
    "        debug_final_inverse_transformations: bool = False,\n",
    "        debug_validate_inverse_transformations: bool = False,\n",
    "        debug_generate_recommendations: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with model type, column assets, and user-defined options.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model (e.g., 'Logistic Regression').\n",
    "            column_assets (Dict[str, List[str]]): Dictionary containing lists of columns for different categories.\n",
    "            mode (str): Operational mode ('train', 'predict', 'clustering').\n",
    "            options (Optional[Dict]): User-defined options for preprocessing steps.\n",
    "            perform_split (bool): Whether to perform train-test split (True for training).\n",
    "            debug (bool): General debug flag to control overall verbosity.\n",
    "            debug_split_dataset (bool): Debug flag for dataset splitting.\n",
    "            debug_handle_missing_values (bool): Debug flag for missing value handling.\n",
    "            debug_test_normality (bool): Debug flag for normality testing.\n",
    "            debug_handle_outliers (bool): Debug flag for outlier handling.\n",
    "            debug_choose_transformations (bool): Debug flag for choosing transformations.\n",
    "            debug_encode_categoricals (bool): Debug flag for encoding categorical variables.\n",
    "            debug_apply_scaling (bool): Debug flag for feature scaling.\n",
    "            debug_implement_smote (bool): Debug flag for SMOTE implementation.\n",
    "            debug_final_inverse_transformations (bool): Debug flag for inverse transformations.\n",
    "            debug_validate_inverse_transformations (bool): Debug flag for validating inverse transformations.\n",
    "            debug_generate_recommendations (bool): Debug flag for generating preprocessing recommendations.\n",
    "            normalize_debug (bool): Flag to display normalization plots.\n",
    "            normalize_graphs_output (bool): Flag to save normalization plots.\n",
    "            graphs_output_dir (str): Directory to save plots.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.column_assets = column_assets\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.perform_split = perform_split\n",
    "        self.debug = debug\n",
    "        self.debug_split_dataset = debug_split_dataset\n",
    "        self.debug_handle_missing_values = debug_handle_missing_values\n",
    "        self.debug_test_normality = debug_test_normality\n",
    "        self.debug_handle_outliers = debug_handle_outliers\n",
    "        self.debug_choose_transformations = debug_choose_transformations\n",
    "        self.debug_encode_categoricals = debug_encode_categoricals\n",
    "        self.debug_apply_scaling = debug_apply_scaling\n",
    "        self.debug_implement_smote = debug_implement_smote\n",
    "        self.debug_final_inverse_transformations = debug_final_inverse_transformations\n",
    "        self.debug_validate_inverse_transformations = debug_validate_inverse_transformations\n",
    "        self.debug_generate_recommendations = debug_generate_recommendations\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "\n",
    "        # Define model categories for accurate processing\n",
    "        self.model_category = self.map_model_type_to_category()\n",
    "        \n",
    "        self.final_feature_order = []  # Initialize an empty list to store feature order\n",
    "\n",
    "        # Set y_variable based on model category and mode\n",
    "        if self.mode == 'train':\n",
    "            if self.model_category in ['classification', 'regression']:\n",
    "                self.y_variable = column_assets.get('y_variable', [])\n",
    "                if not self.y_variable:\n",
    "                    self.logger.error(\"No target variable specified in 'y_variable'.\")\n",
    "                    raise ValueError(\"Target variable 'y_variable' must be specified for supervised models.\")\n",
    "            else:\n",
    "                self.y_variable = []\n",
    "        elif self.mode == 'predict':\n",
    "            if self.model_category in ['classification', 'regression']:\n",
    "                self.y_variable = column_assets.get('y_variable', [])\n",
    "            else:\n",
    "                self.y_variable = []\n",
    "        elif self.mode == 'clustering':\n",
    "            # For clustering, no target variable\n",
    "            self.y_variable = []\n",
    "\n",
    "        # Fetch feature lists\n",
    "        self.ordinal_categoricals = column_assets.get('ordinal_categoricals', [])\n",
    "        self.nominal_categoricals = column_assets.get('nominal_categoricals', [])\n",
    "        self.numericals = column_assets.get('numericals', [])\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.extreme_imbalance_threshold = self.options.get('smote_recommendation', {}).get('extreme_imbalance_threshold', 0.05)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def _log(self, message: str, debug_flag: bool, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Helper method to handle logging based on debug flags.\n",
    "\n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            debug_flag (bool): The debug flag for the specific section.\n",
    "            level (str): The logging level ('info' or 'debug').\n",
    "        \"\"\"\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "        else:\n",
    "            if level != 'debug':\n",
    "                self.logger.info(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_models = [\n",
    "            'Logistic Regression',\n",
    "            'Tree Based Classifier',\n",
    "            'k-NN Classifier',\n",
    "            'SVM Classifier',\n",
    "            'Neural Network Classifier'\n",
    "        ]\n",
    "\n",
    "        regression_models = [\n",
    "            'Linear Regression',\n",
    "            'Tree Based Regressor',\n",
    "            'k-NN Regressor',\n",
    "            'SVM Regressor',\n",
    "            'Neural Network Regressor'\n",
    "        ]\n",
    "\n",
    "        clustering_models = [\n",
    "            'K-Means Clustering', 'Hierarchical Clustering', 'DBSCAN', 'KModes', 'KPrototypes'\n",
    "        ]\n",
    "\n",
    "        time_series_models = [\n",
    "            # Add any time series models if applicable\n",
    "        ]\n",
    "\n",
    "        if self.model_type in classification_models:\n",
    "            return 'classification'\n",
    "        elif self.model_type in regression_models:\n",
    "            return 'regression'\n",
    "        elif self.model_type in clustering_models:\n",
    "            return 'clustering'\n",
    "        elif self.model_type in time_series_models:\n",
    "            return 'time_series'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "\n",
    "    def split_dataset(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        step_name = \"Split Dataset into Train and Test\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", self.debug_split_dataset, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", self.debug_split_dataset, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", self.debug_split_dataset, 'debug')\n",
    "\n",
    "        if self.perform_split and self.mode == 'train':\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    stratify=stratify, \n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                if self.debug_split_dataset:\n",
    "                    self._log(\"Performed stratified split for classification.\", self.debug_split_dataset, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                if self.debug_split_dataset:\n",
    "                    self._log(\"Performed random split for regression.\", self.debug_split_dataset, 'debug')\n",
    "            else:\n",
    "                stratify = self.options.get('split_dataset', {}).get('stratify', None)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42),\n",
    "                    stratify=stratify\n",
    "                )\n",
    "                self.logger.warning(\"Model category not recognized for specific split strategy. Performed default random split.\")\n",
    "        elif self.mode == 'clustering':\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = None\n",
    "            y_test = None\n",
    "            self.logger.info(\"No splitting performed for clustering models.\")\n",
    "        else:\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.debug_split_dataset:\n",
    "            self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", self.debug_split_dataset, 'debug')\n",
    "            if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "                self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "                self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "            elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "                self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "                self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"Handle Missing Values\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'mean'  # For clustering, mean imputation is acceptable\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            if self.debug_handle_missing_values:\n",
    "                self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", self.debug_handle_missing_values, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            if self.debug_handle_missing_values:\n",
    "                self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", self.debug_handle_missing_values, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.debug_handle_missing_values:\n",
    "            self.logger.debug(f\"Completed: {step_name}. Dataset shape after imputation: {X_train.shape}\")\n",
    "            self.logger.debug(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\")\n",
    "            self.logger.debug(f\"New columns handled: {new_columns}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            self.logger.debug(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\")\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.debug_test_normality:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", self.debug_test_normality, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "    def plot_qq(\n",
    "        self, \n",
    "        X_original: pd.DataFrame, \n",
    "        X_transformed: pd.DataFrame, \n",
    "        numerical_features: List[str], \n",
    "        model_type: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plot QQ plots before and after normalization for specified numerical features.\n",
    "\n",
    "        Args:\n",
    "            X_original (pd.DataFrame): Original numerical features before normalization.\n",
    "            X_transformed (pd.DataFrame): Transformed numerical features after normalization.\n",
    "            numerical_features (List[str]): List of numerical feature names.\n",
    "            model_type (str): Type of the machine learning model.\n",
    "        \"\"\"\n",
    "        for feature in numerical_features:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            # QQ Plot for Original Distribution\n",
    "            plt.subplot(1, 2, 1)\n",
    "            probplot(X_original[feature], dist=\"norm\", plot=plt)\n",
    "            plt.title(f'Original QQ Plot of {feature}')\n",
    "\n",
    "            # QQ Plot for Transformed Distribution\n",
    "            plt.subplot(1, 2, 2)\n",
    "            probplot(X_transformed[feature], dist=\"norm\", plot=plt)\n",
    "            plt.title(f'Transformed QQ Plot of {feature}')\n",
    "\n",
    "            plt.suptitle(f'QQ Plot Normalization Check for {feature} ({model_type})', fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "            # Display plot in Jupyter if normalize_debug is True\n",
    "            if self.normalize_debug:\n",
    "                plt.show()\n",
    "\n",
    "            # Save the plot if normalize_graphs_output is True\n",
    "            if self.normalize_graphs_output:\n",
    "                # Automate naming based on model type and feature\n",
    "                safe_model_type = model_type.replace(\" \", \"_\")\n",
    "                plot_filename = f'{safe_model_type}_{feature}_qq_plot.png'\n",
    "                plot_path = os.path.join(self.graphs_output_dir, plot_filename)\n",
    "                os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                plt.savefig(plot_path)\n",
    "                self.logger.info(f\"Saved QQ plot for '{feature}' at '{plot_path}'\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def plot_normalization(self, X_original: pd.DataFrame, X_transformed: pd.DataFrame, numerical_features: List[str], model_type: str):\n",
    "        \"\"\"\n",
    "        Plot feature distributions before and after normalization.\n",
    "\n",
    "        Args:\n",
    "            X_original (pd.DataFrame): Original numerical features before normalization.\n",
    "            X_transformed (pd.DataFrame): Transformed numerical features after normalization.\n",
    "            numerical_features (List[str]): List of numerical feature names.\n",
    "            model_type (str): Type of the machine learning model.\n",
    "        \"\"\"\n",
    "        for feature in numerical_features:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            # Original Distribution\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(X_original[feature], kde=True, color='blue')\n",
    "            plt.title(f'Original Distribution of {feature}')\n",
    "\n",
    "            # Transformed Distribution\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.histplot(X_transformed[feature], kde=True, color='green')\n",
    "            plt.title(f'Transformed Distribution of {feature}')\n",
    "\n",
    "            plt.suptitle(f'Normalization Check for {feature} ({model_type})', fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "            # Display plot in Jupyter if normalize_debug is True\n",
    "            if self.normalize_debug:\n",
    "                plt.show()\n",
    "\n",
    "            # Save the plot if normalize_graphs_output is True\n",
    "            if self.normalize_graphs_output:\n",
    "                # Automate naming based on model type and feature\n",
    "                safe_model_type = model_type.replace(\" \", \"_\")\n",
    "                plot_filename = f'{safe_model_type}_{feature}_normalization.png'\n",
    "                plot_path = os.path.join(self.graphs_output_dir, plot_filename)\n",
    "                os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "                plt.savefig(plot_path)\n",
    "                self.logger.info(f\"Saved normalization plot for '{feature}' at '{plot_path}'\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train without outliers and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Handle Outliers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        initial_shape = X_train.shape[0]\n",
    "        new_columns = []\n",
    "\n",
    "        # Fetch user-defined outlier handling options or set defaults\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        winsor_limits = outlier_options.get('winsor_limits', [0.05, 0.05])\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        for col in self.numericals:\n",
    "            if self.model_category in ['regression', 'classification']:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    if self.debug_handle_outliers:\n",
    "                        self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering\", self.debug_handle_outliers, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    if self.debug_handle_outliers:\n",
    "                        self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", self.debug_handle_outliers, 'debug')\n",
    "\n",
    "            elif self.model_category in ['svm', 'knn']:\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    if self.debug_handle_outliers:\n",
    "                        self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", self.debug_handle_outliers, 'debug')\n",
    "\n",
    "                # Winsorization\n",
    "                apply_winsor = outlier_options.get('apply_winsor', True)\n",
    "                if apply_winsor:\n",
    "                    from scipy.stats.mstats import winsorize\n",
    "                    X_train[col] = winsorize(X_train[col], limits=winsor_limits)\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Winsorization (limits={winsor_limits}) | '\n",
    "                    if self.debug_handle_outliers:\n",
    "                        self._log(f\"Winsorized '{col}' to limits {winsor_limits}\", self.debug_handle_outliers, 'debug')\n",
    "\n",
    "            elif self.model_category == 'neural_networks':\n",
    "                # Winsorization\n",
    "                apply_winsor = outlier_options.get('apply_winsor', True)\n",
    "                if apply_winsor:\n",
    "                    from scipy.stats.mstats import winsorize\n",
    "                    X_train[col] = winsorize(X_train[col], limits=winsor_limits)\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Winsorization (limits={winsor_limits}) | '\n",
    "                    if self.debug_handle_outliers:\n",
    "                        self._log(f\"Winsorized '{col}' to limits {winsor_limits}\", self.debug_handle_outliers, 'debug')\n",
    "\n",
    "            elif self.model_category == 'clustering':\n",
    "                # For clustering, apply IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    # y_train is None for clustering\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    if self.debug_handle_outliers:\n",
    "                        self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", self.debug_handle_outliers, 'debug')\n",
    "\n",
    "            else:\n",
    "                self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.debug_handle_outliers:\n",
    "            self.logger.debug(f\"Completed: {step_name}. Dataset shape after outlier handling: {X_train.shape}\")\n",
    "            self.logger.debug(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\")\n",
    "            self.logger.debug(f\"New columns handled: {new_columns}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed.\")\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def choose_and_apply_transformations(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Choose and apply transformations based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Transformed X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Choose and Apply Transformations (Based on Normality Tests)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined transformation options or set defaults\n",
    "        transformation_options = self.options.get('choose_transformations', {})\n",
    "        transformation_method = transformation_options.get('method', 'power')  # 'power', 'log', 'None'\n",
    "        skewness_threshold = transformation_options.get('skewness_threshold', 1.0)\n",
    "\n",
    "        # Initialize list to collect features needing transformation based on normality tests\n",
    "        if self.normality_results:\n",
    "            features_to_transform = [col for col in self.numericals if self.normality_results[col]['needs_transform']]\n",
    "        else:\n",
    "            # Default transformation for clustering if normality_results is empty\n",
    "            features_to_transform = self.numericals  # Apply to all numerical features\n",
    "            self.logger.info(\"No normality results available. Applying default transformations to all numerical features.\")\n",
    "\n",
    "        if features_to_transform:\n",
    "            self.features_to_transform = features_to_transform  # Store the transformed features\n",
    "            if transformation_method == 'power':\n",
    "                method = transformation_options.get('power_method', 'yeo-johnson')  # 'yeo-johnson' or 'box-cox'\n",
    "                self.transformer = PowerTransformer(method=method)\n",
    "                self.logger.debug(f\"Applying PowerTransformer with method '{method}' to features: {features_to_transform}\")\n",
    "                X_train[features_to_transform] = self.transformer.fit_transform(X_train[features_to_transform])\n",
    "                if X_test is not None:\n",
    "                    # Ensure X_test is reindexed to match X_train after transformations\n",
    "                    X_test = X_test.reindex(X_train.index)\n",
    "                    X_test[features_to_transform] = self.transformer.transform(X_test[features_to_transform])\n",
    "                for col in features_to_transform:\n",
    "                    self.feature_reasons[col] += f'Applied PowerTransformer ({method}) | '\n",
    "            elif transformation_method == 'log':\n",
    "                # Apply log transformation if data is strictly positive\n",
    "                apply_log = True\n",
    "                for col in features_to_transform:\n",
    "                    if (X_train[col] <= 0).any():\n",
    "                        self.logger.warning(f\"Cannot apply log transform to '{col}' as it contains non-positive values.\")\n",
    "                        apply_log = False\n",
    "                        break\n",
    "                if apply_log:\n",
    "                    self.logger.debug(f\"Applying Log Transform to features: {features_to_transform}\")\n",
    "                    X_train[features_to_transform] = np.log1p(X_train[features_to_transform])\n",
    "                    if X_test is not None:\n",
    "                        X_test[features_to_transform] = np.log1p(X_test[features_to_transform])\n",
    "                    for col in features_to_transform:\n",
    "                        self.feature_reasons[col] += 'Applied Log Transform | '\n",
    "                else:\n",
    "                    self.logger.info(\"Log Transform skipped due to non-positive values.\")\n",
    "            elif transformation_method is None:\n",
    "                self.logger.info(\"Transformation method set to None. No transformations applied.\")\n",
    "            else:\n",
    "                self.logger.error(f\"Transformation method '{transformation_method}' is not supported.\")\n",
    "                raise ValueError(f\"Transformation method '{transformation_method}' is not supported.\")\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_choose_transformations:\n",
    "                self.logger.debug(f\"Completed: {step_name}. Transformed features: {features_to_transform}\")\n",
    "                self.logger.debug(f\"Sample of transformed X_train:\\n{X_train[features_to_transform].head()}\")\n",
    "                if X_test is not None:\n",
    "                    self.logger.debug(f\"Sample of transformed X_test:\\n{X_test[features_to_transform].head()}\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied transformations to {len(features_to_transform)} features.\")\n",
    "        else:\n",
    "            self.logger.info(\"No significant skewness or p-value indicators detected. No transformations applied.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_choose_transformations:\n",
    "                self.logger.debug(f\"Completed: {step_name}. No transformations were applied.\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No transformations were applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Encoded X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Encode Categorical Variables\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OneHotEncoder')  # Options: 'OneHotEncoder', 'OrdinalEncoder', 'FrequencyEncoder', etc.\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'ignore')  # For OneHotEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Override to ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OneHotEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OneHotEncoder(handle_unknown=handle_unknown, sparse_output=False), self.nominal_categoricals)\n",
    "                )\n",
    "            elif nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(), self.nominal_categoricals)\n",
    "                )\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self.logger.debug(f\"Applied Frequency Encoding to '{col}'.\")\n",
    "                transformers = []  # No transformers needed\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_encode_categoricals:\n",
    "                self.logger.debug(f\"Completed: {step_name}. No encoding was applied.\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No encoding was applied.\")\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough'  # Keep other columns unchanged\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            if self.debug_encode_categoricals:\n",
    "                self._log(\"Fitted and transformed X_train with ColumnTransformer.\", self.debug_encode_categoricals, 'debug')\n",
    "            else:\n",
    "                self.logger.info(\"Fitted and transformed X_train with ColumnTransformer.\")\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                if self.debug_encode_categoricals:\n",
    "                    self._log(\"Transformed X_test with fitted ColumnTransformer.\", self.debug_encode_categoricals, 'debug')\n",
    "                else:\n",
    "                    self.logger.info(\"Transformed X_test with fitted ColumnTransformer.\")\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OneHotEncoder':\n",
    "                nominal_encoded_names = self.preprocessor.named_transformers_['nominal'].get_feature_names_out(self.nominal_categoricals).tolist()\n",
    "                encoded_feature_names += nominal_encoded_names\n",
    "                new_columns.extend(nominal_encoded_names)\n",
    "                self.nominal_encoded_feature_names = nominal_encoded_names  # Update the list\n",
    "                self.logger.debug(f\"Nominal encoded feature names (OneHotEncoder): {self.nominal_encoded_feature_names}\")\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "                new_columns.extend(self.nominal_categoricals)\n",
    "                self.nominal_encoded_feature_names = self.nominal_categoricals  # Update the list\n",
    "                self.logger.debug(f\"Nominal encoded feature names (OrdinalEncoder): {self.nominal_encoded_feature_names}\")\n",
    "\n",
    "            # Identify passthrough (numerical) feature names\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(passthrough_features)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_['ordinal'] if 'ordinal' in self.preprocessor.named_transformers_ else None\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_['nominal'] if 'nominal' in self.preprocessor.named_transformers_ else None\n",
    "\n",
    "            # Store encoded nominal feature names for inverse transformation\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OneHotEncoder':\n",
    "                self.nominal_encoded_feature_names = nominal_encoded_names\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_encode_categoricals:\n",
    "                self.logger.debug(f\"Completed: {step_name}. X_train_encoded shape: {X_train_encoded_df.shape}\")\n",
    "                self.logger.debug(f\"Columns after encoding: {encoded_feature_names}\")\n",
    "                self.logger.debug(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\")\n",
    "                self.logger.debug(f\"New columns added: {new_columns}\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Encoded categorical variables.\")\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "        else:\n",
    "            # Frequency Encoding was applied; no transformers to handle\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_encode_categoricals:\n",
    "                self.logger.debug(f\"Completed: {step_name}. Frequency encoding applied to nominal features.\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Frequency encoding applied to nominal features.\")\n",
    "            return X_train, X_test\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'neural_networks', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # User-specified scaling method\n",
    "            if scaling_method == 'StandardScaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method == 'MinMaxScaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method == 'RobustScaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method == 'None':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            self.logger.debug(f\"Features to scale: {features_to_scale}\")\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_apply_scaling:\n",
    "                self.logger.debug(f\"Applied {scaling_type} to features: {features_to_scale}\")\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self.logger.debug(f\"Scaler Parameters: mean={scaler.mean_}\")\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self.logger.debug(f\"Scaler Parameters: scale={scaler.scale_}\")\n",
    "                self.logger.debug(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\")\n",
    "                if X_test is not None:\n",
    "                    self.logger.debug(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if self.debug_apply_scaling:\n",
    "                self.logger.debug(f\"Completed: {step_name}. No scaling was applied.\")\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "    def smote_numerics_criteria(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        imbalance_threshold: float = 0.1,\n",
    "        extreme_imbalance_threshold: float = 0.05,\n",
    "        noise_threshold: float = 0.1,\n",
    "        overlap_threshold: float = 0.1,\n",
    "        boundary_threshold: float = 0.1,\n",
    "        debug: bool = False\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recommend SMOTE variants for numerical-only datasets based on dataset characteristics.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "            imbalance_threshold (float): Threshold for considering class imbalance.\n",
    "            extreme_imbalance_threshold (float): Threshold for extreme class imbalance.\n",
    "            noise_threshold (float): Threshold for noise level.\n",
    "            overlap_threshold (float): Threshold for class overlap.\n",
    "            boundary_threshold (float): Threshold for boundary complexities.\n",
    "            debug (bool): Flag to enable debug logging.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Recommended SMOTE variants in order of preference.\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Step 1: Class Distribution\n",
    "        class_distribution = y_train.value_counts(normalize=True)\n",
    "        majority_class = class_distribution.idxmax()\n",
    "        minority_class = class_distribution.idxmin()\n",
    "\n",
    "        severe_imbalance = class_distribution[minority_class] < imbalance_threshold\n",
    "        extreme_imbalance = class_distribution[minority_class] < extreme_imbalance_threshold\n",
    "\n",
    "        if debug:\n",
    "            self.logger.debug(f\"X_train Shape: {X_train.shape}\")\n",
    "            self.logger.debug(f\"Class Distribution: {class_distribution.to_dict()}\")\n",
    "            if extreme_imbalance:\n",
    "                self.logger.warning(f\"Extreme imbalance detected: {class_distribution[minority_class]:.2%}\")\n",
    "\n",
    "        # Step 2: Noise Analysis\n",
    "        minority_samples = X_train[y_train == minority_class]\n",
    "        majority_samples = X_train[y_train == majority_class]\n",
    "\n",
    "        try:\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(majority_samples)\n",
    "            distances, _ = knn.kneighbors(minority_samples)\n",
    "            median_distance = np.median(distances)\n",
    "            noise_ratio = np.mean(distances < median_distance)\n",
    "            noisy_data = noise_ratio > noise_threshold\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Median Distance to Nearest Neighbors: {median_distance}\")\n",
    "                self.logger.debug(f\"Noise Ratio: {noise_ratio:.2%}\")\n",
    "        except ValueError as e:\n",
    "            self.logger.error(f\"Noise analysis error: {e}\")\n",
    "            noisy_data = False\n",
    "\n",
    "        # Step 3: Overlap Analysis\n",
    "        try:\n",
    "            pdistances = pairwise_distances(minority_samples, majority_samples)\n",
    "            overlap_metric = np.mean(pdistances < 1.0)  # Threshold can be adjusted\n",
    "            overlapping_classes = overlap_metric > overlap_threshold\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Overlap Metric: {overlap_metric:.2%}\")\n",
    "        except ValueError as e:\n",
    "            self.logger.error(f\"Overlap analysis error: {e}\")\n",
    "            overlapping_classes = False\n",
    "\n",
    "        # Step 4: Boundary Concentration\n",
    "        try:\n",
    "            boundary_ratio = np.mean(np.min(distances, axis=1) < np.percentile(distances, 25))\n",
    "            boundary_concentration = boundary_ratio > boundary_threshold\n",
    "\n",
    "            if debug:\n",
    "                self.logger.debug(f\"Boundary Concentration Ratio: {boundary_ratio:.2%}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Boundary concentration error: {e}\")\n",
    "            boundary_concentration = False\n",
    "\n",
    "        # Step 5: Recommendations\n",
    "        if extreme_imbalance:\n",
    "            recommendations.append(\"ADASYN\" if not noisy_data else \"SMOTEENN\")\n",
    "            recommendations.append(\"SMOTEENN\")\n",
    "            recommendations.append(\"SMOTETomek\")\n",
    "            recommendations.append(\"BorderlineSMOTE\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Extreme imbalance detected. Recommended variants: ADASYN/SMOTEENN, SMOTEENN, SMOTETomek, BorderlineSMOTE\")\n",
    "            return recommendations\n",
    "\n",
    "        if severe_imbalance:\n",
    "            recommendations.append(\"ADASYN\" if not noisy_data else \"SMOTEENN\")\n",
    "            recommendations.append(\"SMOTEENN\")\n",
    "            recommendations.append(\"SMOTETomek\")\n",
    "            recommendations.append(\"BorderlineSMOTE\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Severe imbalance detected. Recommended variants: ADASYN/SMOTEENN, SMOTEENN, SMOTETomek, BorderlineSMOTE\")\n",
    "            return recommendations\n",
    "\n",
    "        if noisy_data:\n",
    "            recommendations.append(\"SMOTEENN\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Noisy data detected. Recommended variant: SMOTEENN\")\n",
    "        \n",
    "        if overlapping_classes:\n",
    "            recommendations.append(\"SMOTETomek\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Overlapping classes detected. Recommended variant: SMOTETomek\")\n",
    "        \n",
    "        if boundary_concentration:\n",
    "            recommendations.append(\"BorderlineSMOTE\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"Boundary concentration detected. Recommended variant: BorderlineSMOTE\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            recommendations.append(\"SMOTE\")\n",
    "            if debug:\n",
    "                self.logger.debug(\"No specific issues detected. Recommended variant: SMOTE\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        recommendations = [x for x in recommendations if not (x in seen or seen.add(x))]\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance, dataset characteristics, and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Fetch user-defined SMOTE options or set defaults\n",
    "        smote_options = self.options.get('implement_smote', {})\n",
    "        user_smote_variant = smote_options.get('variant', None)\n",
    "        smote_params = smote_options.get('params', {})\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # SMOTE Variant Selection\n",
    "        recommended_variants = []\n",
    "        variant_name = 'SMOTE'  # Default variant\n",
    "\n",
    "        if user_smote_variant:\n",
    "            variant_name = user_smote_variant\n",
    "            self.logger.info(f\"User-specified SMOTE variant: {variant_name}\")\n",
    "        else:\n",
    "            if has_numericals and not has_categoricals:\n",
    "                # Numeric only dataset: Recommend based on criteria\n",
    "                self.logger.info(\"Dataset contains only numerical features. Analyzing to recommend SMOTE variants...\")\n",
    "                recommended_variants = self.smote_numerics_criteria(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    imbalance_threshold=self.imbalance_threshold,\n",
    "                    extreme_imbalance_threshold=self.extreme_imbalance_threshold,\n",
    "                    noise_threshold=self.noise_threshold,\n",
    "                    overlap_threshold=self.overlap_threshold,\n",
    "                    boundary_threshold=self.boundary_threshold,\n",
    "                    debug=self.debug_implement_smote\n",
    "                )\n",
    "                # Select the first recommended variant\n",
    "                if recommended_variants:\n",
    "                    variant_name = recommended_variants[0]\n",
    "                    self.logger.info(f\"Recommended SMOTE variant: {variant_name}\")\n",
    "                else:\n",
    "                    variant_name = 'SMOTE'\n",
    "                    self.logger.info(\"No specific recommendation from criteria. Using default SMOTE.\")\n",
    "            elif has_numericals and has_categoricals:\n",
    "                # Mixed dataset: Use SMOTENC\n",
    "                variant_name = 'SMOTENC'\n",
    "                self.logger.info(\"Dataset contains numerical and categorical features. Using SMOTENC.\")\n",
    "            elif not has_numericals and has_categoricals:\n",
    "                # Categorical only dataset: Use SMOTEN\n",
    "                variant_name = 'SMOTEN'\n",
    "                self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "            else:\n",
    "                # Fallback to SMOTE\n",
    "                variant_name = 'SMOTE'\n",
    "                self.logger.info(\"Dataset composition not recognized. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE variant\n",
    "        try:\n",
    "            self.logger.debug(f\"Initializing SMOTE Variant '{variant_name}' with parameters: {smote_params}\")\n",
    "\n",
    "            if variant_name == 'SMOTENC':\n",
    "                if not self.nominal_encoded_feature_names:\n",
    "                    self.logger.error(\"No nominal encoded feature names available for SMOTENC.\")\n",
    "                    raise ValueError(\"No nominal encoded feature names available for SMOTENC.\")\n",
    "                categorical_features = [X_train.columns.get_loc(col) for col in self.nominal_encoded_feature_names]\n",
    "                smote = SMOTENC(\n",
    "                    categorical_features=categorical_features,\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            elif variant_name == 'SMOTEN':\n",
    "                smote = SMOTEN(\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            elif variant_name in ['SMOTE', 'ADASYN', 'BorderlineSMOTE', 'SMOTETomek', 'SMOTEENN']:\n",
    "                smote_class = {\n",
    "                    'SMOTE': SMOTE,\n",
    "                    'ADASYN': ADASYN,\n",
    "                    'BorderlineSMOTE': BorderlineSMOTE,\n",
    "                    'SMOTETomek': SMOTETomek,\n",
    "                    'SMOTEENN': SMOTEENN\n",
    "                }.get(variant_name, SMOTE)  # Default to SMOTE if not found\n",
    "                smote = smote_class(\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown SMOTE variant '{variant_name}'. Falling back to SMOTE.\")\n",
    "                smote = SMOTE(random_state=42, **smote_params)\n",
    "\n",
    "            # Validate parameters using inspect\n",
    "            smote_signature = signature(smote.__class__)\n",
    "            valid_params = smote_signature.parameters.keys()\n",
    "            invalid_params = set(smote_params.keys()) - set(valid_params)\n",
    "            if invalid_params:\n",
    "                self.logger.warning(f\"Invalid parameters for SMOTE variant '{variant_name}': {invalid_params}. These will be ignored.\")\n",
    "\n",
    "        except TypeError as e:\n",
    "            self.logger.error(f\"Error initializing SMOTE variant '{variant_name}': {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Apply SMOTE variant\n",
    "        try:\n",
    "            X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "            self.smote = smote\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Applied SMOTE Variant '{variant_name}'. Resampled X_train shape: {X_res.shape}, y_train shape: {y_res.shape}\")\n",
    "            return X_res, y_res\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"ValueError during SMOTE application: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error during SMOTE application: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def final_inverse_transformations(self, X_test_preprocessed: pd.DataFrame, X_test_original: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformations to revert preprocessed data back to its original form based on user options.\n",
    "\n",
    "        Args:\n",
    "            X_test_preprocessed (pd.DataFrame): Preprocessed test features.\n",
    "            X_test_original (pd.DataFrame): Original test features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inverse-transformed test features.\n",
    "        \"\"\"\n",
    "        step_name = \"Final Inverse Transformations for Interpretability\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Fetch user-defined inverse transformation options or set defaults\n",
    "        inverse_options = self.options.get('inverse_transformations', {})\n",
    "        inverse_scaling = inverse_options.get('inverse_scaling', True)\n",
    "        inverse_transformation = inverse_options.get('inverse_transformation', True)\n",
    "        inverse_encoding = inverse_options.get('inverse_encoding', True)\n",
    "\n",
    "        # Initialize DataFrame for inverse transformed data\n",
    "        X_inverse = pd.DataFrame(index=X_test_preprocessed.index)\n",
    "\n",
    "        # Inverse Scaling\n",
    "        if inverse_scaling and hasattr(self, 'scaler') and self.scaler is not None:\n",
    "            try:\n",
    "                # Ensure that features_to_scale were scaled\n",
    "                scaling_features = self.options.get('apply_scaling', {}).get('features', self.numericals)\n",
    "                X_inverse[scaling_features] = self.scaler.inverse_transform(X_test_preprocessed[scaling_features])\n",
    "                for col in scaling_features:\n",
    "                    self.feature_reasons[col] += f'Inverse Scaling Applied | '\n",
    "                if self.debug_final_inverse_transformations:\n",
    "                    self.logger.debug(\"Inverse Scaling Completed\")\n",
    "                    self.logger.debug(f\"Sample of inverse-scaled data:\\n{X_inverse[scaling_features].head()}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Scaling: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            # If scaling was not applied, retain original numerical features\n",
    "            X_inverse[self.numericals] = X_test_preprocessed[self.numericals]\n",
    "\n",
    "        # Inverse Transformation (PowerTransformer or Log Transform)\n",
    "        if inverse_transformation and self.transformer is not None:\n",
    "            try:\n",
    "                # Inverse transform only the transformed features\n",
    "                X_inverse[self.features_to_transform] = self.transformer.inverse_transform(X_inverse[self.features_to_transform])\n",
    "                for col in self.features_to_transform:\n",
    "                    self.feature_reasons[col] += f'Inverse Transformation Applied | '\n",
    "                if self.debug_final_inverse_transformations:\n",
    "                    self.logger.debug(\"Inverse Transformation Applied\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Transformation: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Ordinal Categorical Features\n",
    "        if inverse_encoding and self.ordinal_categoricals and self.ordinal_encoder:\n",
    "            try:\n",
    "                X_inverse[self.ordinal_categoricals] = self.ordinal_encoder.inverse_transform(X_test_preprocessed[self.ordinal_categoricals])\n",
    "                for col in self.ordinal_categoricals:\n",
    "                    self.feature_reasons[col] += 'Inverse Ordinal Encoding Applied | '\n",
    "                if self.debug_final_inverse_transformations:\n",
    "                    self.logger.debug(\"Inverse Ordinal Encoding Completed for Ordinal Categorical Features\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse Ordinal Encoding for Ordinal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Inverse Encoding for Nominal Categorical Features\n",
    "        if inverse_encoding and self.nominal_categoricals and self.preprocessor and 'nominal' in self.preprocessor.named_transformers_:\n",
    "            try:\n",
    "                if hasattr(self.preprocessor.named_transformers_['nominal'], 'get_feature_names_out'):\n",
    "                    # Extract nominal encoded features\n",
    "                    nominal_encoded = X_test_preprocessed[self.preprocessor.named_transformers_['nominal'].get_feature_names_out(self.nominal_categoricals)]\n",
    "                    nominal_original = self.preprocessor.named_transformers_['nominal'].inverse_transform(nominal_encoded)\n",
    "                    nominal_original_df = pd.DataFrame(nominal_original, columns=self.nominal_categoricals, index=X_test_preprocessed.index)\n",
    "                    X_inverse[self.nominal_categoricals] = nominal_original_df\n",
    "                    for col in self.nominal_categoricals:\n",
    "                        self.feature_reasons[col] += 'Inverse One-Hot Encoding Applied | '\n",
    "                    if self.debug_final_inverse_transformations:\n",
    "                        self.logger.debug(\"Inverse One-Hot Encoding Completed for Nominal Categorical Features\")\n",
    "                else:\n",
    "                    self.logger.warning(\"Nominal encoder does not support get_feature_names_out. Skipping inverse transformation for nominal features.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during inverse One-Hot Encoding for Nominal Categorical Features: {e}\")\n",
    "                raise\n",
    "\n",
    "        # Combine all features\n",
    "        try:\n",
    "            # Include passthrough (non-transformed) features\n",
    "            passthrough_features = [col for col in X_test_original.columns if col not in self.numericals + self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            if passthrough_features:\n",
    "                X_inverse = pd.concat([X_inverse, X_test_preprocessed[passthrough_features]], axis=1)\n",
    "\n",
    "            # Reorder columns to match the original DataFrame\n",
    "            X_final_inverse = X_inverse[X_test_original.columns]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during combining inverse transformed data: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        if self.debug_final_inverse_transformations:\n",
    "            self.logger.debug(f\"Completed: {step_name}. Inverse-transformed X_test shape: {X_final_inverse.shape}\")\n",
    "            self.logger.debug(f\"Sample of inverse-transformed X_test:\\n{X_final_inverse.head()}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Inverse transformations applied.\")\n",
    "\n",
    "        return X_final_inverse\n",
    "\n",
    "    def validate_inverse_transformations(self, X_original: pd.DataFrame, X_inverse: pd.DataFrame, tolerance: float = 1e-4):\n",
    "        \"\"\"\n",
    "        Validate that inverse transformations accurately restore original data within acceptable tolerances.\n",
    "\n",
    "        Args:\n",
    "            X_original (pd.DataFrame): Original features.\n",
    "            X_inverse (pd.DataFrame): Inverse-transformed features.\n",
    "            tolerance (float, optional): Tolerance level for differences. Defaults to 1e-4.\n",
    "        \"\"\"\n",
    "        step_name = \"Final Inverse Transformation Validation\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        differences = {}\n",
    "\n",
    "        for col in self.nominal_categoricals + self.ordinal_categoricals:\n",
    "            diff = X_original[col].astype(str) != X_inverse[col].astype(str)\n",
    "            differences[col] = {\n",
    "                'total_differences': diff.sum(),\n",
    "                'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "            }\n",
    "\n",
    "        for col in self.numericals:\n",
    "            diff = np.abs(X_original[col] - X_inverse[col]) > tolerance\n",
    "            differences[col] = {\n",
    "                'total_differences': diff.sum(),\n",
    "                'percentage_differences': (diff.sum() / len(diff)) * 100\n",
    "            }\n",
    "\n",
    "        # Display the differences\n",
    "        for col, stats in differences.items():\n",
    "            self.logger.info(f\"Column: {col}\")\n",
    "            self.logger.info(f\" - Total Differences: {stats['total_differences']}\")\n",
    "            self.logger.info(f\" - Percentage Differences: {stats['percentage_differences']:.2f}%\")\n",
    "\n",
    "            if stats['total_differences'] > 0:\n",
    "                self.logger.warning(f\"Differences found in column '{col}':\")\n",
    "                if col in self.nominal_categoricals + self.ordinal_categoricals:\n",
    "                    mask = X_original[col].astype(str) != X_inverse[col].astype(str)\n",
    "                else:\n",
    "                    mask = np.abs(X_original[col] - X_inverse[col]) > tolerance\n",
    "                comparison = pd.concat([\n",
    "                    X_original.loc[mask, col].reset_index(drop=True).rename('Original'),\n",
    "                    X_inverse.loc[mask, col].reset_index(drop=True).rename('Inverse Transformed')\n",
    "                ], axis=1)\n",
    "                self.logger.debug(f\"Differences in '{col}':\\n{comparison}\")\n",
    "                self.logger.debug(\"\\n\")\n",
    "\n",
    "        # Check if indices are aligned\n",
    "        if not X_original.index.equals(X_inverse.index):\n",
    "            self.logger.warning(\"Indices of original and inverse transformed data do not match.\")\n",
    "        else:\n",
    "            self.logger.info(\"Success: Indices of original and inverse transformed data are aligned.\")\n",
    "\n",
    "        # Check overall success\n",
    "        total_differences = sum([v['total_differences'] for v in differences.values()])\n",
    "        if total_differences == 0:\n",
    "            self.logger.info(\"Inverse Transformation Validation Passed: No differences found.\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Inverse Transformation Validation Failed: {total_differences} differences found across all features.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        if self.debug_validate_inverse_transformations:\n",
    "            self.logger.debug(f\"Completed: {step_name}. Validation results generated.\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Validation results generated.\")\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        if self.debug_generate_recommendations:\n",
    "            self.logger.debug(f\"Completed: {step_name}. Recommendations generated.\")\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "    def save_transformers(self):\n",
    "        \"\"\"\n",
    "        Save fitted transformers to disk for future use during prediction.\n",
    "        \"\"\"\n",
    "        transformers_path = os.path.join(self.graphs_output_dir, 'transformers.pkl')\n",
    "        transformers = {\n",
    "            'numerical_imputer': self.numerical_imputer if hasattr(self, 'numerical_imputer') else None,\n",
    "            'categorical_imputer': self.categorical_imputer if hasattr(self, 'categorical_imputer') else None,\n",
    "            'transformer': self.transformer,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'scaler': self.scaler,\n",
    "            'ordinal_encoder': self.ordinal_encoder,\n",
    "            'nominal_encoder': self.nominal_encoder,\n",
    "            'cluster_transformers': self.cluster_transformers,\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order\n",
    "        }\n",
    "        joblib.dump(transformers, transformers_path)\n",
    "        self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "\n",
    "    def load_transformers(self):\n",
    "        \"\"\"\n",
    "        Load transformers from disk for use during prediction.\n",
    "        \"\"\"\n",
    "        transformers_path = os.path.join(self.graphs_output_dir, 'transformers.pkl')\n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "\n",
    "        transformers = joblib.load(transformers_path)\n",
    "        self.numerical_imputer = transformers.get('numerical_imputer')\n",
    "        self.categorical_imputer = transformers.get('categorical_imputer')\n",
    "        self.transformer = transformers.get('transformer')\n",
    "        self.preprocessor = transformers.get('preprocessor')\n",
    "        self.scaler = transformers.get('scaler')\n",
    "        self.ordinal_encoder = transformers.get('ordinal_encoder')\n",
    "        self.nominal_encoder = transformers.get('nominal_encoder')\n",
    "        self.cluster_transformers = transformers.get('cluster_transformers', {})\n",
    "        self.smote = transformers.get('smote', None)\n",
    "        self.final_feature_order = transformers.get('final_feature_order', [])\n",
    "        self.logger.info(f\"Transformers loaded from '{transformers_path}'.\")\n",
    "\n",
    "    def handle_missing_values_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for prediction mode using fitted imputers.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Transformed X and None.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Handling missing values for prediction.\")\n",
    "\n",
    "        if self.numericals:\n",
    "            X[self.numericals] = self.numerical_imputer.transform(X[self.numericals])\n",
    "\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            X[all_categoricals] = self.categorical_imputer.transform(X[all_categoricals])\n",
    "\n",
    "        return X, None\n",
    "\n",
    "    def choose_and_apply_transformations_predict(self, X: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply transformations for prediction mode using fitted transformers.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Transformed X and None.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Applying transformations for prediction.\")\n",
    "\n",
    "        if self.transformer is not None:\n",
    "            X[self.features_to_transform] = self.transformer.transform(X[self.features_to_transform])\n",
    "\n",
    "        return X, None\n",
    "\n",
    "    def encode_categorical_variables_predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encode categorical variables for prediction mode using fitted encoders.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Encoded features.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Encoding categorical variables for prediction.\")\n",
    "\n",
    "        if self.preprocessor is not None:\n",
    "            X_encoded = self.preprocessor.transform(X)\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and self.ordinal_encoder is not None:\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and self.nominal_encoder is not None:\n",
    "                if hasattr(self.preprocessor.named_transformers_['nominal'], 'get_feature_names_out'):\n",
    "                    nominal_encoded_names = self.preprocessor.named_transformers_['nominal'].get_feature_names_out(self.nominal_categoricals).tolist()\n",
    "                    encoded_feature_names += nominal_encoded_names\n",
    "            passthrough_features = [col for col in X.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "\n",
    "            X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_feature_names, index=X.index)\n",
    "            return X_encoded_df\n",
    "\n",
    "        return X\n",
    "\n",
    "    def apply_scaling_predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply scaling for prediction mode using fitted scaler.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Scaled features.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Applying scaling for prediction.\")\n",
    "\n",
    "        if self.scaler is not None and self.options.get('apply_scaling', {}).get('features', []):\n",
    "            scaling_features = self.options.get('apply_scaling', {}).get('features', self.numericals)\n",
    "            X[scaling_features] = self.scaler.transform(X[scaling_features])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        y: Optional[pd.Series] = None  # Make y optional\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features.\n",
    "            y (Optional[pd.Series]): Target variable (required for training).\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Processed data based on mode.\n",
    "        \"\"\"\n",
    "        step_name = \"Final Preprocessing Pipeline\"\n",
    "        self.logger.info(f\"Starting: {step_name} in '{self.mode}' mode.\")\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Step 1: Split Dataset\n",
    "            X_train, X_test, y_train, y_test = self.split_dataset(X, y)\n",
    "\n",
    "            # Step 2: Handle Missing Values\n",
    "            X_train, X_test = self.handle_missing_values(X_train, X_test)\n",
    "\n",
    "            # Step 3: Test for Normality (including 'clustering')\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                self.test_normality(X_train)\n",
    "\n",
    "            # Step 4: Handle Outliers\n",
    "            X_train, y_train = self.handle_outliers(X_train, y_train)\n",
    "\n",
    "            # Step 5: Generate Preprocessing Recommendations\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.info(\"Preprocessing Recommendations Generated.\")\n",
    "\n",
    "            # Step 6: Choose and Apply Transformations\n",
    "            X_train_before_transformation = X_train.copy()\n",
    "            X_test_before_transformation = X_test.copy() if X_test is not None else None\n",
    "            X_train, X_test = self.choose_and_apply_transformations(X_train, X_test)\n",
    "\n",
    "            # Step 7: Plot Normalization Before and After Transformations\n",
    "            if self.transformer is not None:\n",
    "                self.plot_normalization(\n",
    "                    X_original=X_train_before_transformation[self.numericals],\n",
    "                    X_transformed=X_train[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                self.plot_qq(\n",
    "                    X_original=X_train_before_transformation[self.numericals],\n",
    "                    X_transformed=X_train[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                if X_test_before_transformation is not None:\n",
    "                    self.plot_normalization(\n",
    "                        X_original=X_test_before_transformation[self.numericals],\n",
    "                        X_transformed=X_test[self.numericals],\n",
    "                        numerical_features=self.numericals,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "                    self.plot_qq(\n",
    "                        X_original=X_test_before_transformation[self.numericals],\n",
    "                        X_transformed=X_test[self.numericals],\n",
    "                        numerical_features=self.numericals,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "\n",
    "            # Step 8: Encode Categorical Variables\n",
    "            X_train, X_test = self.encode_categorical_variables(X_train, X_test)\n",
    "\n",
    "            # Step 9: Apply Scaling\n",
    "            X_train_before_scaling = X_train.copy()\n",
    "            X_test_before_scaling = X_test.copy() if X_test is not None else None\n",
    "            X_train, X_test = self.apply_scaling(X_train, X_test)\n",
    "\n",
    "            # Step 10: Plot Normalization After Scaling\n",
    "            if hasattr(self, 'scaler') and self.scaler is not None:\n",
    "                scaling_features = self.options.get('apply_scaling', {}).get('features', self.numericals)\n",
    "                self.plot_normalization(\n",
    "                    X_original=X_train_before_scaling[scaling_features],\n",
    "                    X_transformed=X_train[scaling_features],\n",
    "                    numerical_features=scaling_features,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                self.plot_qq(\n",
    "                    X_original=X_train_before_scaling[scaling_features],\n",
    "                    X_transformed=X_train[scaling_features],\n",
    "                    numerical_features=scaling_features,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                if X_test_before_scaling is not None:\n",
    "                    self.plot_normalization(\n",
    "                        X_original=X_test_before_scaling[scaling_features],\n",
    "                        X_transformed=X_test[scaling_features],\n",
    "                        numerical_features=scaling_features,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "                    self.plot_qq(\n",
    "                        X_original=X_test_before_scaling[scaling_features],\n",
    "                        X_transformed=X_test[scaling_features],\n",
    "                        numerical_features=scaling_features,\n",
    "                        model_type=self.model_type\n",
    "                    )\n",
    "\n",
    "            # Step 11: Implement SMOTE (Train Only for Classification)\n",
    "            if self.model_category == 'classification':\n",
    "                X_train, y_train = self.implement_smote(X_train, y_train)\n",
    "            else:\n",
    "                self.logger.info(\"SMOTE not applied: Not a classification model.\")\n",
    "\n",
    "            # Step 12: Save Transformers for Prediction\n",
    "            if self.mode == 'train':\n",
    "                # Assign and save final feature order\n",
    "                self.final_feature_order = list(X_train.columns)\n",
    "                self.logger.info(f\"Final feature order: {self.final_feature_order}\")\n",
    "                \n",
    "                # Reindex to ensure consistent ordering\n",
    "                X_train = X_train[self.final_feature_order]\n",
    "                if X_test is not None:\n",
    "                    X_test = X_test[self.final_feature_order]\n",
    "                \n",
    "                # Save transformers including feature order\n",
    "                self.save_transformers()\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Step '{step_name}' completed in '{self.mode}' mode.\")\n",
    "\n",
    "            if self.model_category in ['classification', 'regression']:\n",
    "                return X_train, X_test, y_train, y_test, recommendations\n",
    "            elif self.model_category == 'clustering':\n",
    "                return X_train, recommendations\n",
    "            else:\n",
    "                return X_train, recommendations\n",
    "\n",
    "        elif self.mode == 'clustering':\n",
    "            # Perform clustering-specific preprocessing steps\n",
    "            # Step 1: Handle Missing Values\n",
    "            X_processed, _ = self.handle_missing_values(X, None)\n",
    "\n",
    "            # Step 2: Handle Outliers\n",
    "            X_processed, _ = self.handle_outliers(X_processed, None)\n",
    "\n",
    "            # Step 3: Choose and Apply Transformations\n",
    "            X_processed, _ = self.choose_and_apply_transformations(X_processed, None)\n",
    "\n",
    "            # Step 4: Encode Categorical Variables\n",
    "            X_processed, _ = self.encode_categorical_variables(X_processed, None)\n",
    "\n",
    "            # Step 5: Apply Scaling\n",
    "            X_processed, _ = self.apply_scaling(X_processed, None)\n",
    "\n",
    "            # Step 6: Generate Preprocessing Recommendations\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.info(\"Preprocessing Recommendations Generated.\")\n",
    "\n",
    "            # Step 7: Plot Normalization Before and After Transformations\n",
    "            if self.transformer is not None:\n",
    "                self.plot_normalization(\n",
    "                    X_original=X.copy()[self.numericals],\n",
    "                    X_transformed=X_processed[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "                self.plot_qq(\n",
    "                    X_original=X.copy()[self.numericals],\n",
    "                    X_transformed=X_processed[self.numericals],\n",
    "                    numerical_features=self.numericals,\n",
    "                    model_type=self.model_type\n",
    "                )\n",
    "\n",
    "            # Step 8: Save Transformers for Prediction\n",
    "            self.save_transformers()\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Step '{step_name}' completed in '{self.mode}' mode.\")\n",
    "\n",
    "            return X_processed, recommendations\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            # Step 1: Load Transformers\n",
    "            self.load_transformers()\n",
    "\n",
    "            # Step 2: Handle Missing Values using fitted imputers\n",
    "            X_processed, _ = self.handle_missing_values_predict(X)\n",
    "\n",
    "            # Step 3: Choose and Apply Transformations using fitted transformers\n",
    "            X_processed, _ = self.choose_and_apply_transformations_predict(X_processed)\n",
    "\n",
    "            # Step 4: Encode Categorical Variables using fitted encoders\n",
    "            X_processed = self.encode_categorical_variables_predict(X_processed)\n",
    "\n",
    "            # Step 5: Apply Scaling using fitted scaler\n",
    "            X_processed = self.apply_scaling_predict(X_processed)\n",
    "\n",
    "            # Ensure correct final order\n",
    "            if self.final_feature_order:\n",
    "                missing_in_processed = set(self.final_feature_order) - set(X_processed.columns)\n",
    "                if missing_in_processed:\n",
    "                    raise KeyError(f\"Missing columns in X_processed: {missing_in_processed}\")\n",
    "\n",
    "                # Reindex\n",
    "                X_processed = X_processed[self.final_feature_order]\n",
    "        \n",
    "            # Step 6: Generate Preprocessing Recommendations\n",
    "            recommendations = self.generate_recommendations()\n",
    "            self.logger.info(\"Preprocessing Recommendations Generated.\")\n",
    "\n",
    "            # Step 7: Save Preprocessed Data for Prediction\n",
    "            recommendations.to_csv(os.path.join(self.graphs_output_dir, 'preprocessing_recommendations.csv'))\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Step '{step_name}' completed in '{self.mode}' mode.\")\n",
    "\n",
    "            return X_processed, recommendations\n",
    "\n",
    "        else:\n",
    "            # Handle other modes if necessary\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not yet implemented.\")\n",
    "\n",
    "\n",
    "# Main Execution Script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# from feature_manager import FeatureManager  # Ensure correct import\n",
    "# from data_preprocessor import DataPreprocessor  # Ensure correct import\n",
    "\n",
    "def initialize_feature_manager(save_path: str):\n",
    "    \"\"\"\n",
    "    Initialize the FeatureManager and ensure the save directory exists.\n",
    "\n",
    "    Args:\n",
    "        save_path (str): Path to save the feature metadata pickle file.\n",
    "\n",
    "    Returns:\n",
    "        FeatureManager: An instance of the FeatureManager class.\n",
    "    \"\"\"\n",
    "    # Ensure the directory for save_path exists\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    if save_dir and not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    return FeatureManager(save_path=save_path)\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Define Paths and Flags\n",
    "    # ----------------------------\n",
    "\n",
    "    # Define the path to the single pickle file containing all metadata\n",
    "    save_path = '../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl'  # Adjust as needed\n",
    "\n",
    "    # Define a debug flag based on user preference\n",
    "    debug_flag = False  # Set to False for minimal outputs\n",
    "\n",
    "    # Define normalization plotting flags\n",
    "    normalize_debug = False  # Set to True to display plots in Jupyter\n",
    "    normalize_graphs_output = False  # Set to True to save plots to a directory\n",
    "    graphs_output_dir = '../../ml-preprocessing-utils/data/dataset/test/plots'  # Specify the desired output directory\n",
    "\n",
    "    # Configure root logger based on debug_flag\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else logging.INFO, \n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Initialize FeatureManager\n",
    "    # ----------------------------\n",
    "\n",
    "    feature_manager = initialize_feature_manager(save_path=save_path)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Save Features and Metadata\n",
    "    # ----------------------------\n",
    "\n",
    "    # Define paths for saving\n",
    "    start_dataset_path = '../../data/processed/final_ml_dataset.csv'  # Original dataset CSV path\n",
    "\n",
    "    # Load the original dataset\n",
    "    try:\n",
    "        logger.info(f\"📥 Loading original dataset from {start_dataset_path}...\")\n",
    "        original_df = load_dataset(start_dataset_path)\n",
    "        logger.info(\"✅ Original dataset loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load original dataset: {e}\")\n",
    "        return  # Exit if loading fails\n",
    "\n",
    "    # Define feature categories and column names\n",
    "    ordinal_categoricals = []\n",
    "    nominal_categoricals = []\n",
    "    numericals = [\n",
    "        'release_ball_direction_x', 'release_ball_direction_z', 'release_ball_direction_y',\n",
    "        'elbow_release_angle', 'elbow_max_angle',\n",
    "        'wrist_release_angle', 'wrist_max_angle',\n",
    "        'knee_release_angle', 'knee_max_angle',\n",
    "        'release_ball_speed', 'calculated_release_angle',\n",
    "        'release_ball_velocity_x', 'release_ball_velocity_y', 'release_ball_velocity_z'\n",
    "    ]\n",
    "    y_variable = ['result']\n",
    "    \n",
    "    # Final columns we keep\n",
    "    final_keep_list = ordinal_categoricals + nominal_categoricals + numericals + y_variable\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Save Selected Features and Metadata\n",
    "    # ----------------------------\n",
    "\n",
    "    missing_columns = set(final_keep_list) - set(original_df.columns)\n",
    "    if missing_columns:\n",
    "        logger.error(f\"The following columns are missing in the dataset: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"🔍 Selecting and filtering dataset based on defined features...\")\n",
    "    selected_features_df = original_df[final_keep_list]\n",
    "    logger.info(\"✅ Selected features filtered successfully.\")\n",
    "\n",
    "    assert 'result' in y_variable, \"'result' must be in y_variable.\"\n",
    "    assert y_variable not in numericals, \"'result' should not be in numericals.\"\n",
    "\n",
    "    try:\n",
    "        feature_manager.save_features(\n",
    "            features_df=selected_features_df,\n",
    "            ordinal_categoricals=ordinal_categoricals,\n",
    "            nominal_categoricals=nominal_categoricals,\n",
    "            numericals=numericals,\n",
    "            y_variable=y_variable,\n",
    "            dataset_csv_path=start_dataset_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save features and metadata: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Load Features and Filtered Dataset\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        filtered_df, column_assets = feature_manager.load_features_and_dataset(\n",
    "            debug=debug_flag\n",
    "        )\n",
    "        logger.info(\"✅ Features loaded and dataset filtered successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load features and dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    # Access column assets\n",
    "    ordinals = column_assets.get('ordinal_categoricals', [])\n",
    "    nominals = column_assets.get('nominal_categoricals', [])\n",
    "    nums = column_assets.get('numericals', [])\n",
    "    y_var = column_assets.get('y_variable', [])\n",
    "    logger.debug(f\"y_var = {y_var}\")\n",
    "    if debug_flag:\n",
    "        logger.debug(\"\\n📥 Loaded Data:\")\n",
    "        logger.debug(f\"Column Assets: {column_assets}\")\n",
    "        logger.debug(f\"Ordinal Categoricals: {ordinals}\")\n",
    "        logger.debug(f\"Nominal Categoricals: {nominals}\")\n",
    "        logger.debug(f\"Numericals: {nums}\")\n",
    "        logger.debug(f\"Y Variable: {y_var}\")\n",
    "    else:\n",
    "        logger.info(\"Features and metadata loaded successfully.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Define Models and Options\n",
    "    # ----------------------------\n",
    "\n",
    "    # Define your models and their specific preprocessing options\n",
    "    model_types = [\n",
    "        'Logistic Regression',\n",
    "        'Tree Based Classifier',\n",
    "        'Linear Regression',\n",
    "        'Tree Based Regressor',\n",
    "        # 'K-Means Clustering',\n",
    "    ]\n",
    "\n",
    "    model_specific_options = {\n",
    "        # -----------------------------------------------------------\n",
    "        # 1. LOGISTIC REGRESSION (Classification)\n",
    "        # -----------------------------------------------------------\n",
    "        'Logistic Regression': {\n",
    "            'split_dataset': {\n",
    "                'test_size': 0.25,\n",
    "                'random_state': 42,\n",
    "                'stratify_for_classification': True\n",
    "            },\n",
    "            'handle_missing_values': {\n",
    "                'numerical_strategy': {\n",
    "                    'strategy': 'mean',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                },\n",
    "                'categorical_strategy': {\n",
    "                    'strategy': 'most_frequent',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                }\n",
    "            },\n",
    "            'test_normality': {\n",
    "                'p_value_threshold': 0.05,\n",
    "                'skewness_threshold': 1.0,\n",
    "                'use_p_value_other_models': False\n",
    "            },\n",
    "            'handle_outliers': {\n",
    "                'zscore_threshold': 3,\n",
    "                'iqr_multiplier': 1.5,\n",
    "                'apply_zscore': True,\n",
    "                'apply_iqr': True,\n",
    "                'apply_winsor': False,\n",
    "                'apply_isolation_forest': False\n",
    "            },\n",
    "            'choose_transformations': {\n",
    "                'method': 'power',  # 'power', 'log', or None\n",
    "                'power_method': 'yeo-johnson',\n",
    "                'skewness_threshold': 1.0\n",
    "            },\n",
    "            'encode_categoricals': {\n",
    "                'ordinal_encoding': 'OrdinalEncoder',\n",
    "                'nominal_encoding': 'OrdinalEncoder',  \n",
    "                'handle_unknown': 'ignore'\n",
    "            },\n",
    "            'apply_scaling': {\n",
    "                'method': 'StandardScaler',  \n",
    "                'features': nums\n",
    "            },\n",
    "            'implement_smote': {\n",
    "                'variant': 'SMOTENC',\n",
    "                'params': {\n",
    "                    'k_neighbors': 5\n",
    "                }\n",
    "            },\n",
    "            'inverse_transformations': {\n",
    "                'inverse_scaling': True,\n",
    "                'inverse_transformation': True,\n",
    "                'inverse_encoding': True\n",
    "            },\n",
    "            # Debug flags for each step\n",
    "            'debug_split_dataset': False,\n",
    "            'debug_handle_missing_values': False,\n",
    "            'debug_test_normality': False,\n",
    "            'debug_handle_outliers': False,\n",
    "            'debug_choose_transformations': False,\n",
    "            'debug_encode_categoricals': False,\n",
    "            'debug_apply_scaling': False,\n",
    "            'debug_implement_smote': False,\n",
    "            'debug_final_inverse_transformations': False,\n",
    "            'debug_validate_inverse_transformations': False,\n",
    "            'debug_generate_recommendations': False\n",
    "        },\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 2. TREE BASED CLASSIFIER (Classification)\n",
    "        # -----------------------------------------------------------\n",
    "        'Tree Based Classifier': {\n",
    "            'split_dataset': {\n",
    "                'test_size': 0.2,\n",
    "                'random_state': 42,\n",
    "                'stratify_for_classification': True\n",
    "            },\n",
    "            'handle_missing_values': {\n",
    "                'numerical_strategy': {\n",
    "                    'strategy': 'median',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                },\n",
    "                'categorical_strategy': {\n",
    "                    'strategy': 'most_frequent',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                }\n",
    "            },\n",
    "            'test_normality': {\n",
    "                'p_value_threshold': 0.05,\n",
    "                'skewness_threshold': 1.0,\n",
    "                'use_p_value_other_models': False\n",
    "            },\n",
    "            'handle_outliers': {\n",
    "                'zscore_threshold': 3,\n",
    "                'iqr_multiplier': 1.5,\n",
    "                'apply_zscore': False,\n",
    "                'apply_iqr': True,\n",
    "                'apply_winsor': False,\n",
    "                'apply_isolation_forest': False\n",
    "            },\n",
    "            'choose_transformations': {\n",
    "                'method': None,\n",
    "                'skewness_threshold': 1.0\n",
    "            },\n",
    "            'encode_categoricals': {\n",
    "                'ordinal_encoding': 'OrdinalEncoder',\n",
    "                'nominal_encoding': 'OrdinalEncoder',\n",
    "                'handle_unknown': 'ignore'\n",
    "            },\n",
    "            'apply_scaling': {\n",
    "                'method': None,\n",
    "                'features': []\n",
    "            },\n",
    "            'implement_smote': {\n",
    "                'variant': 'SMOTENC',\n",
    "                'params': {\n",
    "                    'k_neighbors': 5\n",
    "                }\n",
    "            },\n",
    "            'inverse_transformations': {\n",
    "                'inverse_scaling': True,\n",
    "                'inverse_transformation': True,\n",
    "                'inverse_encoding': True\n",
    "            },\n",
    "            # Debug flags\n",
    "            'debug_split_dataset': False,\n",
    "            'debug_handle_missing_values': False,\n",
    "            'debug_test_normality': False,\n",
    "            'debug_handle_outliers': False,\n",
    "            'debug_choose_transformations': False,\n",
    "            'debug_encode_categoricals': False,\n",
    "            'debug_apply_scaling': False,\n",
    "            'debug_implement_smote': False,\n",
    "            'debug_final_inverse_transformations': False,\n",
    "            'debug_validate_inverse_transformations': False,\n",
    "            'debug_generate_recommendations': False\n",
    "        },\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 3. K-MEANS CLUSTERING (Clustering)\n",
    "        # -----------------------------------------------------------\n",
    "        'K-Means Clustering': {\n",
    "            'split_dataset': {\n",
    "                'test_size': None,\n",
    "                'random_state': 42,\n",
    "                'stratify_for_classification': False\n",
    "            },\n",
    "            'handle_missing_values': {\n",
    "                'numerical_strategy': {\n",
    "                    'strategy': 'mean',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                },\n",
    "                'categorical_strategy': {\n",
    "                    'strategy': 'most_frequent',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                }\n",
    "            },\n",
    "            'test_normality': {\n",
    "                'p_value_threshold': 0.05,\n",
    "                'skewness_threshold': 1.0,\n",
    "                'use_p_value_other_models': False\n",
    "            },\n",
    "            'handle_outliers': {\n",
    "                'zscore_threshold': 3,\n",
    "                'iqr_multiplier': 1.5,\n",
    "                'apply_zscore': True,\n",
    "                'apply_iqr': True,\n",
    "                'apply_winsor': False,\n",
    "                'apply_isolation_forest': False\n",
    "            },\n",
    "            'choose_transformations': {\n",
    "                'method': 'power',\n",
    "                'power_method': 'yeo-johnson',\n",
    "                'skewness_threshold': 1.0\n",
    "            },\n",
    "            'encode_categoricals': {\n",
    "                'ordinal_encoding': 'OrdinalEncoder',\n",
    "                'nominal_encoding': 'OrdinalEncoder',\n",
    "                'handle_unknown': 'ignore'\n",
    "            },\n",
    "            'apply_scaling': {\n",
    "                'method': 'MinMaxScaler',\n",
    "                'features': nums\n",
    "            },\n",
    "            'implement_smote': {\n",
    "                'variant': None,\n",
    "                'params': {}\n",
    "            },\n",
    "            'inverse_transformations': {\n",
    "                'inverse_scaling': True,\n",
    "                'inverse_transformation': True,\n",
    "                'inverse_encoding': True\n",
    "            },\n",
    "            # Debug flags\n",
    "            'debug_split_dataset': False,\n",
    "            'debug_handle_missing_values': True,\n",
    "            'debug_test_normality': True,\n",
    "            'debug_handle_outliers': True,\n",
    "            'debug_choose_transformations': True,\n",
    "            'debug_encode_categoricals': True,\n",
    "            'debug_apply_scaling': True,\n",
    "            'debug_implement_smote': False,\n",
    "            'debug_final_inverse_transformations': True,\n",
    "            'debug_validate_inverse_transformations': True,\n",
    "            'debug_generate_recommendations': True\n",
    "        },\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 4. LINEAR REGRESSION (Regression)\n",
    "        # -----------------------------------------------------------\n",
    "        'Linear Regression': {\n",
    "            'split_dataset': {\n",
    "                'test_size': 0.2,\n",
    "                'random_state': 42,\n",
    "                'stratify_for_classification': False  # Not classification, so no stratify\n",
    "            },\n",
    "            'handle_missing_values': {\n",
    "                'numerical_strategy': {\n",
    "                    'strategy': 'mean',     # Could choose 'mean' or 'median'\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                },\n",
    "                'categorical_strategy': {\n",
    "                    'strategy': 'most_frequent',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                }\n",
    "            },\n",
    "            'test_normality': {\n",
    "                'p_value_threshold': 0.05,\n",
    "                'skewness_threshold': 1.0,\n",
    "                'use_p_value_other_models': False\n",
    "            },\n",
    "            'handle_outliers': {\n",
    "                'zscore_threshold': 3,\n",
    "                'iqr_multiplier': 1.5,\n",
    "                'apply_zscore': True,   # More typical for linear regression\n",
    "                'apply_iqr': True,\n",
    "                'apply_winsor': False,\n",
    "                'apply_isolation_forest': False\n",
    "            },\n",
    "            'choose_transformations': {\n",
    "                'method': 'power',   # 'power', 'log', or None\n",
    "                'power_method': 'yeo-johnson',\n",
    "                'skewness_threshold': 1.0\n",
    "            },\n",
    "            'encode_categoricals': {\n",
    "                'ordinal_encoding': 'OrdinalEncoder',\n",
    "                'nominal_encoding': 'OneHotEncoder',  # For linear regression, one-hot is often used\n",
    "                'handle_unknown': 'ignore'\n",
    "            },\n",
    "            'apply_scaling': {\n",
    "                'method': 'StandardScaler',  # Common for regression\n",
    "                'features': nums\n",
    "            },\n",
    "            'implement_smote': {\n",
    "                'variant': None,  # SMOTE is typically for classification, so skip here\n",
    "                'params': {}\n",
    "            },\n",
    "            'inverse_transformations': {\n",
    "                'inverse_scaling': True,\n",
    "                'inverse_transformation': True,\n",
    "                'inverse_encoding': True\n",
    "            },\n",
    "            # Debug flags\n",
    "            'debug_split_dataset': True,\n",
    "            'debug_handle_missing_values': True,\n",
    "            'debug_test_normality': True,\n",
    "            'debug_handle_outliers': True,\n",
    "            'debug_choose_transformations': True,\n",
    "            'debug_encode_categoricals': True,\n",
    "            'debug_apply_scaling': True,\n",
    "            'debug_implement_smote': False,  # Not classification\n",
    "            'debug_final_inverse_transformations': True,\n",
    "            'debug_validate_inverse_transformations': True,\n",
    "            'debug_generate_recommendations': True\n",
    "        },\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # 5. TREE BASED REGRESSOR (Regression)\n",
    "        # -----------------------------------------------------------\n",
    "        'Tree Based Regressor': {\n",
    "            'split_dataset': {\n",
    "                'test_size': 0.2,\n",
    "                'random_state': 42,\n",
    "                'stratify_for_classification': False\n",
    "            },\n",
    "            'handle_missing_values': {\n",
    "                'numerical_strategy': {\n",
    "                    'strategy': 'median',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                },\n",
    "                'categorical_strategy': {\n",
    "                    'strategy': 'most_frequent',\n",
    "                    'imputer': 'SimpleImputer'\n",
    "                }\n",
    "            },\n",
    "            'test_normality': {\n",
    "                'p_value_threshold': 0.05,\n",
    "                'skewness_threshold': 1.0,\n",
    "                'use_p_value_other_models': False\n",
    "            },\n",
    "            'handle_outliers': {\n",
    "                'zscore_threshold': 3,\n",
    "                'iqr_multiplier': 1.5,\n",
    "                'apply_zscore': False,  \n",
    "                'apply_iqr': True,\n",
    "                'apply_winsor': False,\n",
    "                'apply_isolation_forest': False\n",
    "            },\n",
    "            'choose_transformations': {\n",
    "                'method': None,   # Typically not needed for tree-based\n",
    "                'skewness_threshold': 1.0\n",
    "            },\n",
    "            'encode_categoricals': {\n",
    "                'ordinal_encoding': 'OrdinalEncoder',\n",
    "                'nominal_encoding': 'OrdinalEncoder',  \n",
    "                'handle_unknown': 'ignore'\n",
    "            },\n",
    "            'apply_scaling': {\n",
    "                'method': None,   # Trees don't need scaling\n",
    "                'features': []\n",
    "            },\n",
    "            'implement_smote': {\n",
    "                'variant': None,  # Not classification\n",
    "                'params': {}\n",
    "            },\n",
    "            'inverse_transformations': {\n",
    "                'inverse_scaling': True,\n",
    "                'inverse_transformation': True,\n",
    "                'inverse_encoding': True\n",
    "            },\n",
    "            # Debug flags\n",
    "            'debug_split_dataset': False,\n",
    "            'debug_handle_missing_values': True,\n",
    "            'debug_test_normality': True,\n",
    "            'debug_handle_outliers': True,\n",
    "            'debug_choose_transformations': True,\n",
    "            'debug_encode_categoricals': True,\n",
    "            'debug_apply_scaling': True,\n",
    "            'debug_implement_smote': False,\n",
    "            'debug_final_inverse_transformations': True,\n",
    "            'debug_validate_inverse_transformations': True,\n",
    "            'debug_generate_recommendations': True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 7: Loop Over Each Model\n",
    "    # ----------------------------\n",
    "    results = []\n",
    "    for model_type in model_types:\n",
    "        # 1) Grab the options for the model\n",
    "        options = model_specific_options.get(model_type, {})\n",
    "\n",
    "        # 2) Extract debug flags from options (to pass into the constructor).\n",
    "        debug_split_dataset = options.pop('debug_split_dataset', False)\n",
    "        debug_handle_missing_values = options.pop('debug_handle_missing_values', False)\n",
    "        debug_test_normality = options.pop('debug_test_normality', False)\n",
    "        debug_handle_outliers = options.pop('debug_handle_outliers', False)\n",
    "        debug_choose_transformations = options.pop('debug_choose_transformations', False)\n",
    "        debug_encode_categoricals = options.pop('debug_encode_categoricals', False)\n",
    "        debug_apply_scaling = options.pop('debug_apply_scaling', False)\n",
    "        debug_implement_smote = options.pop('debug_implement_smote', False)\n",
    "        debug_final_inverse_transformations = options.pop('debug_final_inverse_transformations', False)\n",
    "        debug_validate_inverse_transformations = options.pop('debug_validate_inverse_transformations', False)\n",
    "        debug_generate_recommendations = options.pop('debug_generate_recommendations', False)\n",
    "\n",
    "        # 3) Determine mode & whether we split\n",
    "        if 'Clustering' in model_type:\n",
    "            mode = 'clustering'\n",
    "            perform_split = False\n",
    "        else:\n",
    "            mode = 'train'\n",
    "            perform_split = True\n",
    "\n",
    "        # 4) Instantiate DataPreprocessor\n",
    "        preprocessor = DataPreprocessor(\n",
    "            model_type=model_type,\n",
    "            column_assets=column_assets,\n",
    "            mode=mode,\n",
    "            #options=options,  # Pass in the entire dictionary (minus the popped debug flags)\n",
    "            perform_split=perform_split,\n",
    "            debug=debug_flag,\n",
    "            debug_split_dataset=debug_split_dataset,\n",
    "            debug_handle_missing_values=debug_handle_missing_values,\n",
    "            debug_test_normality=debug_test_normality,\n",
    "            debug_handle_outliers=debug_handle_outliers,\n",
    "            debug_choose_transformations=debug_choose_transformations,\n",
    "            debug_encode_categoricals=debug_encode_categoricals,\n",
    "            debug_apply_scaling=debug_apply_scaling,\n",
    "            debug_implement_smote=debug_implement_smote,\n",
    "            debug_final_inverse_transformations=debug_final_inverse_transformations,\n",
    "            debug_validate_inverse_transformations=debug_validate_inverse_transformations,\n",
    "            debug_generate_recommendations=debug_generate_recommendations,\n",
    "            normalize_debug=normalize_debug,\n",
    "            normalize_graphs_output=normalize_graphs_output,\n",
    "            graphs_output_dir=graphs_output_dir\n",
    "        )\n",
    "\n",
    "        # 5) Execute the preprocessing pipeline\n",
    "        try:\n",
    "            if perform_split and preprocessor.y_variable:\n",
    "                # Supervised learning\n",
    "                X = filtered_df.drop(preprocessor.y_variable, axis=1)\n",
    "                y = filtered_df[preprocessor.y_variable].iloc[:, 0]\n",
    "                logger.debug(f\"y shape: {y.shape}, type: {type(y)}\")\n",
    "            elif mode == 'predict':\n",
    "                # Handle prediction (if applicable)\n",
    "                # Existing prediction handling...\n",
    "                pass\n",
    "            else:\n",
    "                # Unsupervised learning (clustering)\n",
    "                X = filtered_df.copy()\n",
    "                y = None\n",
    "\n",
    "            # Preprocess\n",
    "            preprocessed = preprocessor.final_preprocessing(X, y)\n",
    "\n",
    "            # 6) Unpack results based on model category\n",
    "            if preprocessor.mode == 'train':\n",
    "                if preprocessor.model_category in ['classification', 'regression']:\n",
    "                    X_train, X_test, y_train, y_test, recommendations = preprocessed\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported model category during training: {preprocessor.model_category}\")\n",
    "            elif preprocessor.model_category == 'clustering':\n",
    "                X_processed, recommendations = preprocessed\n",
    "            elif preprocessor.mode == 'predict':\n",
    "                # Handle prediction outputs\n",
    "                # X, y, _, _, recommendations = preprocessed\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model category: {preprocessor.model_category}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Preprocessing failed for {model_type}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 7) Show Preprocessing Recommendations\n",
    "        if debug_flag:\n",
    "            logger.debug(\"\\n📋 Preprocessing Recommendations:\")\n",
    "            logger.debug(recommendations)\n",
    "        else:\n",
    "            logger.info(\"Preprocessing Recommendations:\")\n",
    "            logger.info(recommendations)\n",
    "\n",
    "        # 8) Show dataset shapes\n",
    "        if debug_flag:\n",
    "            logger.debug(\"\\n📊 Preprocessed Dataset Shapes:\")\n",
    "        else:\n",
    "            logger.info(\"Preprocessed Dataset Shapes:\")\n",
    "\n",
    "        if mode == 'train':\n",
    "            if debug_flag:\n",
    "                logger.debug(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\")\n",
    "                logger.debug(f\"y_train shape: {y_train.shape if y_train is not None else 'N/A'}, y_test shape: {y_test.shape if y_test is not None else 'N/A'}\")\n",
    "            else:\n",
    "                logger.info(f\"X_train: {X_train.shape}, X_test: {X_test.shape if X_test is not None else 'N/A'}\")\n",
    "                logger.info(f\"y_train: {y_train.shape if y_train is not None else 'N/A'}, y_test: {y_test.shape if y_test is not None else 'N/A'}\")\n",
    "        else:\n",
    "            if debug_flag:\n",
    "                if 'X_processed' in locals():\n",
    "                    logger.debug(f\"X_processed shape: {X_processed.shape}\")\n",
    "            else:\n",
    "                if 'X_processed' in locals():\n",
    "                    logger.info(f\"X_processed: {X_processed.shape}\")\n",
    "\n",
    "        # 9) Save the preprocessed data for each model\n",
    "        try:\n",
    "            safe_model_type = model_type.replace(\" \", \"_\")\n",
    "            save_dir = os.path.join(graphs_output_dir, safe_model_type)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            if mode == 'train':\n",
    "                X_train.to_csv(os.path.join(save_dir, 'X_train.csv'), index=False)\n",
    "                if y_train is not None:\n",
    "                    y_train.to_csv(os.path.join(save_dir, 'y_train.csv'), index=False)\n",
    "\n",
    "                if X_test is not None:\n",
    "                    X_test.to_csv(os.path.join(save_dir, 'X_test.csv'), index=False)\n",
    "                if y_test is not None:\n",
    "                    y_test.to_csv(os.path.join(save_dir, 'y_test.csv'), index=False)\n",
    "            elif preprocessor.mode == 'clustering':\n",
    "                if 'X_processed' in locals():\n",
    "                    X_processed.to_csv(os.path.join(save_dir, 'X_processed.csv'), index=False)\n",
    "\n",
    "            recommendations.to_csv(os.path.join(save_dir, 'preprocessing_recommendations.csv'))\n",
    "\n",
    "            logger.info(f\"Preprocessed data saved for model '{model_type}' to '{save_dir}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save preprocessed data for model '{model_type}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # 10) (Optional) Validate Inverse Transformations\n",
    "        try:\n",
    "            inv_opts = options.get('inverse_transformations', {})\n",
    "            inverse_scaling = inv_opts.get('inverse_scaling', True)\n",
    "            inverse_transformation = inv_opts.get('inverse_transformation', True)\n",
    "            inverse_encoding = inv_opts.get('inverse_encoding', True)\n",
    "\n",
    "            # Only do if any of these are True and we have the fitted objects\n",
    "            if ((inverse_scaling and preprocessor.scaler) or\n",
    "                (inverse_transformation and preprocessor.transformer) or\n",
    "                (inverse_encoding and\n",
    "                    (preprocessor.ordinal_encoder or preprocessor.nominal_encoder))):\n",
    "\n",
    "                if mode == 'train' and X_test is not None:\n",
    "                    # Reconstruct the original subset for inverse transformation\n",
    "                    features_to_inverse = (\n",
    "                        list(preprocessor.numericals) \n",
    "                        + list(preprocessor.ordinal_categoricals) \n",
    "                        + list(preprocessor.nominal_categoricals)\n",
    "                    )\n",
    "                    X_test_original_subset = filtered_df.loc[X_test.index, features_to_inverse]\n",
    "\n",
    "                    X_test_inverse = preprocessor.final_inverse_transformations(\n",
    "                        X_test_preprocessed=X_test, \n",
    "                        X_test_original=X_test_original_subset\n",
    "                    )\n",
    "                    # Validate\n",
    "                    preprocessor.validate_inverse_transformations(\n",
    "                        X_original=X_test_original_subset,\n",
    "                        X_inverse=X_test_inverse,\n",
    "                        tolerance=1e-4\n",
    "                    )\n",
    "\n",
    "                elif mode == 'clustering':\n",
    "                    if 'X_processed' in locals():\n",
    "                        features_to_inverse = (\n",
    "                            list(preprocessor.numericals) \n",
    "                            + list(preprocessor.ordinal_categoricals) \n",
    "                            + list(preprocessor.nominal_categoricals)\n",
    "                        )\n",
    "                        X_processed_original_subset = filtered_df.loc[X_processed.index, features_to_inverse]\n",
    "                        X_processed_inverse = preprocessor.final_inverse_transformations(\n",
    "                            X_test_preprocessed=X_processed,\n",
    "                            X_test_original=X_processed_original_subset\n",
    "                        )\n",
    "                        # Validate\n",
    "                        preprocessor.validate_inverse_transformations(\n",
    "                            X_original=X_processed_original_subset,\n",
    "                            X_inverse=X_processed_inverse,\n",
    "                            tolerance=1e-4\n",
    "                        )\n",
    "\n",
    "                elif mode == 'predict':\n",
    "                    # Handle prediction-specific inverse transformations if needed\n",
    "                    pass\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Inverse transformations or validation failed for '{model_type}': {e}\")\n",
    "\n",
    "    logger.info(\"✅ All model preprocessing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
