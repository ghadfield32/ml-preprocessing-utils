{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ml/feature_selection/data_loader_post_select_features.py\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import Optional, Dict\n",
    "\n",
    "\n",
    "def load_feature_names_for_base_data(filepath: str):\n",
    "    \"\"\"\n",
    "    Load feature names from a pickle file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as file:\n",
    "        feature_names = pickle.load(file)\n",
    "    return feature_names\n",
    "\n",
    "\n",
    "def load_base_data_for_dataset(filepath: str):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "def filter_base_data_for_select_features(dataset: pd.DataFrame, feature_names: list, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Filter the dataset to include only the specified feature names.\n",
    "    \"\"\"\n",
    "    if feature_names is not None and len(feature_names) > 0:\n",
    "        # Ensure only columns present in both the DataFrame and the selected features list are retained\n",
    "        common_columns = set(dataset.columns).intersection(feature_names)\n",
    "        filtered_dataset = dataset[list(common_columns)]\n",
    "        if debug:\n",
    "            print(\"Loaded and filtered dataset based on selected features:\")\n",
    "            print(filtered_dataset.head())\n",
    "        return filtered_dataset\n",
    "    else:\n",
    "        print(\"No valid selected features found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_selected_features_data(\n",
    "    features_path: str,\n",
    "    dataset_path: str,\n",
    "    y_variable: str,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process machine learning data.\n",
    "\n",
    "    Args:\n",
    "        features_path (str): Path to the file containing feature names.\n",
    "        dataset_path (str): Path to the main dataset file.\n",
    "        y_variable (str): The target variable name.\n",
    "        debug (bool): Flag to enable detailed debugging information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset ready for further analysis.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any required step fails or invalid input is provided.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        # Load the list of selected feature names\n",
    "        logging.info(\"Loading selected features...\")\n",
    "        selected_features = load_feature_names_for_base_data(features_path)\n",
    "        length_of_features = len(selected_features)\n",
    "        logging.debug(f\"Number of Features Selected = {length_of_features}\" )\n",
    "        logging.debug(f\"Features Selected = {selected_features}\" )\n",
    "\n",
    "        # Load the dataset\n",
    "        logging.info(\"Loading dataset...\")\n",
    "        final_ml_df = load_base_data_for_dataset(dataset_path)\n",
    "\n",
    "        # Filter the DataFrame using the loaded list of selected feature names\n",
    "        logging.info(\"Filtering dataset for selected features...\")\n",
    "        final_ml_df_selected_features = filter_base_data_for_select_features(\n",
    "            final_ml_df, \n",
    "            selected_features, \n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "        if final_ml_df_selected_features is None or final_ml_df_selected_features.empty:\n",
    "            raise ValueError(\"Filtered DataFrame is empty or invalid.\")\n",
    "        \n",
    "        logging.info(\"Data processing complete. Returning processed DataFrame.\")\n",
    "        return final_ml_df_selected_features\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during data processing: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path='../../data/model/pipeline/final_ml_df_selected_features_columns.pkl',\n",
    "        dataset_path='../../data/processed/final_ml_dataset.csv',\n",
    "        y_variable='result',\n",
    "        debug=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ml/classification_preprocessor/smote_automation.py\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN, SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "# Configure root logger\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Ensure DEBUG level is enabled\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Create a module-specific logger (if not already created)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)  # Explicitly set module-specific logger to DEBUG\n",
    "\n",
    "\n",
    "def check_dataset_for_smote(\n",
    "    X_train, y_train, debug=False,\n",
    "    imbalance_threshold=0.2, noise_threshold=0.5,\n",
    "    overlap_threshold=0.3, boundary_threshold=0.4,\n",
    "    extreme_imbalance_threshold=0.05\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes a dataset to recommend the best SMOTE variant.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target labels.\n",
    "        debug (bool): Whether to log debug information.\n",
    "        imbalance_threshold (float): Threshold for severe imbalance.\n",
    "        noise_threshold (float): Threshold for noise detection.\n",
    "        overlap_threshold (float): Threshold for class overlap detection.\n",
    "        boundary_threshold (float): Threshold for boundary concentration detection.\n",
    "        extreme_imbalance_threshold (float): Threshold for extreme imbalance.\n",
    "\n",
    "    Returns:\n",
    "        dict: Recommendations for SMOTE variants and analysis details.\n",
    "    \"\"\"\n",
    "    if not isinstance(X_train, pd.DataFrame) or not isinstance(y_train, pd.Series):\n",
    "        raise TypeError(\"X_train must be a DataFrame and y_train must be a Series.\")\n",
    "\n",
    "    # Step 1: Class Distribution\n",
    "    class_distribution = y_train.value_counts(normalize=True)\n",
    "    majority_class = class_distribution.idxmax()\n",
    "    minority_class = class_distribution.idxmin()\n",
    "\n",
    "    severe_imbalance = class_distribution[minority_class] < imbalance_threshold\n",
    "    extreme_imbalance = class_distribution[minority_class] < extreme_imbalance_threshold\n",
    "\n",
    "    if debug:\n",
    "        logger.debug(f\"X_train Shape: {X_train.shape}\")\n",
    "        logger.debug(f\"Class Distribution: {class_distribution.to_dict()}\")\n",
    "        if extreme_imbalance:\n",
    "            logging.warning(f\"Extreme imbalance detected: {class_distribution[minority_class]:.2%}\")\n",
    "\n",
    "    # Step 2: Noise Analysis\n",
    "    minority_samples = X_train[y_train == minority_class]\n",
    "    majority_samples = X_train[y_train == majority_class]\n",
    "\n",
    "    try:\n",
    "        knn = NearestNeighbors(n_neighbors=5).fit(majority_samples)\n",
    "        distances, _ = knn.kneighbors(minority_samples)\n",
    "        median_distance = np.median(distances)\n",
    "        noise_ratio = np.mean(distances < median_distance)\n",
    "        noisy_data = noise_ratio > noise_threshold\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(f\"Median Distance to Nearest Neighbors: {median_distance}\")\n",
    "            logger.debug(f\"Noise Ratio: {noise_ratio:.2%}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Noise analysis error: {e}\")\n",
    "        noisy_data = False\n",
    "\n",
    "    # Step 3: Overlap Analysis\n",
    "    try:\n",
    "        pdistances = pairwise_distances(minority_samples, majority_samples)\n",
    "        overlap_metric = np.mean(pdistances < 1.0)\n",
    "        overlapping_classes = overlap_metric > overlap_threshold\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(f\"Overlap Metric: {overlap_metric:.2%}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Overlap analysis error: {e}\")\n",
    "        overlapping_classes = False\n",
    "\n",
    "    # Step 4: Boundary Concentration\n",
    "    try:\n",
    "        boundary_ratio = np.mean(np.min(distances, axis=1) < np.percentile(distances, 25))\n",
    "        boundary_concentration = boundary_ratio > boundary_threshold\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(f\"Boundary Concentration Ratio: {boundary_ratio:.2%}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Boundary concentration error: {e}\")\n",
    "        boundary_concentration = False\n",
    "\n",
    "    # Step 5: Recommendations\n",
    "    recommendations = []\n",
    "    if severe_imbalance:\n",
    "        recommendations.append(\"ADASYN\" if not noisy_data else \"SMOTEENN\")\n",
    "    if noisy_data:\n",
    "        recommendations.append(\"SMOTEENN\")\n",
    "    if overlapping_classes:\n",
    "        recommendations.append(\"SMOTETomek\")\n",
    "    if boundary_concentration:\n",
    "        recommendations.append(\"BorderlineSMOTE\")\n",
    "    if not recommendations:\n",
    "        recommendations.append(\"SMOTE\")\n",
    "\n",
    "    if debug:\n",
    "        logger.debug(\"SMOTE Analysis Complete.\")\n",
    "        logger.debug(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "    return {\n",
    "        \"recommendations\": recommendations,\n",
    "        \"details\": {\n",
    "            \"severe_imbalance\": severe_imbalance,\n",
    "            \"noisy_data\": noisy_data,\n",
    "            \"overlapping_classes\": overlapping_classes,\n",
    "            \"boundary_concentration\": boundary_concentration\n",
    "        }\n",
    "    }\n",
    "\n",
    "def apply_smote(X_train, y_train, recommendations, debug=False, smote_params=None):\n",
    "    \"\"\"\n",
    "    Applies the recommended SMOTE variant to the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training target labels.\n",
    "        recommendations (list or str): Recommended SMOTE variants or a single SMOTE variant.\n",
    "        debug (bool): Whether to log debug information.\n",
    "        smote_params (dict): Parameters for SMOTE variants.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.Series: Resampled features and target labels.\n",
    "        str: The applied SMOTE technique.\n",
    "    \"\"\"\n",
    "    if smote_params is None:\n",
    "        smote_params = {\"random_state\": 42}\n",
    "\n",
    "    # Define supported SMOTE variants\n",
    "    smote_variants = {\n",
    "        \"SMOTE\": SMOTE(**smote_params),\n",
    "        \"ADASYN\": ADASYN(**smote_params),\n",
    "        \"BorderlineSMOTE\": BorderlineSMOTE(**smote_params),\n",
    "        \"SMOTEENN\": SMOTEENN(**smote_params),\n",
    "        \"SMOTETomek\": SMOTETomek(**smote_params)\n",
    "    }\n",
    "\n",
    "    # Determine SMOTE technique\n",
    "    if isinstance(recommendations, list):\n",
    "        if len(recommendations) == 0:\n",
    "            logging.warning(\"Empty SMOTE recommendations. Skipping SMOTE.\")\n",
    "            return X_train, y_train, None\n",
    "        elif len(recommendations) == 1:\n",
    "            smote_technique = recommendations[0]\n",
    "        else:\n",
    "            logging.info(\"Multiple SMOTE variants recommended. Choosing the first.\")\n",
    "            smote_technique = recommendations[0]\n",
    "    elif isinstance(recommendations, str):\n",
    "        smote_technique = recommendations\n",
    "    else:\n",
    "        logging.error(\"Recommendations must be a list or string.\")\n",
    "        raise ValueError(\"Recommendations must be a list or string.\")\n",
    "\n",
    "    logger.debug(f\"SMOTE Technique Requested: {smote_technique}\")\n",
    "    logger.debug(f\"Available SMOTE Variants: {list(smote_variants.keys())}\")\n",
    "\n",
    "    # Ensure the technique exists\n",
    "    if smote_technique not in smote_variants:\n",
    "        logging.error(f\"SMOTE variant '{smote_technique}' is not recognized. Available variants: {list(smote_variants.keys())}\")\n",
    "        raise KeyError(f\"SMOTE variant '{smote_technique}' is not recognized.\")\n",
    "\n",
    "    smote_instance = smote_variants[smote_technique]\n",
    "    X_resampled, y_resampled = smote_instance.fit_resample(X_train, y_train)\n",
    "\n",
    "    if debug:\n",
    "        logger.debug(f\"Applied SMOTE Technique: {smote_technique}\")\n",
    "        logger.debug(f\"Original X_train Shape: {X_train.shape}\")\n",
    "        logger.debug(f\"Resampled X_train Shape: {X_resampled.shape}\")\n",
    "        logger.debug(f\"Original Class Distribution: {Counter(y_train)}\")\n",
    "        logger.debug(f\"Resampled Class Distribution: {Counter(y_resampled)}\")\n",
    "\n",
    "    return X_resampled, y_resampled, smote_technique\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "    y_variable = \"result\"\n",
    "    debug = False\n",
    "    from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "    # Example usage:\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path='../../data/model/pipeline/final_ml_df_selected_features_columns.pkl',\n",
    "        dataset_path='../../data/processed/final_ml_dataset.csv',\n",
    "        y_variable='result',\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Assuming numerical_info_df, categorical_info_df, and final_ml_df_selected_features are already defined\n",
    "    y_variable = 'result'\n",
    "\n",
    "    print(\"\\n[Initial Dataset Info]\")\n",
    "    print(f\"Columns to work with: {final_ml_df_selected_features.columns.tolist()}\")\n",
    "\n",
    "    # Step 1: Split dataset into features (X) and target (y)\n",
    "    X = final_ml_df_selected_features.drop(columns=[y_variable])\n",
    "    y = final_ml_df_selected_features[y_variable]\n",
    "\n",
    "    # Step 2: Train-test split\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Apply scaling based on suggestions\n",
    "    scaler_standard = StandardScaler()\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "\n",
    "    # Features requiring StandardScaler\n",
    "    standard_features = [\n",
    "        'release_ball_velocity_z', 'knee_release_angle', 'wrist_release_angle',\n",
    "        'knee_max_angle', 'release_ball_direction_z', 'wrist_max_angle'\n",
    "    ]\n",
    "\n",
    "    # Features requiring MinMaxScaler\n",
    "    minmax_features = [\n",
    "        'elbow_max_angle', 'elbow_release_angle', 'release_ball_direction_y',\n",
    "        'release_ball_speed', 'release_ball_direction_x', 'release_ball_velocity_x',\n",
    "        'release_ball_velocity_y', 'calculated_release_angle'\n",
    "    ]\n",
    "\n",
    "    # Apply StandardScaler\n",
    "    X_train_standard = scaler_standard.fit_transform(X_train[standard_features])\n",
    "    X_test_standard = scaler_standard.transform(X_test[standard_features])\n",
    "\n",
    "    # Apply MinMaxScaler\n",
    "    X_train_minmax = scaler_minmax.fit_transform(X_train[minmax_features])\n",
    "    X_test_minmax = scaler_minmax.transform(X_test[minmax_features])\n",
    "\n",
    "    # Combine scaled features\n",
    "    import pandas as pd\n",
    "    # Combine scaled features with aligned index\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        data=np.hstack((X_train_standard, X_train_minmax)),\n",
    "        columns=standard_features + minmax_features,\n",
    "        index=X_train.index  # Preserve the original index\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        data=np.hstack((X_test_standard, X_test_minmax)),\n",
    "        columns=standard_features + minmax_features,\n",
    "        index=X_test.index  # Preserve the original index\n",
    "    )\n",
    "\n",
    "\n",
    "    # add in SMOTE TO TRAINING DATASETS ONLY \n",
    "\n",
    "    # from smote_automation import  check_dataset_for_smote, apply_smote\n",
    "\n",
    "    # Analyze dataset for SMOTE\n",
    "    smote_analysis = check_dataset_for_smote(X_train_scaled, y_train, debug=True)\n",
    "    print(\"SMOTE Analysis Recommendations:\", smote_analysis[\"recommendations\"])\n",
    "\n",
    "    # Apply SMOTE\n",
    "    X_resampled, y_resampled, smote_used = apply_smote(X_train, \n",
    "                                                       y_train, \n",
    "                                                       \"SMOTEENN\", # Can also select individual: ADASYN, SMOTEENN, SMOTETomek, BorderlineSMOTE, and SMOTE\n",
    "                                                       debug=True)\n",
    "    print(\"Applied SMOTE Variant:\", smote_used)\n",
    "    print(\"Resampled Class Distribution:\", Counter(y_resampled))\n",
    "\n",
    "    logging.info(f\"SMOTE Technique Used: {smote_used}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%writefile ../../src/freethrow_predictions/ml/classification_preprocessor/preprocessor_encoding_filtered_datasets.py\n",
    "# Filter into different Preprocessing/Encoding Datasets for Automated Preprocessing\n",
    "\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "from ml.classification_preprocessor.smote_automation import check_dataset_for_smote, apply_smote\n",
    "import pandas as pd\n",
    "\n",
    "def filter_features(numerical_info_df, categorical_info_df, dataset, y_variable, debug=False):\n",
    "    \"\"\"\n",
    "    Filters features based on the analysis and returns lists for each processing type, excluding the target variable.\n",
    "    \"\"\"\n",
    "    # Type checks for input DataFrames\n",
    "    if not isinstance(numerical_info_df, pd.DataFrame):\n",
    "        raise TypeError(\"numerical_info_df must be a pandas DataFrame\")\n",
    "    if not isinstance(categorical_info_df, pd.DataFrame):\n",
    "        raise TypeError(\"categorical_info_df must be a pandas DataFrame\")\n",
    "    if not isinstance(dataset, pd.DataFrame):\n",
    "        raise TypeError(\"dataset must be a pandas DataFrame\")\n",
    "    if y_variable not in dataset.columns:\n",
    "        raise ValueError(f\"Target variable '{y_variable}' not found in the dataset.\")\n",
    "\n",
    "    # Exclude the target variable from the dataset columns\n",
    "    dataset_features = set(dataset.columns) - {y_variable}\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[Debug] Dataset Features: {dataset_features}\")\n",
    "\n",
    "    # Filter numerical features\n",
    "    numerical_scaler_features = numerical_info_df[\n",
    "        numerical_info_df['Preprocessing Suggestion'].str.contains('StandardScaler', na=False)\n",
    "    ]['Feature'].tolist()\n",
    "\n",
    "    numerical_minmax_features = numerical_info_df[\n",
    "        numerical_info_df['Preprocessing Suggestion'].str.contains('MinMaxScaler', na=False)\n",
    "    ]['Feature'].tolist()\n",
    "\n",
    "    numerical_kbins_features = numerical_info_df[\n",
    "        numerical_info_df['Preprocessing Suggestion'].str.contains('KBinsDiscretizer', na=False)\n",
    "    ]['Feature'].tolist()\n",
    "\n",
    "    numerical_dimred_features = numerical_info_df[\n",
    "        numerical_info_df['Preprocessing Suggestion'].str.contains('Dimensionality Reduction', na=False)\n",
    "    ]['Feature'].tolist()\n",
    "\n",
    "    # Filter categorical features\n",
    "    categorical_info_df = categorical_info_df[categorical_info_df['Feature'] != y_variable]\n",
    "    categorical_onehot_features = categorical_info_df[\n",
    "        categorical_info_df['Encoding Suggestion'].str.contains('OneHotEncoder', na=False)\n",
    "    ]['Feature'].tolist()\n",
    "\n",
    "    categorical_labelencode_features = categorical_info_df[\n",
    "        categorical_info_df['Encoding Suggestion'].str.contains('LabelEncoder', na=False)\n",
    "    ]['Feature'].tolist()\n",
    "\n",
    "    # Debug: Identify missing preprocessing\n",
    "    all_preprocessed_features = (\n",
    "        numerical_scaler_features +\n",
    "        numerical_minmax_features +\n",
    "        numerical_kbins_features +\n",
    "        numerical_dimred_features +\n",
    "        categorical_onehot_features +\n",
    "        categorical_labelencode_features\n",
    "    )\n",
    "    missing_preprocessing = set(numerical_info_df['Feature']) - set(all_preprocessed_features)\n",
    "    if debug and missing_preprocessing:\n",
    "        print(f\"[Debug] Features missing preprocessing suggestions: {missing_preprocessing}\")\n",
    "\n",
    "    # Identify all processed features\n",
    "    processed_features = set(all_preprocessed_features)\n",
    "\n",
    "    # Identify unprocessed features\n",
    "    unprocessed_features = dataset_features - processed_features\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[Debug] Processed Features: {processed_features}\")\n",
    "        print(f\"[Debug] Unprocessed Features: {unprocessed_features}\")\n",
    "\n",
    "    # Validate feature split\n",
    "    if len(processed_features | unprocessed_features) != len(dataset_features):\n",
    "        raise ValueError(\"Filtered features do not match dataset columns!\")\n",
    "\n",
    "    return (\n",
    "        numerical_scaler_features,\n",
    "        numerical_minmax_features,  # Added MinMaxScaler\n",
    "        numerical_kbins_features,\n",
    "        numerical_dimred_features,\n",
    "        categorical_onehot_features,\n",
    "        categorical_labelencode_features,\n",
    "        list(unprocessed_features)\n",
    "    )\n",
    "\n",
    "\n",
    "def check_on_processed_datasets(\n",
    "    dataset,\n",
    "    numerical_scaler_features,\n",
    "    numerical_minmax_features,  # Include MinMax features\n",
    "    numerical_kbins_features,\n",
    "    numerical_dimred_features,\n",
    "    categorical_onehot_features,\n",
    "    categorical_labelencode_features,\n",
    "    unprocessed_features,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the dataset into subsets based on the filtered features, excluding the target variable.\n",
    "    \"\"\"\n",
    "    # Ensure all feature subsets are valid\n",
    "    numerical_scaler_data = dataset[numerical_scaler_features]\n",
    "    numerical_minmax_data = dataset[numerical_minmax_features]  # Added MinMaxScaler group\n",
    "    numerical_kbins_data = dataset[numerical_kbins_features]\n",
    "    numerical_dimred_data = dataset[numerical_dimred_features]\n",
    "    onehot_data = dataset[categorical_onehot_features]\n",
    "    labelencode_data = dataset[categorical_labelencode_features]\n",
    "    unprocessed_data = dataset[unprocessed_features]\n",
    "\n",
    "    # Validate counts\n",
    "    total_features_count = (\n",
    "        len(numerical_scaler_features) +\n",
    "        len(numerical_minmax_features) +  # Include MinMaxScaler features\n",
    "        len(numerical_kbins_features) +\n",
    "        len(numerical_dimred_features) +\n",
    "        len(categorical_onehot_features) +\n",
    "        len(categorical_labelencode_features) +\n",
    "        len(unprocessed_features)\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(\"[Debug] Processed Data Summary:\")\n",
    "        print(f\"  Numerical Scaler Data Shape: {numerical_scaler_data.shape}\")\n",
    "        print(f\"  Numerical MinMax Data Shape: {numerical_minmax_data.shape}\")  # Debug MinMaxScaler\n",
    "        print(f\"  Numerical KBins Data Shape: {numerical_kbins_data.shape}\")\n",
    "        print(f\"  Numerical DimRed Data Shape: {numerical_dimred_data.shape}\")\n",
    "        print(f\"  OneHot Data Shape: {onehot_data.shape}\")\n",
    "        print(f\"  LabelEncode Data Shape: {labelencode_data.shape}\")\n",
    "        print(f\"  Unprocessed Data Shape: {unprocessed_data.shape}\")\n",
    "        print(f\"  Total Processed Features: {total_features_count}\")\n",
    "        print(f\"  Original Dataset Features (Excluding Target): {dataset.shape[1]}\")\n",
    "\n",
    "    if total_features_count != dataset.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Feature count mismatch! Processed: {total_features_count}, Original: {dataset.shape[1]}\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'numerical_scaler_data': numerical_scaler_data,\n",
    "        'numerical_minmax_data': numerical_minmax_data,  # Added MinMaxScaler data\n",
    "        'numerical_kbins_data': numerical_kbins_data,\n",
    "        'numerical_dimred_data': numerical_dimred_data,\n",
    "        'onehot_data': onehot_data,\n",
    "        'labelencode_data': labelencode_data,\n",
    "        'unprocessed_data': unprocessed_data\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # from preprocessor_recommendations import filter_features_by_type, analyze_categorical_features, analyze_numerical_features_enhanced_v2, \n",
    "    # from preprocessor_encoding_filtered_datasets import filter_features, check_on_processed_datasets\n",
    "    debug = True\n",
    "    # Example parameter tuning\n",
    "    zscore_threshold = 3\n",
    "    tukey_threshold = 1.5\n",
    "    max_rows_shapiro = 5000\n",
    "    min_rows_normality_percentage = 0.05\n",
    "    high_outlier_percentage = 5\n",
    "    correlation_threshold = 0.8  # Threshold for multicollinearity check\n",
    "\n",
    "    # File paths\n",
    "    features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "    dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    \n",
    "    # from data_loader_post_select_features import load_selected_features_data\n",
    "    # Example usage:\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        y_variable='result',\n",
    "        debug=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Step 1: Filter features\n",
    "    categorical_df, numerical_df = filter_features_by_type(final_ml_df_selected_features, debug=debug)\n",
    "\n",
    "    # Step 2: Analyze categorical features\n",
    "    categorical_info_df = analyze_categorical_features(\n",
    "        categorical_df,\n",
    "        low_cardinality_threshold=10,\n",
    "        high_cardinality_threshold=50,\n",
    "        missing_threshold=0.3,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Step 3: Analyze numerical features and handle outliers automatically\n",
    "    numerical_info_df = analyze_numerical_features_enhanced_v2(\n",
    "        numerical_df,\n",
    "        y_feature=None,\n",
    "        zscore_threshold=zscore_threshold,\n",
    "        tukey_threshold=tukey_threshold,\n",
    "        missing_threshold=0.5,\n",
    "        high_cardinality_threshold=1000,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCategorical Features Analysis:\")\n",
    "    print(categorical_info_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\nNumerical Features Analysis:\")\n",
    "    print(numerical_info_df.to_string(index=False))\n",
    "\n",
    "\n",
    "    # Assuming numerical_info_df, categorical_info_df, and final_ml_df_selected_features are already defined\n",
    "    y_variable = 'result'\n",
    "    print(\"columns to work with =\", final_ml_df_selected_features.columns)\n",
    "    print(\"categorical_info_df columns to work with =\", categorical_info_df['Feature'])\n",
    "    print(\"numerical_info_df columns to work with =\", numerical_info_df['Feature'])\n",
    "\n",
    "    # Step 1: Filter features\n",
    "    (\n",
    "        numerical_scaler_features,\n",
    "        numerical_minmax_features,  # Added MinMaxScaler\n",
    "        numerical_kbins_features,\n",
    "        numerical_dimred_features,\n",
    "        onehot_features,\n",
    "        labelencode_features,\n",
    "        unprocessed_features\n",
    "    ) = filter_features(\n",
    "        numerical_info_df, categorical_info_df, final_ml_df_selected_features, y_variable, debug=debug\n",
    "    )\n",
    "\n",
    "    # Debugging outputs\n",
    "    print(f\"[Debug] Numerical Scaler Features: {numerical_scaler_features}\")\n",
    "    print(f\"[Debug] Numerical MinMax Features: {numerical_minmax_features}\")\n",
    "    print(f\"[Debug] Unprocessed Features: {unprocessed_features}\")\n",
    "\n",
    "    # Step 2: Process data, ensuring target variable is excluded\n",
    "    checks_on_processed_datasets = check_on_processed_datasets(\n",
    "        final_ml_df_selected_features.drop(columns=[y_variable]),  # Exclude target\n",
    "        numerical_scaler_features,\n",
    "        numerical_minmax_features,  # Added MinMaxScaler\n",
    "        numerical_kbins_features,\n",
    "        numerical_dimred_features,\n",
    "        onehot_features,\n",
    "        labelencode_features,\n",
    "        unprocessed_features,  # Ensure this argument is passed\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n[Final Validation] Processed Data Shapes:\")\n",
    "    for key, df in checks_on_processed_datasets.items():\n",
    "        print(f\"  {key}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../../src/freethrow_predictions/ml/classification_preprocessor/datapreprocessor_class.py\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from data_loader_post_select_features import load_feature_names, load_dataset, filter_base_data_for_select_features\n",
    "# from preprocessor_recommendations import filter_features_by_type, analyze_categorical_features, analyze_numerical_features_enhanced_v2, \n",
    "# from preprocessor_encoding_filtered_datasets import filter_features, check_on_processed_datasets\n",
    "    # from preprocessor import (\n",
    "    #     create_numerical_scaler_pipeline, \n",
    "    #     create_numerical_minmax_pipeline, \n",
    "    #     create_numerical_kbins_pipeline, \n",
    "    #     create_categorical_onehot_pipeline, \n",
    "    #     create_categorical_labelencode_pipeline, \n",
    "    #     create_numerical_dimred_pipeline, \n",
    "    #     process_feature_groups, \n",
    "    #     preprocess_target, \n",
    "    #     combine_transformed_data_with_index,\n",
    "    #      save_pipeline_and_assets\n",
    "    # )\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "from ml.classification_preprocessor.smote_automation import check_dataset_for_smote, apply_smote\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    DataPreprocessor handles the preprocessing of the dataset, including feature transformations,\n",
    "    handling train-test splits, applying SMOTE, and encoding the target variable.\n",
    "\n",
    "    Attributes:\n",
    "        features_path (str): Path to the selected features pickle file.\n",
    "        dataset_path (str): Path to the processed dataset CSV.\n",
    "        assets_path (str): Path to save preprocessing assets.\n",
    "        y_variable (str): The target variable name.\n",
    "        optimization_columns (list): Columns to consider for optimization ranges.\n",
    "        debug (bool): If True, enables detailed logging and validation outputs.\n",
    "        preprocessor_train_test_split (bool): Whether to perform a train-test split.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        stratify (bool): Whether to stratify the split based on the target variable.\n",
    "        specific_smote (str or None): SMOTE variant to apply ('SMOTE', 'ADASYN', 'recommended', or None).\n",
    "        original_index (pd.Index): The original index of the dataset before preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_path: str,\n",
    "        dataset_path: str,\n",
    "        assets_path: str,\n",
    "        y_variable: str = 'result',\n",
    "        optimization_columns: list = None,  # New parameter\n",
    "        debug: bool = True,\n",
    "        preprocessor_train_test_split: bool = False,\n",
    "        test_size: float = 0.2,\n",
    "        random_state: int = 42,\n",
    "        stratify: bool = True,\n",
    "        specific_smote: str = 'recommended',  # New parameter\n",
    "    ) -> None:\n",
    "        # Initialization parameters\n",
    "        self.features_path = features_path\n",
    "        self.dataset_path = dataset_path\n",
    "        self.assets_path = assets_path\n",
    "        self.y_variable = y_variable\n",
    "        self.debug = debug\n",
    "\n",
    "        # Parameters for train-test split\n",
    "        self.preprocessor_train_test_split = preprocessor_train_test_split\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.stratify = stratify\n",
    "\n",
    "        # SMOTE handling parameter\n",
    "        self.specific_smote = specific_smote  # Added SMOTE parameter\n",
    "\n",
    "        # Optimization-specific parameters\n",
    "        self.optimization_columns = optimization_columns if optimization_columns else []\n",
    "        self.optimization_ranges = None  # Lazy initialization\n",
    "\n",
    "        # Define parameters used in analysis functions\n",
    "        self.zscore_threshold = 3\n",
    "        self.tukey_threshold = 1.5\n",
    "        self.high_cardinality_threshold = 1000\n",
    "        self.missing_threshold = 0.5\n",
    "        self.low_cardinality_threshold = 10\n",
    "        self.high_cardinality_threshold_categorical = 50\n",
    "\n",
    "        self.original_index = None  # Initialize original_index\n",
    "\n",
    "    def _load_original_dataset(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            original_df = pd.read_csv(self.dataset_path)  # Placeholder\n",
    "            self.original_data = original_df  # Assign to class attribute\n",
    "            if self.debug:\n",
    "                logger.debug(f\"Original dataset loaded with index:\\n{original_df.index}\")\n",
    "            return original_df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading original dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the data using the new data loading function.\n",
    "        \"\"\"\n",
    "        self.filtered_data = load_selected_features_data(\n",
    "            features_path=self.features_path,\n",
    "            dataset_path=self.dataset_path,\n",
    "            y_variable=self.y_variable,\n",
    "            debug=self.debug,\n",
    "        )\n",
    "        self.original_df = self._load_original_dataset()\n",
    "        self.original_index = self.original_df.index  # Store original index\n",
    "        if self.debug:\n",
    "            logger.debug(f\"Original index in DataPreprocessor:\\n{self.original_index}\")\n",
    "        print(\"Selected features loaded and data processing complete.\")\n",
    "        \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"\n",
    "        Generic preprocessing method for dataset, \n",
    "        reusable for train-test split and full dataset.\n",
    "        \"\"\"\n",
    "        # Step 1: Filter features by type\n",
    "        categorical_df, numerical_df = filter_features_by_type(data, debug=self.debug)\n",
    "\n",
    "        # Step 2: Analyze categorical features\n",
    "        categorical_info = analyze_categorical_features(\n",
    "            categorical_df,\n",
    "            low_cardinality_threshold=self.low_cardinality_threshold,\n",
    "            high_cardinality_threshold=self.high_cardinality_threshold_categorical,\n",
    "            missing_threshold=self.missing_threshold,\n",
    "            debug=self.debug,\n",
    "        )\n",
    "\n",
    "        # Step 3: Analyze numerical features\n",
    "        numerical_info = analyze_numerical_features_enhanced_v2(\n",
    "            numerical_df,\n",
    "            y_feature=None,\n",
    "            zscore_threshold=self.zscore_threshold,\n",
    "            tukey_threshold=self.tukey_threshold,\n",
    "            missing_threshold=self.missing_threshold,\n",
    "            high_cardinality_threshold=self.high_cardinality_threshold,\n",
    "            debug=self.debug,\n",
    "        )\n",
    "\n",
    "        # Step 4: Filter features for processing\n",
    "        (\n",
    "            numerical_scaler_features,\n",
    "            numerical_minmax_features,\n",
    "            numerical_kbins_features,\n",
    "            numerical_dimred_features,\n",
    "            onehot_features,\n",
    "            labelencode_features,\n",
    "            unprocessed_features,\n",
    "        ) = filter_features(\n",
    "            numerical_info, categorical_info, data, y_variable=self.y_variable, debug=self.debug\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            numerical_scaler_features,\n",
    "            numerical_minmax_features,\n",
    "            numerical_kbins_features,\n",
    "            numerical_dimred_features,\n",
    "            onehot_features,\n",
    "            labelencode_features,\n",
    "            unprocessed_features,\n",
    "        )\n",
    "        \n",
    "    def _combine_transformed_data(self, transformed_data_dict, original_index):\n",
    "        \"\"\"\n",
    "        Combine transformed feature arrays into a single DataFrame with prefixed column names.\n",
    "\n",
    "        Args:\n",
    "            transformed_data_dict (dict): Dictionary of transformed feature arrays.\n",
    "            original_index (pd.Index): Original index to align the combined DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Unified DataFrame with all transformed features.\n",
    "        \"\"\"\n",
    "        combined_data = {}\n",
    "        for group_name, array in transformed_data_dict.items():\n",
    "            if array is None:\n",
    "                logger.warning(f\"[Combine Transformed Data] Transformed data for group '{group_name}' is None. Filling with empty array.\")\n",
    "                array = np.zeros((len(original_index), 0))  # Fallback to an empty array\n",
    "            group_df = pd.DataFrame(array, columns=[f\"{group_name}_{i}\" for i in range(array.shape[1])])\n",
    "            combined_data[group_name] = group_df\n",
    "\n",
    "        combined_df = pd.concat(combined_data.values(), axis=1)\n",
    "        combined_df.index = original_index\n",
    "        if self.debug:\n",
    "            logger.debug(\"\\n[Combine Transformed Data]\")\n",
    "            logger.debug(f\"Combined Shape: {combined_df.shape}\")\n",
    "            logger.debug(f\"Combined Columns: {list(combined_df.columns)}[:10]...\")  # Show first 10 columns\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def process_feature_groups(self, data, use_existing_pipelines=False):\n",
    "        if use_existing_pipelines:\n",
    "            transformed_data, _, _ = process_feature_groups(\n",
    "                data,\n",
    "                self.numerical_scaler_features,\n",
    "                self.numerical_minmax_features,\n",
    "                self.numerical_kbins_features,\n",
    "                self.numerical_dimred_features,\n",
    "                self.onehot_features,\n",
    "                self.labelencode_features,\n",
    "                self.unprocessed_features,\n",
    "                fitted_pipelines=self.fitted_pipelines,\n",
    "                debug=self.debug,\n",
    "            )\n",
    "        else:\n",
    "            transformed_data, self.fitted_pipelines, self.feature_indices = process_feature_groups(\n",
    "                data,\n",
    "                self.numerical_scaler_features,\n",
    "                self.numerical_minmax_features,\n",
    "                self.numerical_kbins_features,\n",
    "                self.numerical_dimred_features,\n",
    "                self.onehot_features,\n",
    "                self.labelencode_features,\n",
    "                self.unprocessed_features,\n",
    "                debug=self.debug,\n",
    "            )\n",
    "        \n",
    "        # Generate flattened feature indices\n",
    "        self.flattened_feature_indices = {}\n",
    "        start_idx = 0\n",
    "        for group_name, array in transformed_data.items():\n",
    "            if array is None or array.shape[1] == 0:\n",
    "                logger.warning(f\"[Process Feature Groups] Transformed data for group '{group_name}' is empty or None. Skipping.\")\n",
    "                continue\n",
    "            if group_name not in self.feature_indices or self.feature_indices[group_name] is None:\n",
    "                logger.warning(f\"[Process Feature Groups] Feature indices for group '{group_name}' are missing. Skipping.\")\n",
    "                continue\n",
    "            num_features = array.shape[1]\n",
    "            feature_names = self.feature_indices[group_name]['feature_names']\n",
    "            for i in range(num_features):\n",
    "                feature_name = feature_names[i]\n",
    "                self.flattened_feature_indices[start_idx] = feature_name\n",
    "                start_idx += 1\n",
    "\n",
    "        # Create reverse mapping from feature name to index\n",
    "        self.feature_name_to_index = {feature_name: index for index, feature_name in self.flattened_feature_indices.items()}\n",
    "        \n",
    "        transformed_data_df = self._combine_transformed_data(transformed_data, data.index)\n",
    "        return transformed_data, transformed_data_df\n",
    "\n",
    "    def finalize_data(self, transformed_data, original_data):\n",
    "        \"\"\"\n",
    "        Finalizes transformed data by combining it with the original index.\n",
    "        \"\"\"\n",
    "        combined_df = combine_transformed_data_with_index(\n",
    "            transformed_data, self.feature_indices, original_data.index, debug=self.debug\n",
    "        )\n",
    "        return combined_df\n",
    "\n",
    "    def preprocess_target(self, y_train, y_test=None):\n",
    "        y_train_encoded, y_test_encoded, label_encoder = preprocess_target(\n",
    "            y_train, y_test, debug=self.debug\n",
    "        )\n",
    "        return y_train_encoded, y_test_encoded, label_encoder\n",
    "\n",
    "    def save_assets(self):\n",
    "        if not hasattr(self, \"fitted_pipelines\") or not hasattr(self, \"feature_indices\"):\n",
    "            raise RuntimeError(\n",
    "                \"Assets are not available for saving. Ensure preprocessing is complete before saving.\"\n",
    "            )\n",
    "        assets = {\n",
    "            'fitted_pipelines': self.fitted_pipelines,\n",
    "            'feature_indices': self.feature_indices,\n",
    "            'flattened_feature_indices': self.flattened_feature_indices,\n",
    "            'feature_name_to_index': self.feature_name_to_index,  # Add this line\n",
    "            'label_encoder': self.label_encoder,\n",
    "        }\n",
    "        with open(self.assets_path, 'wb') as f:\n",
    "            pickle.dump(assets, f)\n",
    "        logger.debug(f\"Pipeline and assets saved to {self.assets_path}\")\n",
    "\n",
    "    def extract_transformed_optimization_ranges(self):\n",
    "        \"\"\"\n",
    "        Extract min and max ranges for the optimization columns from the transformed dataset.\n",
    "        \"\"\"\n",
    "        if not self.optimization_columns:\n",
    "            raise ValueError(\"No optimization columns specified.\")\n",
    "        \n",
    "        self.optimization_transformed_ranges = {}\n",
    "        for col in self.optimization_columns:\n",
    "            if col not in self.feature_name_to_index:\n",
    "                logger.warning(f\"Optimization column '{col}' not found in feature indices.\")\n",
    "                continue\n",
    "            col_index = self.feature_name_to_index[col]\n",
    "            transformed_values = self.X_transformed.iloc[:, col_index]\n",
    "            \n",
    "            # Check if transformed_values is valid\n",
    "            if transformed_values.empty:\n",
    "                logger.warning(f\"No values found for column '{col}'.\")\n",
    "                continue\n",
    "\n",
    "            self.optimization_transformed_ranges[col] = (transformed_values.min(), transformed_values.max())\n",
    "        \n",
    "        return self.optimization_transformed_ranges\n",
    "\n",
    "    def _preprocess_and_transform(self, X_train, X_test=None):\n",
    "        # Preprocess features\n",
    "        (\n",
    "            self.numerical_scaler_features,\n",
    "            self.numerical_minmax_features,\n",
    "            self.numerical_kbins_features,\n",
    "            self.numerical_dimred_features,\n",
    "            self.onehot_features,\n",
    "            self.labelencode_features,\n",
    "            self.unprocessed_features,\n",
    "        ) = self.preprocess_data(self.filtered_data)\n",
    "\n",
    "        # Process and finalize training data\n",
    "        self.transformed_train, self.transformed_train_df = self.process_feature_groups(X_train)\n",
    "        self.X_train_transformed = self.finalize_data(\n",
    "            self.transformed_train, X_train\n",
    "        )\n",
    "\n",
    "        if self.debug:\n",
    "            logger.debug(f\"X_train_transformed index:\\n{self.X_train_transformed.index}\")\n",
    "            logger.debug(f\"Original X_train index:\\n{X_train.index}\")\n",
    "\n",
    "        if X_test is not None:\n",
    "            # Process and finalize testing data\n",
    "            self.transformed_test, self.transformed_test_df = self.process_feature_groups(\n",
    "                X_test, use_existing_pipelines=True\n",
    "            )\n",
    "            self.X_test_transformed = self.finalize_data(\n",
    "                self.transformed_test, X_test\n",
    "            )\n",
    "            if self.debug:\n",
    "                logger.debug(f\"X_test_transformed index:\\n{self.X_test_transformed.index}\")\n",
    "                logger.debug(f\"Original X_test index:\\n{X_test.index}\")\n",
    "        else:\n",
    "            # Process and finalize testing data\n",
    "            self.transformed_data, self.transformed_df = self.process_feature_groups(\n",
    "                X_train, use_existing_pipelines=True\n",
    "            )\n",
    "            self.X_transformed = self.finalize_data(\n",
    "                self.transformed_train, X_train\n",
    "            )\n",
    "            if self.debug:\n",
    "                logger.debug(f\"X_transformed index:\\n{self.X_transformed.index}\")\n",
    "                logger.debug(f\"Original X index:\\n{X_train.index}\")\n",
    "\n",
    "    def _preprocess_target(self, y_train, y_test=None):\n",
    "        self.y_train_encoded, self.y_test_encoded, self.label_encoder = self.preprocess_target(\n",
    "            y_train, y_test\n",
    "        )\n",
    "        if not self.preprocessor_train_test_split:\n",
    "            # In the no-split case, assign y_encoded for consistency\n",
    "            self.y_encoded = self.y_train_encoded\n",
    "\n",
    "    def extract_optimization_ranges(self):\n",
    "        \"\"\"\n",
    "        Extract min and max ranges for the optimization columns from the original dataset.\n",
    "        \"\"\"\n",
    "        if not self.optimization_columns:\n",
    "            raise ValueError(\"No optimization columns specified.\")\n",
    "        \n",
    "        self.optimization_ranges = {}\n",
    "        for col in self.optimization_columns:\n",
    "            if col not in self.filtered_data.columns:\n",
    "                logger.warning(f\"Optimization column '{col}' not found in the dataset.\")\n",
    "                continue\n",
    "            values = self.filtered_data[col]\n",
    "            \n",
    "            # Check if values is valid\n",
    "            if values.empty:\n",
    "                logger.warning(f\"No values found for column '{col}'.\")\n",
    "                continue\n",
    "\n",
    "            self.optimization_ranges[col] = (values.min(), values.max())\n",
    "        \n",
    "        return self.optimization_ranges\n",
    "\n",
    "    def transform_new_data(self, input_df):\n",
    "        \"\"\"\n",
    "        Transform new optimization data using the preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            input_df (pd.DataFrame): DataFrame with new data to transform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Transformed data.\n",
    "        \"\"\"\n",
    "        # Ensure optimization columns are present\n",
    "        missing_features = set(self.optimization_columns) - set(input_df.columns)\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Input data is missing optimization columns: {missing_features}\")\n",
    "\n",
    "        # Reorder columns to match the training data\n",
    "        input_df = input_df[self.optimization_columns]\n",
    "\n",
    "        # Apply the preprocessing pipeline\n",
    "        transformed_data = self.pipeline.transform(input_df)\n",
    "\n",
    "        return transformed_data\n",
    "\n",
    "    def run(self, return_optimization_ranges=False):\n",
    "        \"\"\"\n",
    "        Main entry point for preprocessing.\n",
    "\n",
    "        Args:\n",
    "            return_optimization_ranges (bool): Whether to include optimization ranges in the output.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Preprocessed data and optional optimization ranges.\n",
    "        \"\"\"\n",
    "        self.load_data()\n",
    "        if self.filtered_data is not None:\n",
    "            # Split features and target\n",
    "            y_variable = self.y_variable\n",
    "            if y_variable not in self.filtered_data.columns:\n",
    "                raise ValueError(f\"Target variable '{y_variable}' not found in dataset.\")\n",
    "            \n",
    "            X = self.filtered_data.drop(columns=[y_variable])\n",
    "            y = self.filtered_data[y_variable]\n",
    "\n",
    "            if self.preprocessor_train_test_split:\n",
    "                # Perform train-test split\n",
    "                stratify = y if self.stratify else None\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X,\n",
    "                    y,\n",
    "                    test_size=self.test_size,\n",
    "                    random_state=self.random_state,\n",
    "                    stratify=stratify,\n",
    "                )\n",
    "                if self.debug:\n",
    "                    logger.debug(f\"Index after train-test split:\")\n",
    "                    logger.debug(f\"X_train index:\\n{X_train.index}\")\n",
    "                    logger.debug(f\"X_test index:\\n{X_test.index}\")\n",
    "                    logger.debug(f\"y_train index:\\n{y_train.index}\")\n",
    "                    logger.debug(f\"y_test index:\\n{y_test.index}\")\n",
    "\n",
    "                # SMOTE automation on TRAINING ONLY\n",
    "                smote_analysis = check_dataset_for_smote(X_train, y_train, debug=self.debug)\n",
    "                logger.info(f\"SMOTE Analysis Recommendations: {smote_analysis['recommendations']}\")\n",
    "\n",
    "                # Apply SMOTE based on specific_smote parameter\n",
    "                if self.specific_smote is not None:\n",
    "                    if self.specific_smote.lower() == 'recommended':\n",
    "                        smote_variant = smote_analysis.get('recommendations', None)\n",
    "                        if smote_variant is None:\n",
    "                            logger.warning(\"No SMOTE recommendations available. Skipping SMOTE.\")\n",
    "                            X_train_resampled, y_train_resampled = X_train, y_train\n",
    "                            smote_used = None\n",
    "                        else:\n",
    "                            X_train_resampled, y_train_resampled, smote_used = apply_smote(\n",
    "                                X_train, y_train, smote_variant, debug=self.debug\n",
    "                            )\n",
    "                            logger.info(f\"Applied SMOTE Variant: {smote_used}\")\n",
    "                    else:\n",
    "                        # Use the specified SMOTE variant\n",
    "                        smote_variant = self.specific_smote\n",
    "                        X_train_resampled, y_train_resampled, smote_used = apply_smote(\n",
    "                            X_train, y_train, smote_variant, debug=self.debug\n",
    "                        )\n",
    "                        logger.info(f\"Applied SMOTE Variant: {smote_used}\")\n",
    "                else:\n",
    "                    # No SMOTE applied\n",
    "                    X_train_resampled, y_train_resampled = X_train, y_train\n",
    "                    smote_used = None\n",
    "                    logger.info(\"SMOTE not applied.\")\n",
    "\n",
    "                if smote_used:\n",
    "                    logger.info(f\"Resampled Class Distribution: {Counter(y_train_resampled)}\")\n",
    "                else:\n",
    "                    logger.info(f\"Original Class Distribution: {Counter(y_train_resampled)}\")\n",
    "\n",
    "                # Proceed with preprocessing using resampled data\n",
    "                self._preprocess_and_transform(X_train_resampled, X_test)\n",
    "                self._preprocess_target(y_train_resampled, y_test)\n",
    "\n",
    "                # Validate index consistency\n",
    "                assert self.X_train_transformed.index.equals(X_train_resampled.index), \"Index mismatch in X_train_transformed\"\n",
    "                assert self.X_test_transformed.index.equals(X_test.index), \"Index mismatch in X_test_transformed\"\n",
    "\n",
    "                if self.debug:\n",
    "                    logger.debug(\"Index consistency verified after preprocessing with train-test split.\")\n",
    "\n",
    "                self.save_assets()\n",
    "                return (\n",
    "                    self.X_train_transformed,\n",
    "                    self.X_test_transformed,\n",
    "                    self.y_train_encoded,\n",
    "                    self.y_test_encoded,\n",
    "                    self.transformed_train,\n",
    "                    self.transformed_train_df,\n",
    "                    X_train_resampled,\n",
    "                    y_train_resampled,\n",
    "                )\n",
    "            else:\n",
    "                # Preprocess without split\n",
    "                self._preprocess_and_transform(X)\n",
    "                self._preprocess_target(y)\n",
    "\n",
    "                # Validate index consistency\n",
    "                assert self.X_transformed.index.equals(X.index), \"Index mismatch in X_transformed\"\n",
    "\n",
    "                if self.debug:\n",
    "                    logger.debug(\"Index consistency verified after preprocessing without train-test split.\")\n",
    "\n",
    "                self.save_assets()\n",
    "\n",
    "                if return_optimization_ranges and self.optimization_columns:\n",
    "                    self.extract_optimization_ranges()  # Compute normal ranges\n",
    "                    self.extract_transformed_optimization_ranges()\n",
    "                    return (\n",
    "                        self.X_transformed,\n",
    "                        self.y_encoded,\n",
    "                        self.transformed_data,\n",
    "                        self.transformed_df,\n",
    "                        self.original_data,  # Included original data\n",
    "                        X,\n",
    "                        y,\n",
    "                        self.optimization_ranges,  # Now computed\n",
    "                        self.optimization_transformed_ranges,  # Return transformed ranges\n",
    "                    )\n",
    "                else:\n",
    "                    return (\n",
    "                        self.X_transformed,\n",
    "                        self.y_encoded,\n",
    "                        self.transformed_data,\n",
    "                        self.transformed_df,\n",
    "                        self.original_data,  # Included original data\n",
    "                        X,\n",
    "                        y,\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "    dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    assets_path = '../../data/model/pipeline/preprocessing_assets.pkl'\n",
    "\n",
    "    # Example 1: With train-test split\n",
    "    dp_split = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable='result',\n",
    "        preprocessor_train_test_split=True,\n",
    "        test_size=0.3,\n",
    "        random_state=123,\n",
    "        stratify=True,\n",
    "        specific_smote='SMOTEENN',  # Specify desired SMOTE variant\n",
    "    )\n",
    "    (\n",
    "        X_train_transformed,\n",
    "        X_test_transformed,\n",
    "        y_train_encoded,\n",
    "        y_test_encoded,\n",
    "        transformed_data_train,\n",
    "        transformed_train_df,  # Include the additional output\n",
    "        X_train,\n",
    "        y_train,\n",
    "    ) = dp_split.run()\n",
    "\n",
    "\n",
    "    print(f\"X_train_transformed shape: {X_train_transformed.shape}\")\n",
    "    print(f\"X_test_transformed shape: {X_test_transformed.shape}\")\n",
    "    print(f\"Transformed Data Train: {transformed_data_train.keys()}\")\n",
    "    print(f\"Original X_train shape: {X_train.shape}\")\n",
    "    print(f\"Original y_train shape: {y_train.shape}\")\n",
    "\n",
    "    # Example 2: Without train-test split\n",
    "    dp_no_split = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable='result',\n",
    "        preprocessor_train_test_split=False,\n",
    "        specific_smote='recommended',  # Specify desired SMOTE variant\n",
    "    )\n",
    "    X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y = dp_no_split.run()\n",
    "\n",
    "    print(f\"X_transformed shape: {X_transformed.shape}\")\n",
    "    print(f\"Transformed Data: {transformed_data.keys()}\")\n",
    "    print(f\"Transformed Data: {transformed_data_df.columns}\")\n",
    "    print(f\"Original X shape: {X.shape}\")\n",
    "    print(f\"Original y shape: {y.shape}\")\n",
    "\n",
    "    #-------------Preprocessing with Optimization Ranges------------\n",
    "    dp = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable=\"result\",\n",
    "        optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"], # can and do use all x columns for bayesian optimization test\n",
    "        preprocessor_train_test_split=False,\n",
    "        specific_smote='recommended',  # Specify desired SMOTE variant\n",
    "    )\n",
    "\n",
    "    results = dp.run(return_optimization_ranges=True)\n",
    "    X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y, optimization_ranges, optimization_transformed_ranges = results\n",
    "\n",
    "    print(f\"Optimization Ranges: {optimization_ranges}\")\n",
    "    print(f\"optimization_transformed_ranges: {optimization_transformed_ranges}\")\n",
    "    print(f\"original_data shape: {original_data.shape}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"X_transformed shape: {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../../src/freethrow_predictions/ml/classification_processors/inverse_preprocessor_class.py\n",
    "\n",
    "from typing import Dict, Optional, Any, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "# Import necessary inverse preprocessor functions\n",
    "# from inverse_preprocessor_functions import (load_pipeline_and_assets, \n",
    "#                                            inverse_transform_feature_groups,\n",
    "#                                            combine_inverse_transformed_data,\n",
    "#                                            prepare_final_dataset_with_target)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InversePreprocessor:\n",
    "    \"\"\"\n",
    "    InversePreprocessor reverses the transformations applied during preprocessing,\n",
    "    allowing the reconstructed dataset to be compared or interpreted in its original form.\n",
    "\n",
    "    Attributes:\n",
    "        # ... existing attributes\n",
    "\n",
    "    Methods:\n",
    "        transform(original_data, transformed_data, y_encoded):\n",
    "            Performs the inverse transformation on the transformed data.\n",
    "            - Validates that the index remains consistent throughout inverse preprocessing.\n",
    "            - Logs detailed debug information if debug=True.\n",
    "        append_columns_from_original(final_dataset, original_data, columns_to_append, debug=False):\n",
    "            Appends specified columns from original_data to final_dataset after validating index consistency.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 pipelines: Optional[Dict[str, Any]] = None, \n",
    "                 feature_indices: Optional[Dict[str, Any]] = None, \n",
    "                 flattened_feature_indices: Optional[Dict[int, str]] = None,\n",
    "                 label_encoder: Optional[Any] = None,\n",
    "                 assets_path: Optional[str] = None,\n",
    "                 debug: bool = False):\n",
    "        self.pipelines = pipelines\n",
    "        self.feature_indices = feature_indices\n",
    "        self.flattened_feature_indices = flattened_feature_indices\n",
    "        self.label_encoder = label_encoder\n",
    "        self.assets_path = assets_path\n",
    "        self.debug = debug\n",
    "\n",
    "        if not self.pipelines or not self.feature_indices or not self.flattened_feature_indices:\n",
    "            if self.assets_path:\n",
    "                if self.debug:\n",
    "                    logger.debug(f\"[InversePreprocessor] Loading pipelines and assets from: {self.assets_path}\")\n",
    "                self.pipelines, self.feature_indices, self.flattened_feature_indices, self.label_encoder = load_pipeline_and_assets(self.assets_path)\n",
    "            else:\n",
    "                raise ValueError(\"Pipelines and feature indices are not provided and no assets_path is specified.\")\n",
    "\n",
    "        if self.debug:\n",
    "            logger.debug(f\"[InversePreprocessor] Initialized InversePreprocessor with assets_path: {assets_path}\")\n",
    "            logger.debug(f\"[InversePreprocessor] Flattened Feature Indices: {self.flattened_feature_indices}\")\n",
    "            logger.debug(f\"[InversePreprocessor] Loaded Pipelines: {list(self.pipelines.keys())}\")\n",
    "            logger.debug(f\"[InversePreprocessor] Feature Indices: {self.feature_indices}\")\n",
    "            logger.debug(f\"[InversePreprocessor] Flattened Feature Indices: {self.flattened_feature_indices}\")\n",
    "            logger.debug(f\"[InversePreprocessor] Label Encoder: {self.label_encoder}\")\n",
    "\n",
    "\n",
    "    def append_columns_from_original(self, \n",
    "                                     final_dataset: pd.DataFrame, \n",
    "                                     original_data: pd.DataFrame, \n",
    "                                     columns_to_append: List[str],\n",
    "                                     debug: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Appends specified columns from original_data to final_dataset after ensuring index alignment.\n",
    "\n",
    "        Args:\n",
    "            final_dataset (pd.DataFrame): The dataset to which columns will be appended.\n",
    "            original_data (pd.DataFrame): The original dataset containing the columns to append.\n",
    "            columns_to_append (List[str]): List of column names to append from original_data.\n",
    "            debug (bool, optional): If True, provides detailed debug information. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated final_dataset with appended columns.\n",
    "        \"\"\"\n",
    "        if debug:\n",
    "            logger.debug(\"[append_columns_from_original] Starting index consistency check.\")\n",
    "\n",
    "        # Step 1: Index Consistency Check\n",
    "        if not original_data.index.equals(final_dataset.index):\n",
    "            logger.warning(\"[append_columns_from_original] Index mismatch between original_data and final_dataset. Reindexing to align.\")\n",
    "            final_dataset = final_dataset.reindex(original_data.index)\n",
    "            logger.debug(\"[append_columns_from_original] Reindexing completed.\")\n",
    "\n",
    "        # Step 2: Column Selection Validation\n",
    "        if debug:\n",
    "            logger.debug(f\"[append_columns_from_original] Columns requested to append: {columns_to_append}\")\n",
    "        \n",
    "        valid_columns = []\n",
    "        for col in columns_to_append:\n",
    "            if not isinstance(col, str) or not col.strip():\n",
    "                logger.warning(f\"[append_columns_from_original] Invalid column name detected: '{col}'. Skipping.\")\n",
    "                continue\n",
    "            if col not in original_data.columns:\n",
    "                logger.warning(f\"[append_columns_from_original] Column '{col}' not found in original_data. Skipping.\")\n",
    "                continue\n",
    "            valid_columns.append(col)\n",
    "\n",
    "        if not valid_columns:\n",
    "            logger.error(\"[append_columns_from_original] No valid columns to append. Returning the final_dataset unchanged.\")\n",
    "            return final_dataset\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(f\"[append_columns_from_original] Valid columns to append: {valid_columns}\")\n",
    "\n",
    "        # Step 3: Append Columns\n",
    "        columns_to_append_df = original_data[valid_columns]\n",
    "        if debug:\n",
    "            logger.debug(f\"[append_columns_from_original] Shape of columns to append: {columns_to_append_df.shape}\")\n",
    "            logger.debug(f\"[append_columns_from_original] Columns before concatenation: {final_dataset.columns.tolist()}\")\n",
    "        \n",
    "        final_dataset_updated = pd.concat([final_dataset, columns_to_append_df], axis=1)\n",
    "\n",
    "        if debug:\n",
    "            logger.debug(f\"[append_columns_from_original] Shape after concatenation: {final_dataset_updated.shape}\")\n",
    "            logger.debug(f\"[append_columns_from_original] Columns after concatenation: {final_dataset_updated.columns.tolist()}\")\n",
    "            logger.debug(f\"[append_columns_from_original] Sample of appended data:\\n{columns_to_append_df.head()}\")\n",
    "\n",
    "        # Step 4: Output the Final Dataset\n",
    "        if debug:\n",
    "            logger.debug(\"[append_columns_from_original] Successfully appended columns from original_data.\")\n",
    "        else:\n",
    "            logger.info(\"[append_columns_from_original] Columns appended successfully.\")\n",
    "\n",
    "        return final_dataset_updated\n",
    "\n",
    "    def inverse_transform_features(self, transformed_data: Dict[str, np.ndarray]) -> Dict[str, pd.DataFrame]:\n",
    "        if self.debug:\n",
    "            logger.debug(\"[InversePreprocessor] Performing inverse transformation on feature groups.\")\n",
    "        try:\n",
    "            inverse_transformed_data = inverse_transform_feature_groups(\n",
    "                transformed_data,\n",
    "                self.pipelines,\n",
    "                self.feature_indices,\n",
    "                debug=self.debug\n",
    "            )\n",
    "            if self.debug:\n",
    "                for group, data in inverse_transformed_data.items():\n",
    "                    logger.debug(f\"[InversePreprocessor] Inverse transformed group '{group}' with shape {data.shape}\")\n",
    "                    logger.debug(f\"[InversePreprocessor] Inverse transformed group '{group}' index:\\n{data.index}\")\n",
    "            return inverse_transformed_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during feature inverse transformation: {e}\")\n",
    "            raise RuntimeError(f\"Error during feature inverse transformation: {e}\")\n",
    "\n",
    "    def inverse_transform_optimization_params(self, params: pd.DataFrame, optimization_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Inverse transform optimization parameters to the original scale.\n",
    "\n",
    "        Args:\n",
    "            params (pd.DataFrame): Transformed parameter values as a DataFrame.\n",
    "            optimization_columns (list): List of columns to inverse-transform.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inverse-transformed parameter values as a DataFrame.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            logger.debug(f\"[InversePreprocessor] Inverse-transforming optimization parameters for columns: {optimization_columns}\")\n",
    "\n",
    "        try:\n",
    "            # Flatten feature_indices to get all feature names\n",
    "            all_features = set()\n",
    "            for group_name, group in self.feature_indices.items():\n",
    "                all_features.update(group[\"feature_names\"])\n",
    "\n",
    "            # Ensure optimization columns exist in all_features\n",
    "            missing_columns = [col for col in optimization_columns if col not in all_features]\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Optimization columns not found in feature indices: {missing_columns}\")\n",
    "\n",
    "            # Map each feature to its pipeline and index\n",
    "            feature_to_pipeline_and_index = {}\n",
    "            for group_name, group in self.feature_indices.items():\n",
    "                pipeline = self.pipelines[group_name]\n",
    "                feature_names = group['feature_names']\n",
    "                for i, feature in enumerate(feature_names):\n",
    "                    feature_to_pipeline_and_index[feature] = (pipeline, i)\n",
    "\n",
    "            # Perform inverse transformation for each column\n",
    "            inverse_transformed = {}\n",
    "            for col in optimization_columns:\n",
    "                pipeline, feature_index = feature_to_pipeline_and_index[col]\n",
    "                transformed_value = params[col].values.reshape(-1, 1)\n",
    "                # Extract the transformer\n",
    "                transformer = pipeline.steps[-1][1]\n",
    "                if isinstance(transformer, StandardScaler):\n",
    "                    scale = transformer.scale_[feature_index]\n",
    "                    mean = transformer.mean_[feature_index]\n",
    "                    original_value = transformed_value * scale + mean\n",
    "                    inverse_transformed[col] = original_value.flatten()\n",
    "                elif isinstance(transformer, MinMaxScaler):\n",
    "                    data_min = transformer.data_min_[feature_index]\n",
    "                    data_max = transformer.data_max_[feature_index]\n",
    "                    feature_range_min, feature_range_max = transformer.feature_range\n",
    "                    original_value = ((transformed_value - feature_range_min) / (feature_range_max - feature_range_min)) * (data_max - data_min) + data_min\n",
    "                    inverse_transformed[col] = original_value.flatten()\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Transformer {transformer} not supported for inverse_transform\")\n",
    "\n",
    "            # Combine results into a DataFrame\n",
    "            inverse_transformed_df = pd.DataFrame(inverse_transformed)\n",
    "            if self.debug:\n",
    "                logger.debug(f\"[InversePreprocessor] Inverse-transformed optimization parameters:\\n{inverse_transformed_df}\")\n",
    "        \n",
    "            return inverse_transformed_df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during inverse transformation of optimization parameters: {e}\")\n",
    "            raise RuntimeError(f\"Error during inverse transformation of optimization parameters: {e}\")\n",
    "\n",
    "    def decode_target(self, y_encoded: np.ndarray) -> Optional[np.ndarray]:\n",
    "        if not self.label_encoder:\n",
    "            if self.debug:\n",
    "                logger.debug(\"[InversePreprocessor] No label encoder provided; skipping target decoding.\")\n",
    "            return None\n",
    "        if self.debug:\n",
    "            logger.debug(\"[InversePreprocessor] Decoding target variable.\")\n",
    "        try:\n",
    "            decoded_targets = self.label_encoder.inverse_transform(y_encoded)\n",
    "            if self.debug:\n",
    "                logger.debug(f\"[InversePreprocessor] Decoded targets (first 5): {decoded_targets[:5]}\")\n",
    "            return decoded_targets\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding target variable: {e}\")\n",
    "            raise RuntimeError(f\"Error decoding target variable: {e}\")\n",
    "\n",
    "    def combine_features_and_targets(self, \n",
    "                                    original_data: pd.DataFrame, \n",
    "                                    inverse_transformed_data: Dict[str, pd.DataFrame], \n",
    "                                    y_encoded: np.ndarray) -> pd.DataFrame:\n",
    "        decoded_targets = self.decode_target(y_encoded)\n",
    "        y_encoded_series = pd.Series(y_encoded, index=original_data.index, name='Encoded_Target')\n",
    "        if decoded_targets is not None:\n",
    "            decoded_targets_series = pd.Series(decoded_targets, index=original_data.index, name='Decoded_Target')\n",
    "        else:\n",
    "            decoded_targets_series = None\n",
    "        try:\n",
    "            combined_features = combine_inverse_transformed_data(\n",
    "                inverse_transformed_data,\n",
    "                original_data.index,\n",
    "                original_data.columns,\n",
    "                debug=self.debug\n",
    "            )\n",
    "            combined_data = prepare_final_dataset_with_target(\n",
    "                original_data,\n",
    "                combined_features,\n",
    "                y_encoded_series,\n",
    "                decoded_targets_series,\n",
    "                debug=self.debug\n",
    "            )\n",
    "            return combined_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error combining features and targets: {e}\")\n",
    "            raise RuntimeError(f\"Error combining features and targets: {e}\")\n",
    "\n",
    "    \n",
    "    # Example of existing transform method for context\n",
    "    def transform(self, \n",
    "                  original_data: pd.DataFrame, \n",
    "                  transformed_data: Dict[str, np.ndarray], \n",
    "                  y_encoded: np.ndarray) -> pd.DataFrame:\n",
    "        if self.debug:\n",
    "            logger.debug(\"[InversePreprocessor] Starting full inverse transformation and recombination process.\")\n",
    "            logger.debug(f\"Original data index:\\n{original_data.index}\")\n",
    "            logger.debug(f\"Transformed data keys: {list(transformed_data.keys())}\")\n",
    "        \n",
    "        # Inverse transformation steps...\n",
    "        inverse_transformed_data = self.inverse_transform_features(transformed_data)\n",
    "        \n",
    "        if self.debug:\n",
    "            for group, data in inverse_transformed_data.items():\n",
    "                logger.debug(f\"[InversePreprocessor] Inverse transformed group '{group}' with shape {data.shape}\")\n",
    "                logger.debug(f\"[InversePreprocessor] Inverse transformed group '{group}' index:\\n{data.index}\")\n",
    "        \n",
    "        # Combine inverse transformed data\n",
    "        final_dataset = self.combine_features_and_targets(\n",
    "            original_data, \n",
    "            inverse_transformed_data, \n",
    "            y_encoded\n",
    "        )\n",
    "        \n",
    "        # Validate index consistency\n",
    "        if not final_dataset.index.equals(original_data.index):\n",
    "            logger.warning(\"Index mismatch after inverse transformation. Reindexing the final dataset.\")\n",
    "            final_dataset = final_dataset.reindex(original_data.index)\n",
    "        \n",
    "        if self.debug:\n",
    "            logger.debug(f\"[InversePreprocessor] Final dataset index:\\n{final_dataset.index}\")\n",
    "            logger.debug(f\"[InversePreprocessor] Original dataset index:\\n{original_data.index}\")\n",
    "        \n",
    "        return final_dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from datapreprocessor_class import DataPreprocessor\n",
    "    # File paths\n",
    "    features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "    dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    assets_path = '../../data/model/pipeline/preprocessing_assets.pkl'\n",
    "\n",
    "    # Example 1: With train-test split\n",
    "    dp_split = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable='result',\n",
    "        preprocessor_train_test_split=True,\n",
    "        test_size=0.3,\n",
    "        random_state=123,\n",
    "        stratify=True,\n",
    "    )\n",
    "    (\n",
    "        X_train_transformed,\n",
    "        X_test_transformed,\n",
    "        y_train_encoded,\n",
    "        y_test_encoded,\n",
    "        transformed_data_train,\n",
    "        transformed_train_df,  # Include the additional output\n",
    "        X_train,\n",
    "        y_train,\n",
    "    ) = dp_split.run()\n",
    "\n",
    "\n",
    "    print(f\"X_train_transformed shape: {X_train_transformed.shape}\")\n",
    "    print(f\"X_test_transformed shape: {X_test_transformed.shape}\")\n",
    "    print(f\"Transformed Data Train: {transformed_data_train.keys()}\")\n",
    "    print(f\"Original X_train shape: {X_train.shape}\")\n",
    "    print(f\"Original y_train shape: {y_train.shape}\")\n",
    "\n",
    "\n",
    "    # Initialize InversePreprocessor with assets_path (no need to preload assets manually)\n",
    "    inverse_transformer = InversePreprocessor(\n",
    "        assets_path=assets_path,  # Assets will be loaded automatically\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Perform inverse transformation and combine with targets\n",
    "    final_dataset = inverse_transformer.transform(\n",
    "        original_data=X_train,\n",
    "        transformed_data=transformed_data_train,\n",
    "        y_encoded=y_train_encoded\n",
    "    )\n",
    "\n",
    "    # Example: Append specified columns from original_data to final_dataset\n",
    "    columns_to_append = ['player_height_in_meters', 'player_weight__in_kg']  # Example columns\n",
    "    final_dataset = inverse_transformer.append_columns_from_original(\n",
    "        final_dataset=final_dataset,\n",
    "        original_data=X_train,\n",
    "        columns_to_append=columns_to_append,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Display the resulting dataset\n",
    "    print(\"[Final Dataset]:\")\n",
    "    print(final_dataset.head())\n",
    "    print(final_dataset.shape)\n",
    "    \n",
    "    # ---------------Example 2: Without train-test split------------------\n",
    "    dp_no_split = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable='result',\n",
    "        preprocessor_train_test_split=False,\n",
    "    )\n",
    "    X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y = dp_no_split.run()\n",
    "\n",
    "    print(f\"X_transformed shape: {X_transformed.shape}\")\n",
    "    print(f\"Transformed Data: {transformed_data.keys()}\")\n",
    "    print(f\"Transformed Data: {transformed_data_df.columns}\")\n",
    "    print(f\"Original X shape: {X.shape}\")\n",
    "    print(f\"Original y shape: {y.shape}\")\n",
    "    \n",
    "    # Inverse transformation for Example 2: Without train-test split\n",
    "    print(\"\\n[Example 2: Without Train-Test Split]\")\n",
    "    inverse_transformer = InversePreprocessor(\n",
    "        assets_path=assets_path,  # Assets will be loaded automatically\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Perform inverse transformation and combine with targets\n",
    "    final_dataset_no_split = inverse_transformer.transform(\n",
    "        original_data=X,\n",
    "        transformed_data=transformed_data,\n",
    "        y_encoded=y_encoded\n",
    "    )\n",
    "\n",
    "    # Example: Append specified columns from original_data to final_dataset_no_split\n",
    "    columns_to_append_no_split = ['trial_id']  # Example columns\n",
    "    final_dataset_no_split = inverse_transformer.append_columns_from_original(\n",
    "        final_dataset=final_dataset_no_split,\n",
    "        original_data=original_data,\n",
    "        columns_to_append=columns_to_append_no_split,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Display the resulting dataset\n",
    "    print(\"[Final Dataset without Train-Test Split]:\")\n",
    "    print(f\"Final Dataset shape (No Split): {final_dataset_no_split.shape}\")\n",
    "    print(f\"Original Dataset shape (With Optimization Ranges): {original_data.shape}\")\n",
    "\n",
    "\n",
    "    #-------------example 3: Preprocessing with Optimization Ranges------------\n",
    "    dp = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable=\"result\",\n",
    "        optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"],\n",
    "        preprocessor_train_test_split=False,\n",
    "    )\n",
    "\n",
    "    results = dp.run(return_optimization_ranges=True)\n",
    "    X_transformed, y_encoded, transformed_data, transformed_data_df, original_data, X, y, optimization_ranges, optimization_transformed_ranges = results\n",
    "\n",
    "    print(f\"Optimization Ranges: {optimization_ranges}\")\n",
    "    print(f\"optimization_transformed_ranges: {optimization_transformed_ranges}\")\n",
    "\n",
    "    # Inverse transformation for Example 3: Preprocessing with Optimization Ranges\n",
    "    print(\"\\n[Example 3: Preprocessing with Optimization Ranges]\")\n",
    "    inverse_transformer = InversePreprocessor(\n",
    "        assets_path=assets_path,  # Assets will be loaded automatically\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Perform inverse transformation and combine with targets\n",
    "    final_dataset_with_optimization = inverse_transformer.transform(\n",
    "        original_data=X,\n",
    "        transformed_data=transformed_data,\n",
    "        y_encoded=y_encoded\n",
    "    )\n",
    "\n",
    "    # Example: Append specified columns from original_data to final_dataset_with_optimization\n",
    "    columns_to_append_with_opt = [\"trial_id\"]  # Example columns\n",
    "    final_dataset_with_optimization = inverse_transformer.append_columns_from_original(\n",
    "        final_dataset=final_dataset_with_optimization,\n",
    "        original_data=original_data,\n",
    "        columns_to_append=columns_to_append_with_opt,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Inverse transform optimization parameters\n",
    "    inverse_optimization_params = inverse_transformer.inverse_transform_optimization_params(\n",
    "        params=pd.DataFrame(optimization_transformed_ranges),\n",
    "        optimization_columns=[\"knee_max_angle\", \"wrist_max_angle\", \"elbow_max_angle\"]\n",
    "    )\n",
    "\n",
    "    # Display the resulting datasets\n",
    "    print(f\"Final Dataset shape (With Optimization Ranges): {final_dataset_with_optimization.shape}\")\n",
    "    print(f\"Original Dataset shape (With Optimization Ranges): {original_data.shape}\")\n",
    "\n",
    "    print(\"\\n[Inverse-Transformed Optimization Parameters]:\")\n",
    "    print(inverse_optimization_params)\n",
    "    print(f\"Inverse-Transformed Optimization Parameters shape: {inverse_optimization_params.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../../src/freethrow_predictions/ml/inverse_preprocessor_functions.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "\n",
    "def inverse_transform_feature_groups(\n",
    "    transformed_data,\n",
    "    fitted_pipelines,\n",
    "    feature_indices,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies inverse transformations for each feature group using the fitted pipelines.\n",
    "    Returns a dictionary of DataFrames with appropriate column names.\n",
    "    \"\"\"\n",
    "    inverse_transformed_data = {}\n",
    "\n",
    "    for feature_type, data in transformed_data.items():\n",
    "        if data is not None:\n",
    "            if feature_type in fitted_pipelines:\n",
    "                pipeline = fitted_pipelines[feature_type]\n",
    "                if hasattr(pipeline, 'inverse_transform'):\n",
    "                    inv_transformed = pipeline.inverse_transform(data)\n",
    "                else:\n",
    "                    # If inverse_transform is not available, use the original data\n",
    "                    inv_transformed = data\n",
    "                    if debug:\n",
    "                        print(f\"\\n[{feature_type}] inverse_transform not available. Using original transformed data.\")\n",
    "                # Retrieve feature names\n",
    "                feature_names = feature_indices[feature_type]['feature_names']\n",
    "                inverse_transformed_data[feature_type] = pd.DataFrame(inv_transformed, columns=feature_names)\n",
    "                if debug:\n",
    "                    print(f\"\\n[Inverse {feature_type}] Features: {feature_names}\")\n",
    "                    print(f\"Inverse Transformed Data Shape: {inv_transformed.shape}\")\n",
    "            else:\n",
    "                # For unprocessed data or if pipeline is not available\n",
    "                feature_names = feature_indices[feature_type]['feature_names']\n",
    "                inverse_transformed_data[feature_type] = pd.DataFrame(data, columns=feature_names)\n",
    "                if debug:\n",
    "                    print(f\"\\n[Unprocessed {feature_type}] Features: {feature_names}\")\n",
    "                    print(f\"Data Shape: {data.shape}\")\n",
    "\n",
    "    return inverse_transformed_data\n",
    "\n",
    "def combine_inverse_transformed_data(inverse_transformed_data, original_index, original_columns, debug=False):\n",
    "    \"\"\"\n",
    "    Combines inverse transformed DataFrames into a single DataFrame,\n",
    "    preserving the original index and matching original column order.\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat(inverse_transformed_data.values(), axis=1)\n",
    "    if debug:\n",
    "        print(f\"[Debug] Combined Data Shape (Before Index Assignment): {combined_df.shape}\")\n",
    "        print(f\"[Debug] Original Index Length: {len(original_index)}\")\n",
    "\n",
    "    # Validate index length\n",
    "    if len(original_index) != combined_df.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Index length mismatch: Combined Data has {combined_df.shape[0]} rows, but original index has {len(original_index)} elements\"\n",
    "        )\n",
    "    combined_df.index = original_index  # Attach the original index\n",
    "\n",
    "    # Validate column alignment\n",
    "    if debug:\n",
    "        print(f\"[Debug] Combined Data Columns (Before Alignment): {list(combined_df.columns)}\")\n",
    "        print(f\"[Debug] Original Columns: {list(original_columns)}\")\n",
    "\n",
    "    if set(original_columns) != set(combined_df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Column mismatch: Combined Data Columns={list(combined_df.columns)}, Original Columns={list(original_columns)}\"\n",
    "        )\n",
    "    combined_df = combined_df[original_columns]  # Align columns with the original order\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "def load_pipeline_and_assets(path):\n",
    "    \"\"\"\n",
    "    Loads the fitted preprocessing pipelines, feature indices, flattened feature indices,\n",
    "    and label encoder from a file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the saved file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fitted_pipelines, feature_indices, flattened_feature_indices, label_encoder)\n",
    "    \"\"\"\n",
    "    assets = joblib.load(path)\n",
    "    print(f\"Pipeline and assets loaded from {path}\")\n",
    "    \n",
    "    # Debug: Print keys and sample values in assets\n",
    "    print(f\"Loaded asset keys: {list(assets.keys())}\")\n",
    "    for key, value in assets.items():\n",
    "        print(f\"[Debug] Key: {key}, Type: {type(value)}\")\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"[Debug] Sample from {key}: {list(value.keys())[:5]}\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"[Debug] Sample from {key}: {value[:5]}\")\n",
    "        elif isinstance(value, pd.DataFrame):\n",
    "            print(f\"[Debug] DataFrame {key} Columns: {value.columns}\")\n",
    "    \n",
    "    # Check flattened_feature_indices\n",
    "    flattened_feature_indices = assets.get(\"flattened_feature_indices\", None)\n",
    "    if flattened_feature_indices is None:\n",
    "        print(\"[Warning] Flattened feature indices not found in the loaded assets.\")\n",
    "    \n",
    "    return (\n",
    "        assets[\"fitted_pipelines\"],\n",
    "        assets[\"feature_indices\"],\n",
    "        flattened_feature_indices,\n",
    "        assets[\"label_encoder\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_final_dataset_with_target(\n",
    "    X_train, X_train_inverse_transformed, y_train, y_train_decoded, debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the final dataset by checking alignment and adding the target variable.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Original training features.\n",
    "        X_train_inverse_transformed (pd.DataFrame): Inverse-transformed training features.\n",
    "        y_train (pd.Series): Original target variable.\n",
    "        y_train_decoded (array-like): Decoded target variable.\n",
    "        debug (bool): Whether to print debug statements.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Final dataset with the target variable included.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n[Inverse Transformed Data Shapes]\")\n",
    "        print(f\"X_train_inverse_transformed Shape: {X_train_inverse_transformed.shape}\")\n",
    "        print(f\"X_train_inverse_transformed Index:\\n{X_train_inverse_transformed.index[:5]}\")\n",
    "        print(f\"y_train Index:\\n{y_train.index[:5]}\")\n",
    "\n",
    "    # Check alignment of indices\n",
    "    if not X_train_inverse_transformed.index.equals(y_train.index):\n",
    "        raise ValueError(\"Index mismatch between X_train_inverse_transformed and y_train\")\n",
    "\n",
    "    # Check if the original and inverse-transformed datasets are identical\n",
    "    # Check if the original and inverse-transformed datasets are identical\n",
    "    if debug:\n",
    "        # Adjusted equality check\n",
    "        if np.allclose(X_train.values, X_train_inverse_transformed.values, atol=1e-6):\n",
    "            print(\"\\nThe inverse-transformed features align with the original features within tolerance.\")\n",
    "        else:\n",
    "            print(\"\\nThe inverse-transformed features DO NOT align with the original features even within tolerance.\")\n",
    "\n",
    "\n",
    "            # Identify misaligned columns\n",
    "            misaligned_columns = X_train.columns[\n",
    "                ~X_train.eq(X_train_inverse_transformed).all(axis=0)\n",
    "            ]\n",
    "            print(f\"\\n[Misaligned Columns]\")\n",
    "            print(misaligned_columns.tolist())\n",
    "\n",
    "            # Show how values differ in mismatched columns\n",
    "            for col in misaligned_columns:\n",
    "                print(f\"\\nColumn: {col}\")\n",
    "                mismatch = ~X_train[col].eq(X_train_inverse_transformed[col])\n",
    "                mismatched_rows = X_train.loc[mismatch, col].head()\n",
    "                transformed_mismatched_rows = X_train_inverse_transformed.loc[mismatch, col].head()\n",
    "                print(f\"Original Values (First 5 mismatches):\\n{mismatched_rows}\")\n",
    "                print(f\"Inverse Transformed Values (First 5 mismatches):\\n{transformed_mismatched_rows}\")\n",
    "\n",
    "\n",
    "    # Add the target variable to the inverse-transformed DataFrame\n",
    "    X_train_with_target = X_train_inverse_transformed.copy()\n",
    "    X_train_with_target['y_variable_original'] = y_train_decoded\n",
    "\n",
    "    if debug:\n",
    "        print(f\"\\n[Final Dataset with Target Variable]\")\n",
    "        print(f\"X_train_with_target Shape: {X_train_with_target.shape}\")\n",
    "        print(X_train_with_target.head())\n",
    "\n",
    "    return X_train_with_target\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # from datapreprocessor_class import DataPreprocessor\n",
    "    # File paths\n",
    "    features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "    dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    assets_path = '../../data/model/pipeline/preprocessing_assets.pkl'\n",
    "\n",
    "    # Example 1: With train-test split\n",
    "    dp_split = DataPreprocessor(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        assets_path=assets_path,\n",
    "        y_variable='result',\n",
    "        preprocessor_train_test_split=True,\n",
    "        test_size=0.3,\n",
    "        random_state=123,\n",
    "        stratify=True,\n",
    "    )\n",
    "    (\n",
    "        X_train_transformed,\n",
    "        X_test_transformed,\n",
    "        y_train_encoded,\n",
    "        y_test_encoded,\n",
    "        transformed_data_train,\n",
    "        transformed_train_df,  # Include the additional output\n",
    "        X_train,\n",
    "        y_train,\n",
    "    ) = dp_split.run()\n",
    "\n",
    "    print(f\"X_train_transformed shape: {X_train_transformed.shape}\")\n",
    "    print(f\"X_test_transformed shape: {X_test_transformed.shape}\")\n",
    "    print(f\"Transformed Data Train: {transformed_data_train.keys()}\")\n",
    "    print(f\"Original X_train shape: {X_train.shape}\")\n",
    "    print(f\"Original y_train shape: {y_train.shape}\")\n",
    "\n",
    "    #------------------------------------------\n",
    "    # Saving/Loading Pipelines/Feature Lists\n",
    "    # Define base paths\n",
    "    BASE_DATA_PATH = '../../data/model/pipeline/preprocessing_assets.pkl'\n",
    "\n",
    "\n",
    "    # Step 2: Load the preprocessing assets (simulate decoding in a different context)\n",
    "    fitted_pipelines, feature_indices_train, flattened_feature_indices, label_encoder = load_pipeline_and_assets(path=BASE_DATA_PATH)\n",
    "    \n",
    "    #------------------------------------------\n",
    "    # Decoding examples:\n",
    "\n",
    "    # Optional: Decode target variable for inverse transformation\n",
    "    if label_encoder:\n",
    "        y_train_decoded = label_encoder.inverse_transform(y_train_encoded)\n",
    "        print(f\"\\n[Decoded y_train (First 5)]: {y_train_decoded[:5]}\")\n",
    "\n",
    "\n",
    "    # Step 8: Inverse Transform the Training Data\n",
    "    inverse_transformed_data_train = inverse_transform_feature_groups(\n",
    "        transformed_data_train,\n",
    "        fitted_pipelines,\n",
    "        feature_indices_train,\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "\n",
    "    # Combine inverse transformed data\n",
    "    X_train_inverse_transformed = combine_inverse_transformed_data(\n",
    "        inverse_transformed_data_train,\n",
    "        X_train.index,       # Original index\n",
    "        X_train.columns,     # Original columns\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Call the function with debug enabled\n",
    "    X_train_with_target = prepare_final_dataset_with_target(\n",
    "        X_train,\n",
    "        X_train_inverse_transformed,\n",
    "        y_train,\n",
    "        y_train_decoded,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%writefile ../../src/freethrow_predictions/ml/classification_preprocessor/preprocessor_functions.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "from ml.classification_preprocessor.smote_automation import check_dataset_for_smote, apply_smote\n",
    "\n",
    "# Define pipelines for feature preprocessing\n",
    "def create_numerical_scaler_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean', add_indicator=True)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "def create_numerical_minmax_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean', add_indicator=True)),\n",
    "        ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "def create_numerical_kbins_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean', add_indicator=True)),\n",
    "        ('kbins', KBinsDiscretizer(encode='ordinal', strategy='uniform'))\n",
    "    ])\n",
    "\n",
    "def create_categorical_onehot_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "def create_categorical_labelencode_pipeline():\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "        ('encoder', OrdinalEncoder())\n",
    "    ])\n",
    "\n",
    "def create_numerical_dimred_pipeline(n_components=2):\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean', add_indicator=True)),\n",
    "        ('dimred', PCA(n_components=n_components))\n",
    "    ])\n",
    "\n",
    "# Process feature groups individually\n",
    "def process_feature_groups(\n",
    "    dataset,\n",
    "    numerical_scaler_features,\n",
    "    numerical_minmax_features,\n",
    "    numerical_kbins_features,\n",
    "    numerical_dimred_features,\n",
    "    categorical_onehot_features,\n",
    "    categorical_labelencode_features,\n",
    "    unprocessed_features,\n",
    "    fitted_pipelines=None,\n",
    "    debug=False\n",
    "):\n",
    "    pipelines = {}\n",
    "    transformed_data = {}\n",
    "    feature_indices = {}  # Dictionary to store feature names and their indices\n",
    "    if fitted_pipelines is None:\n",
    "        fitted_pipelines = {}\n",
    "\n",
    "    current_index = 0  # To keep track of the position in the transformed data\n",
    "\n",
    "\n",
    "    # Numerical Scaler Features\n",
    "    if numerical_scaler_features:\n",
    "        if 'numerical_scaler' in fitted_pipelines:\n",
    "            pipeline = fitted_pipelines['numerical_scaler']\n",
    "            transformed = pipeline.transform(dataset[numerical_scaler_features])\n",
    "        else:\n",
    "            pipeline = create_numerical_scaler_pipeline()\n",
    "            transformed = pipeline.fit_transform(dataset[numerical_scaler_features])\n",
    "            fitted_pipelines['numerical_scaler'] = pipeline\n",
    "        transformed_data['numerical_scaler'] = transformed\n",
    "        # Store feature names and their indices\n",
    "        feature_indices['numerical_scaler'] = {\n",
    "            'feature_names': numerical_scaler_features,\n",
    "            'indices': list(range(current_index, current_index + transformed.shape[1]))\n",
    "        }\n",
    "        current_index += transformed.shape[1]\n",
    "        if debug:\n",
    "            print(f\"\\n[Numerical Scaler] Features: {numerical_scaler_features}\")\n",
    "            print(f\"Transformed Data Shape: {transformed.shape}\")\n",
    "\n",
    "\n",
    "    # Numerical MinMax Features\n",
    "    if numerical_minmax_features:\n",
    "        if 'numerical_minmax' in fitted_pipelines:\n",
    "            pipeline = fitted_pipelines['numerical_minmax']\n",
    "            transformed = pipeline.transform(dataset[numerical_minmax_features])\n",
    "        else:\n",
    "            pipeline = create_numerical_minmax_pipeline()\n",
    "            transformed = pipeline.fit_transform(dataset[numerical_minmax_features])\n",
    "            fitted_pipelines['numerical_minmax'] = pipeline\n",
    "        transformed_data['numerical_minmax'] = transformed\n",
    "        feature_indices['numerical_minmax'] = {\n",
    "            'feature_names': numerical_minmax_features,\n",
    "            'indices': list(range(current_index, current_index + transformed.shape[1]))\n",
    "        }\n",
    "        current_index += transformed.shape[1]\n",
    "        if debug:\n",
    "            print(f\"\\n[Numerical MinMax] Features: {numerical_minmax_features}\")\n",
    "            print(f\"Transformed Data Shape: {transformed.shape}\")\n",
    "\n",
    "    # Numerical KBins Features\n",
    "    if numerical_kbins_features:\n",
    "        if 'numerical_kbins' in fitted_pipelines:\n",
    "            pipeline = fitted_pipelines['numerical_kbins']\n",
    "            transformed_data['numerical_kbins'] = pipeline.transform(dataset[numerical_kbins_features])\n",
    "        else:\n",
    "            pipeline = create_numerical_kbins_pipeline()\n",
    "            transformed_data['numerical_kbins'] = pipeline.fit_transform(dataset[numerical_kbins_features])\n",
    "            fitted_pipelines['numerical_kbins'] = pipeline\n",
    "        if debug:\n",
    "            print(f\"\\n[Numerical KBins] Features: {numerical_kbins_features}\")\n",
    "            print(f\"Transformed Data Shape: {transformed_data['numerical_kbins'].shape}\")\n",
    "\n",
    "    # Numerical Dimensionality Reduction Features\n",
    "    if numerical_dimred_features:\n",
    "        if 'numerical_dimred' in fitted_pipelines:\n",
    "            pipeline = fitted_pipelines['numerical_dimred']\n",
    "            transformed_data['numerical_dimred'] = pipeline.transform(dataset[numerical_dimred_features])\n",
    "        else:\n",
    "            pipeline = create_numerical_dimred_pipeline(n_components=3)\n",
    "            transformed_data['numerical_dimred'] = pipeline.fit_transform(dataset[numerical_dimred_features])\n",
    "            fitted_pipelines['numerical_dimred'] = pipeline\n",
    "        if debug:\n",
    "            print(f\"\\n[Numerical DimRed] Features: {numerical_dimred_features}\")\n",
    "            print(f\"Transformed Data Shape: {transformed_data['numerical_dimred'].shape}\")\n",
    "\n",
    "    # Categorical OneHot Features\n",
    "    if categorical_onehot_features:\n",
    "        if 'categorical_onehot' in fitted_pipelines:\n",
    "            pipeline = fitted_pipelines['categorical_onehot']\n",
    "            transformed = pipeline.transform(dataset[categorical_onehot_features])\n",
    "        else:\n",
    "            pipeline = create_categorical_onehot_pipeline()\n",
    "            transformed = pipeline.fit_transform(dataset[categorical_onehot_features])\n",
    "            fitted_pipelines['categorical_onehot'] = pipeline\n",
    "        transformed_data['categorical_onehot'] = transformed\n",
    "        # Get feature names from OneHotEncoder\n",
    "        onehot_encoder = pipeline.named_steps['onehot']\n",
    "        categories = onehot_encoder.categories_\n",
    "        expanded_feature_names = []\n",
    "        for feature, cats in zip(categorical_onehot_features, categories):\n",
    "            expanded_feature_names.extend([f\"{feature}_{cat}\" for cat in cats])\n",
    "        feature_indices['categorical_onehot'] = {\n",
    "            'feature_names': expanded_feature_names,\n",
    "            'indices': list(range(current_index, current_index + transformed.shape[1]))\n",
    "        }\n",
    "        current_index += transformed.shape[1]\n",
    "        if debug:\n",
    "            print(f\"\\n[Categorical OneHot] Features: {categorical_onehot_features}\")\n",
    "            print(f\"Transformed Data Shape: {transformed.shape}\")\n",
    "\n",
    "    # Categorical LabelEncode Features\n",
    "    if categorical_labelencode_features:\n",
    "        if 'categorical_labelencode' in fitted_pipelines:\n",
    "            pipeline = fitted_pipelines['categorical_labelencode']\n",
    "            transformed_data['categorical_labelencode'] = pipeline.transform(dataset[categorical_labelencode_features])\n",
    "        else:\n",
    "            pipeline = create_categorical_labelencode_pipeline()\n",
    "            transformed_data['categorical_labelencode'] = pipeline.fit_transform(dataset[categorical_labelencode_features])\n",
    "            fitted_pipelines['categorical_labelencode'] = pipeline\n",
    "        if debug:\n",
    "            print(f\"\\n[Categorical LabelEncode] Features: {categorical_labelencode_features}\")\n",
    "            print(f\"Transformed Data Shape: {transformed_data['categorical_labelencode'].shape}\")\n",
    "\n",
    "    # Unprocessed Features\n",
    "    if unprocessed_features:\n",
    "        transformed = dataset[unprocessed_features].values\n",
    "        transformed_data['unprocessed'] = transformed\n",
    "        feature_indices['unprocessed'] = {\n",
    "            'feature_names': unprocessed_features,\n",
    "            'indices': list(range(current_index, current_index + transformed.shape[1]))\n",
    "        }\n",
    "        current_index += transformed.shape[1]\n",
    "        if debug:\n",
    "            print(f\"\\n[Unprocessed] Features: {unprocessed_features}\")\n",
    "            print(f\"Data Shape: {transformed.shape}\")\n",
    "    else:\n",
    "        transformed_data['unprocessed'] = None\n",
    "\n",
    "    return transformed_data, fitted_pipelines, feature_indices\n",
    "\n",
    "# Encode/Decode target variable (y)\n",
    "def preprocess_target(y_train, y_test=None, debug=False):\n",
    "    \"\"\"\n",
    "    Encodes the target variable into binary format if necessary and provides a way to inverse transform.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    if y_test is not None:\n",
    "        y_test_encoded = label_encoder.transform(y_test)\n",
    "    else:\n",
    "        y_test_encoded = None\n",
    "    if debug:\n",
    "        print(\"\\n[Target Variable Encoding]\")\n",
    "        print(f\"Classes: {label_encoder.classes_}\")\n",
    "        print(f\"Encoded y_train (First 5): {y_train_encoded[:5]}\")\n",
    "        if y_test_encoded is not None:\n",
    "            print(f\"Encoded y_test (First 5): {y_test_encoded[:5]}\")\n",
    "\n",
    "    return y_train_encoded, y_test_encoded, label_encoder\n",
    "\n",
    "\n",
    "\n",
    "# Combine transformed data into final arrays\n",
    "def combine_transformed_data_with_index(transformed_data, feature_indices, original_index, debug=False):\n",
    "    \"\"\"\n",
    "    Combines transformed feature arrays into a single dataset and collects feature names,\n",
    "    ensuring the original index is preserved.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    combined_feature_names = []\n",
    "    for feature_type, data in transformed_data.items():\n",
    "        if data is not None:\n",
    "            combined_data.append(data)\n",
    "            # Retrieve feature names from feature_indices\n",
    "            if feature_type in feature_indices:\n",
    "                feature_names = feature_indices[feature_type]['feature_names']\n",
    "                combined_feature_names.extend(feature_names)\n",
    "            else:\n",
    "                feature_names = [f\"{feature_type}_{i}\" for i in range(data.shape[1])]\n",
    "                combined_feature_names.extend(feature_names)\n",
    "            if debug:\n",
    "                print(f\"[Combine] Adding {feature_type}: Shape {data.shape} Features: {feature_names}\")\n",
    "    \n",
    "    combined_array = np.hstack(combined_data)\n",
    "    combined_df = pd.DataFrame(combined_array, columns=combined_feature_names)\n",
    "    combined_df.index = original_index  # Reattach original index\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n[Combine Transformed Data]\")\n",
    "        print(f\"Combined Shape: {combined_df.shape}\")\n",
    "        print(f\"Combined Columns: {list(combined_df.columns)}[:10]...\")  # Show first 10 columns\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def save_pipeline_and_assets(fitted_pipelines, feature_indices, label_encoder, path):\n",
    "    \"\"\"\n",
    "    Saves the fitted preprocessing pipelines, feature indices, and label encoder to a file.\n",
    "\n",
    "    Args:\n",
    "        fitted_pipelines (dict): Dictionary of fitted pipelines for each feature group.\n",
    "        feature_indices (dict): Dictionary of feature indices and names for each transformation step.\n",
    "        label_encoder (LabelEncoder): Fitted LabelEncoder for target variable.\n",
    "        path (str): Path to save the file.\n",
    "    \"\"\"\n",
    "    assets = {\n",
    "        \"fitted_pipelines\": fitted_pipelines,\n",
    "        \"feature_indices\": feature_indices,\n",
    "        \"label_encoder\": label_encoder,\n",
    "    }\n",
    "    joblib.dump(assets, path)\n",
    "    print(f\"Pipeline and assets saved to {path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # from preprocessor_recommendations import filter_features_by_type, analyze_categorical_features, analyze_numerical_features_enhanced_v2, \n",
    "    # from preprocessor_encoding_filtered_datasets import filter_features, check_on_processed_datasets\n",
    "    from collections import Counter\n",
    "    # File paths\n",
    "    features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "    dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    \n",
    "    # from data_loader_post_select_features import load_selected_features_data\n",
    "    # Example usage:\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        y_variable='result',\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    # Step 1: Filter features\n",
    "    categorical_df, numerical_df = filter_features_by_type(final_ml_df_selected_features, debug=debug)\n",
    "\n",
    "    # Step 2: Analyze categorical features\n",
    "    categorical_info_df = analyze_categorical_features(\n",
    "        categorical_df,\n",
    "        low_cardinality_threshold=10,\n",
    "        high_cardinality_threshold=50,\n",
    "        missing_threshold=0.3,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Step 3: Analyze numerical features and handle outliers automatically\n",
    "    numerical_info_df = analyze_numerical_features_enhanced_v2(\n",
    "        numerical_df,\n",
    "        y_feature=None,\n",
    "        zscore_threshold=zscore_threshold,\n",
    "        tukey_threshold=tukey_threshold,\n",
    "        missing_threshold=0.5,\n",
    "        high_cardinality_threshold=1000,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "\n",
    "    debug = True\n",
    "\n",
    "    # Assuming numerical_info_df, categorical_info_df, and final_ml_df_selected_features are already defined\n",
    "    y_variable = 'result'\n",
    "\n",
    "    print(\"\\n[Initial Dataset Info]\")\n",
    "    print(f\"Columns to work with: {final_ml_df_selected_features.columns.tolist()}\")\n",
    "    print(f\"Categorical Features: {categorical_info_df['Feature'].tolist()}\")\n",
    "    print(f\"Numerical Features: {numerical_info_df['Feature'].tolist()}\")\n",
    "\n",
    "    # Step 1: Split dataset into features (X) and target (y)\n",
    "    X = final_ml_df_selected_features.drop(columns=[y_variable])\n",
    "    y = final_ml_df_selected_features[y_variable]\n",
    "\n",
    "    # Step 2: Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Train-Test Split]\")\n",
    "    print(f\"X_train Shape: {X_train.shape}\")\n",
    "    #print(f\"X_train: {X_train.head()}\")\n",
    "    print(f\"X_test Shape: {X_test.shape}\")\n",
    "    print(f\"y_train Shape: {y_train.shape}\")\n",
    "    print(f\"y_test Shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "    # add in SMOTE TO TRAINING DATASETS ONLY \n",
    "\n",
    "    # from smote_automation import  check_dataset_for_smote, apply_smote\n",
    "\n",
    "    # Analyze dataset for SMOTE\n",
    "    smote_analysis = check_dataset_for_smote(X_train, y_train, debug=True)\n",
    "    print(\"SMOTE Analysis Recommendations:\", smote_analysis[\"recommendations\"])\n",
    "\n",
    "    # Apply SMOTE\n",
    "    X_train_resampled, y_train_resampled, smote_used = apply_smote(X_train, y_train, smote_analysis[\"recommendations\"], debug=True)\n",
    "    print(\"Applied SMOTE Variant:\", smote_used)\n",
    "    print(\"Resampled Class Distribution:\", Counter(y_train_resampled))\n",
    "\n",
    "    logging.info(f\"SMOTE Technique Used: {smote_used}\")\n",
    "\n",
    "    # Step 3: Filter features\n",
    "    (\n",
    "        numerical_scaler_features,\n",
    "        numerical_minmax_features,\n",
    "        numerical_kbins_features,\n",
    "        numerical_dimred_features,\n",
    "        onehot_features,\n",
    "        labelencode_features,\n",
    "        unprocessed_features\n",
    "    ) = filter_features(\n",
    "        numerical_info_df, categorical_info_df, final_ml_df_selected_features, y_variable, debug=debug\n",
    "    )\n",
    "    \n",
    "    # Save original feature names\n",
    "    original_feature_names = {\n",
    "        'numerical_scaler': numerical_scaler_features,\n",
    "        'numerical_minmax': numerical_minmax_features,\n",
    "        'numerical_kbins': numerical_kbins_features,\n",
    "        'numerical_dimred': numerical_dimred_features,\n",
    "        'onehot': onehot_features,\n",
    "        'labelencode': labelencode_features,\n",
    "        'unprocessed': unprocessed_features\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # Step 4: Preprocess feature groups on training data\n",
    "    transformed_data_train, fitted_pipelines, feature_indices_train = process_feature_groups(\n",
    "        X_train_resampled,\n",
    "        numerical_scaler_features,\n",
    "        numerical_minmax_features,\n",
    "        numerical_kbins_features,\n",
    "        numerical_dimred_features,\n",
    "        onehot_features,\n",
    "        labelencode_features,\n",
    "        unprocessed_features,\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 5: Preprocess feature groups on testing data using fitted pipelines\n",
    "    transformed_data_test, _, _ = process_feature_groups(\n",
    "        X_test,\n",
    "        numerical_scaler_features,\n",
    "        numerical_minmax_features,\n",
    "        numerical_kbins_features,\n",
    "        numerical_dimred_features,\n",
    "        onehot_features,\n",
    "        labelencode_features,\n",
    "        unprocessed_features,\n",
    "        fitted_pipelines=fitted_pipelines,\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Debugging before combining transformed data\n",
    "    print(\"\\n[Debug] Transformed Data Keys and Shapes:\")\n",
    "    for key, value in transformed_data_train.items():\n",
    "        if value is not None:\n",
    "            print(f\"{key}: {value.shape}\")\n",
    "        \n",
    "    # Step 6: Combine transformed features and collect feature names\n",
    "    X_train_transformed = combine_transformed_data_with_index(transformed_data_train, feature_indices_train, X_train_resampled.index, debug=debug)\n",
    "    X_test_transformed = combine_transformed_data_with_index(transformed_data_test, feature_indices_train, X_test.index, debug=debug)\n",
    "\n",
    "    print(f\"\\n[Transformed Data Shapes]\")\n",
    "    print(f\"X_train_transformed Shape: {X_train_transformed.shape}\")\n",
    "    print(f\"X_test_transformed Shape: {X_test_transformed.shape}\")\n",
    "\n",
    "    # Step 7: Preprocess target variable\n",
    "    y_train_encoded, y_test_encoded, label_encoder = preprocess_target(y_train_resampled, y_test, debug=debug)\n",
    "\n",
    "    # Final data shapes\n",
    "    print(f\"\\n[Final Data Shapes]\")\n",
    "    print(f\"X_train_transformed: {X_train_transformed.shape}\")\n",
    "    print(f\"X_test_transformed: {X_test_transformed.shape}\")\n",
    "    print(f\"y_train_encoded: {y_train_encoded.shape}\")\n",
    "    print(f\"y_test_encoded: {y_test_encoded.shape}\")\n",
    "\n",
    "    #------------------------------------------\n",
    "    # Saving/Loading Pipelines/Feature Lists\n",
    "    # Define base paths\n",
    "    preprocessing_assets_path = '../../data/model/pipeline/preprocessing_assets.pkl'\n",
    "\n",
    "    # Save pipelines, feature indices, and label encoder\n",
    "    save_pipeline_and_assets(fitted_pipelines, feature_indices_train, label_encoder, path=preprocessing_assets_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ../../src/freethrow_predictions/ml/classification_preprocessor/preprocessor_recommendations.py\n",
    "# Data Preprocessing Recommendation Code\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "from scipy.stats import shapiro, normaltest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures, KBinsDiscretizer, Binarizer, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "from ml.classification_preprocessor.smote_automation import check_dataset_for_smote, apply_smote\n",
    "\n",
    "\n",
    "def filter_features_by_type(final_ml_df, debug=False):\n",
    "    \"\"\"\n",
    "    Separates features into categorical and numerical types for further processing.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    excluded_features = []\n",
    "\n",
    "    for col in final_ml_df.columns:\n",
    "        # Exclude known ID columns or irrelevant features\n",
    "        if col in ['player_participant_id', 'trial_id', 'shot_id']:\n",
    "            excluded_features.append(col)\n",
    "            continue\n",
    "        \n",
    "        # Identify categorical features\n",
    "        if final_ml_df[col].dtype in ['object', 'category'] or final_ml_df[col].nunique() < 10:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            numerical_features.append(col)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Categorical Features: {categorical_features}\")\n",
    "        print(f\"Numerical Features: {numerical_features}\")\n",
    "        print(f\"Excluded Features: {excluded_features}\")\n",
    "\n",
    "    return final_ml_df[categorical_features], final_ml_df[numerical_features]\n",
    "\n",
    "\n",
    "# Rules for Numerical Preprocessing\n",
    "\n",
    "#     Scaling/Normalization:\n",
    "#         Use StandardScaler for features that follow a normal distribution.\n",
    "#         Use MinMaxScaler for non-normal distributed features with a wide range of values.\n",
    "#         Use Normalizer for features where relative magnitudes are critical (e.g., vectors).\n",
    "\n",
    "#     Feature Transformation:\n",
    "#         Use KBinsDiscretizer for binning features with continuous values and less than 10 unique bins (e.g., ages, income categories).\n",
    "#         Use Binarizer for threshold-based classification (e.g., binary labels or thresholding a numeric feature like probability).\n",
    "#         Use PolynomialFeatures for generating interaction terms if the feature shows a high correlation with the target.\n",
    "\n",
    "#     Outlier Handling:\n",
    "#         Apply Z-Score for outlier detection in normally distributed data.\n",
    "#         Use Tukey's Method (IQR-based) for skewed or non-normal distributions.\n",
    "\n",
    "#     Imputation:\n",
    "#         Use SimpleImputer with strategy 'mean' or 'median' for low missingness (<30%).\n",
    "#         Use IterativeImputer for multivariate datasets with moderate missingness (30-50%).\n",
    "#         Exclude features with high missingness (>50%).\n",
    "\n",
    "#     High Cardinality:\n",
    "#         Apply dimensionality reduction techniques such as PCA or QuantileTransformer for numerical features with a high number of unique values (>1000).\n",
    "\n",
    "#     Special Cases:\n",
    "#         Use Log Transformation for skewed distributions (e.g., long tails).\n",
    "#         Apply clipping for extreme outliers based on percentiles (e.g., clipping to the 5th and 95th percentiles).\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def analyze_numerical_features_enhanced_v2(\n",
    "    numerical_df,\n",
    "    y_feature=None,\n",
    "    zscore_threshold=3,\n",
    "    tukey_threshold=1.5,\n",
    "    missing_threshold=0.5,\n",
    "    high_cardinality_threshold=1000,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes numerical features and recommends preprocessing based on explicit rules.\n",
    "    \"\"\"\n",
    "    feature_info = []\n",
    "    total_rows = len(numerical_df)\n",
    "\n",
    "    for col in numerical_df.columns:\n",
    "        if col == y_feature:\n",
    "            continue\n",
    "        try:\n",
    "            # Core Metrics\n",
    "            unique_values = numerical_df[col].nunique()\n",
    "            missing_values = numerical_df[col].isnull().sum()\n",
    "            missing_ratio = missing_values / total_rows\n",
    "            valid_values = numerical_df[col].dropna()\n",
    "\n",
    "            # Normality Tests\n",
    "            is_normal_shapiro = is_normal_ad = None\n",
    "            if len(valid_values) > 8:  # Minimum samples for normality tests\n",
    "                shapiro_stat, shapiro_p = shapiro(valid_values)\n",
    "                is_normal_shapiro = shapiro_p > 0.05\n",
    "                ad_stat, ad_p = normaltest(valid_values)\n",
    "                is_normal_ad = ad_p > 0.05\n",
    "\n",
    "            # Outlier Detection\n",
    "            if is_normal_shapiro:\n",
    "                outlier_method = \"Z-Score\"\n",
    "                outlier_reason = \"Assumes normal distribution for outlier detection.\"\n",
    "            else:\n",
    "                outlier_method = \"Tukey's Method\"\n",
    "                outlier_reason = \"Handles non-normal distributions effectively.\"\n",
    "\n",
    "            # Preprocessing Suggestion\n",
    "            if unique_values > high_cardinality_threshold:\n",
    "                preprocessing = \"Dimensionality Reduction\"\n",
    "                preprocessing_reason = \"High cardinality can lead to overfitting; dimensionality reduction avoids it.\"\n",
    "            elif unique_values < 10:\n",
    "                preprocessing = \"KBinsDiscretizer\"\n",
    "                preprocessing_reason = \"Low cardinality; binning simplifies representation.\"\n",
    "            elif is_normal_shapiro:\n",
    "                preprocessing = \"StandardScaler\"\n",
    "                preprocessing_reason = \"Feature is normally distributed; scaling to zero mean and unit variance is recommended.\"\n",
    "            elif len(valid_values) < 100:\n",
    "                preprocessing = \"Normalizer\"\n",
    "                preprocessing_reason = \"Low sample size; normalization avoids over-scaling.\"\n",
    "            else:\n",
    "                preprocessing = \"MinMaxScaler\"\n",
    "                preprocessing_reason = \"Feature is not normally distributed; MinMax scaling normalizes values to [0,1].\"\n",
    "\n",
    "            # Imputation Recommendation\n",
    "            if missing_ratio == 0:\n",
    "                imputation = \"No Imputation\"\n",
    "                imputation_reason = \"No missing values.\"\n",
    "            elif missing_ratio < 0.3:\n",
    "                imputation = \"SimpleImputer\"\n",
    "                imputation_reason = \"Moderate missingness; mean or median imputation suffices.\"\n",
    "            elif missing_ratio <= missing_threshold:\n",
    "                imputation = \"IterativeImputer\"\n",
    "                imputation_reason = \"High missingness; multivariate imputation preserves feature relationships.\"\n",
    "            else:\n",
    "                imputation = \"Exclude Feature\"\n",
    "                imputation_reason = \"Exceeds missingness threshold.\"\n",
    "\n",
    "            # Record Results\n",
    "            feature_info.append({\n",
    "                \"Feature\": col,\n",
    "                \"Data Type\": str(numerical_df[col].dtype),\n",
    "                \"Preprocessing Suggestion\": preprocessing,\n",
    "                \"Reason\": f\"Unique Values: {unique_values}, Missing Ratio: {missing_ratio:.2%}, {preprocessing_reason}\",\n",
    "                \"Imputation Recommendation\": imputation,\n",
    "                \"Imputation Reason\": imputation_reason,\n",
    "                \"Outlier Method\": outlier_method,\n",
    "                \"Outlier Reason\": outlier_reason,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Error analyzing feature {col}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(feature_info)\n",
    "\n",
    "\n",
    "def analyze_categorical_features(\n",
    "    categorical_df,\n",
    "    low_cardinality_threshold=10,\n",
    "    high_cardinality_threshold=50,\n",
    "    missing_threshold=0.3,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes categorical features for preprocessing and recommends imputers and encoders.\n",
    "    \"\"\"\n",
    "    feature_info = []\n",
    "\n",
    "    for col in categorical_df.columns:\n",
    "        try:\n",
    "            unique_values = categorical_df[col].nunique()\n",
    "            missing_values = categorical_df[col].isnull().sum()\n",
    "            missing_ratio = missing_values / len(categorical_df)\n",
    "\n",
    "            # Encoding Suggestion\n",
    "            if unique_values == 1:\n",
    "                encoding = \"Drop Column\"\n",
    "                encoding_reason = \"Only one unique value; not useful for modeling.\"\n",
    "            elif unique_values <= low_cardinality_threshold:\n",
    "                encoding = \"LabelEncoder\"\n",
    "                encoding_reason = \"Low cardinality; integer encoding is efficient.\"\n",
    "            elif unique_values <= high_cardinality_threshold:\n",
    "                encoding = \"OneHotEncoder\"\n",
    "                encoding_reason = \"Moderate cardinality; one-hot encoding balances simplicity and precision.\"\n",
    "            else:\n",
    "                encoding = \"Group or Target Encoding\"\n",
    "                encoding_reason = \"High cardinality; direct encoding may lead to inefficiencies.\"\n",
    "\n",
    "            # Imputation Recommendation\n",
    "            if missing_ratio == 0:\n",
    "                imputation = \"No Imputation\"\n",
    "                imputation_reason = \"No missing values.\"\n",
    "            elif missing_ratio < missing_threshold:\n",
    "                imputation = \"SimpleImputer\"\n",
    "                imputation_reason = \"Moderate missingness; most_frequent strategy is effective.\"\n",
    "            else:\n",
    "                imputation = \"Exclude Feature\"\n",
    "                imputation_reason = \"Exceeds missingness threshold.\"\n",
    "\n",
    "            # Record Results\n",
    "            feature_info.append({\n",
    "                \"Feature\": col,\n",
    "                \"Data Type\": str(categorical_df[col].dtype),\n",
    "                \"Encoding Suggestion\": encoding,\n",
    "                \"Reason\": f\"Unique Values: {unique_values}, Missing Ratio: {missing_ratio:.2%}, {encoding_reason}\",\n",
    "                \"Imputation Recommendation\": imputation,\n",
    "                \"Imputation Reason\": imputation_reason,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Error analyzing feature {col}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(feature_info)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    debug = True\n",
    "    # Example parameter tuning\n",
    "    zscore_threshold = 3\n",
    "    tukey_threshold = 1.5\n",
    "    max_rows_shapiro = 5000\n",
    "    min_rows_normality_percentage = 0.05\n",
    "    high_outlier_percentage = 5\n",
    "    correlation_threshold = 0.8  # Threshold for multicollinearity check\n",
    "\n",
    "    # File paths\n",
    "    features_path = '../../data/model/pipeline/final_ml_df_selected_features_columns.pkl'\n",
    "    dataset_path = \"../../data/processed/final_ml_dataset.csv\"\n",
    "    \n",
    "    # from ml.feature_selection.data_loader_post_select_features import load_selected_features_data\n",
    "    # Example usage:\n",
    "    final_ml_df_selected_features = load_selected_features_data(\n",
    "        features_path=features_path,\n",
    "        dataset_path=dataset_path,\n",
    "        y_variable='result',\n",
    "        debug=False\n",
    "    )\n",
    "    final_ml_df = pd.read_csv(dataset_path)\n",
    "    print(\" final_ml_df shape = \", final_ml_df.shape)\n",
    "    print(\" final_ml_df_selected_features shape = \", final_ml_df_selected_features.shape)\n",
    "    # Step 1: Filter features\n",
    "    categorical_df, numerical_df = filter_features_by_type(final_ml_df_selected_features, debug=debug)\n",
    "\n",
    "    # Step 2: Analyze categorical features\n",
    "    categorical_info_df = analyze_categorical_features(\n",
    "        categorical_df,\n",
    "        low_cardinality_threshold=10,\n",
    "        high_cardinality_threshold=50,\n",
    "        missing_threshold=0.3,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Step 3: Analyze numerical features and handle outliers automatically\n",
    "    numerical_info_df = analyze_numerical_features_enhanced_v2(\n",
    "        numerical_df,\n",
    "        y_feature=None,\n",
    "        zscore_threshold=zscore_threshold,\n",
    "        tukey_threshold=tukey_threshold,\n",
    "        missing_threshold=0.5,\n",
    "        high_cardinality_threshold=1000,\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCategorical Features Analysis:\")\n",
    "    print(categorical_info_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\nNumerical Features Analysis:\")\n",
    "    print(numerical_info_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ft_bio_predictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
