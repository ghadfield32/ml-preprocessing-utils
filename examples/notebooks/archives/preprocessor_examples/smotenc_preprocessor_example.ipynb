{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import unittest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Configure Logging\n",
    "# ----------------------------\n",
    "def configure_logging(debug: bool = False):\n",
    "    \"\"\"\n",
    "    Configure logging settings.\n",
    "    \n",
    "    Args:\n",
    "        debug (bool): Flag to enable detailed debugging.\n",
    "        \n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    log_level = logging.DEBUG if debug else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"preprocessing_pipeline.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('PreprocessingPipeline')\n",
    "    return logger\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Define Feature Categorization within Pipeline\n",
    "# ----------------------------\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(self, config_path: str = None\n",
    "                 ,numerical_features: List[str] = None\n",
    "                 ,ordinal_features: List[str] = None\n",
    "                 ,nominal_features: List[str] = None\n",
    "                 ,target_variable: List[str] = None\n",
    "                 , debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the PreprocessingPipeline with optional debugging and configuration.\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): Path to the YAML configuration file.\n",
    "            debug (bool): Flag to enable detailed debugging.\n",
    "        \n",
    "        Example:\n",
    "            pipeline = PreprocessingPipeline(config_path='config.yaml')\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self.logger = configure_logging(debug=self.debug)\n",
    "        self.pipeline = None\n",
    "        self.numerical_features = numerical_features\n",
    "        self.ordinal_features = ordinal_features\n",
    "        self.nominal_features = nominal_features\n",
    "        self.target_variable = target_variable\n",
    "        self.classifier = None\n",
    "        \n",
    "        if config_path:\n",
    "            with open(config_path, 'r') as file:\n",
    "                config = yaml.safe_load(file)\n",
    "            self.target_variable = config['target_variable']\n",
    "            classifier_info = config.get('classifier', {})\n",
    "            classifier_type = classifier_info.get('type', 'LogisticRegression')\n",
    "            classifier_params = classifier_info.get('parameters', {})\n",
    "            \n",
    "            # Dynamically import the classifier\n",
    "            self.classifier = getattr(__import__('sklearn.linear_model', fromlist=[classifier_type]), classifier_type)(**classifier_params)\n",
    "            self.debug = config.get('debug', False)\n",
    "            self.logger = configure_logging(debug=self.debug)\n",
    "            self.logger.debug(f\"Pipeline initialized with classifier: {self.classifier}\")\n",
    "        else:\n",
    "            self.logger.error(\"No configuration file provided.\")\n",
    "            raise ValueError(\"Configuration file path must be provided.\")\n",
    "    \n",
    "    def categorize_features(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Automatically categorize features into numerical, ordinal, and nominal based on predefined lists.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "        \n",
    "        Raises:\n",
    "            KeyError: If any specified columns are missing from the DataFrame.\n",
    "        \n",
    "        Example:\n",
    "            pipeline.categorize_features(df)\n",
    "        \"\"\"\n",
    "        # Define feature categories\n",
    "        feature_lists = {\n",
    "            'numerical_features': self.numerical_features,\n",
    "            'ordinal_features': self.ordinal_features,\n",
    "            'nominal_features': self.nominal_features\n",
    "        }\n",
    "        \n",
    "        # Verify that all specified columns exist\n",
    "        missing_columns = {}\n",
    "        for category, columns in feature_lists.items():\n",
    "            for col in columns:\n",
    "                if col not in df.columns:\n",
    "                    missing_columns.setdefault(category, []).append(col)\n",
    "        \n",
    "        if missing_columns:\n",
    "            for category, cols in missing_columns.items():\n",
    "                self.logger.error(f\"Missing columns in category '{category}': {cols}\")\n",
    "            raise KeyError(\"One or more columns specified in feature lists are missing from the DataFrame.\")\n",
    "        else:\n",
    "            self.logger.info(\"✅ All specified columns exist in the DataFrame.\")\n",
    "        \n",
    "        # Optionally, log the categorized features\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Numerical Features: {self.numerical_features}\")\n",
    "            self.logger.debug(f\"Ordinal Features: {self.ordinal_features}\")\n",
    "            self.logger.debug(f\"Nominal Features: {self.nominal_features}\")\n",
    "    \n",
    "    def create_preprocessing_pipeline(self, X_train: pd.DataFrame) -> ImbPipeline:\n",
    "        \"\"\"\n",
    "        Create a preprocessing, resampling, and modeling pipeline compatible with SMOTENC.\n",
    "        \n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training feature set for fitting transformers.\n",
    "        \n",
    "        Returns:\n",
    "            ImbPipeline: An imblearn Pipeline object including preprocessing, SMOTENC, and the model.\n",
    "        \n",
    "        Example:\n",
    "            pipeline = pipeline.create_preprocessing_pipeline(X_train)\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Creating preprocessing, resampling, and modeling pipeline with numerical, ordinal, and nominal transformers.\")\n",
    "        \n",
    "        # Numerical Transformer\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        self.logger.debug(\"Numerical transformer created.\")\n",
    "        \n",
    "        # Ordinal Categorical Transformer\n",
    "        ordinal_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('ordinal_encoder', OrdinalEncoder())\n",
    "        ])\n",
    "        \n",
    "        self.logger.debug(\"Ordinal transformer created.\")\n",
    "        \n",
    "        # Nominal Categorical Transformers: Use OneHotEncoder\n",
    "        nominal_transformers = []\n",
    "        for feature in self.nominal_features:\n",
    "            transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ])\n",
    "            nominal_transformers.append((f'onehot_enc_{feature}', transformer, [feature]))\n",
    "            self.logger.debug(f\"Nominal transformer with OneHotEncoder for '{feature}' created.\")\n",
    "        \n",
    "        # Combine Transformers using ColumnTransformer\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num', numerical_transformer, self.numerical_features),\n",
    "            ('ord', ordinal_transformer, self.ordinal_features),\n",
    "            *nominal_transformers  # Unpack nominal transformers\n",
    "        ], remainder='drop')\n",
    "        \n",
    "        self.logger.debug(\"ColumnTransformer created with numerical, ordinal, and nominal transformers.\")\n",
    "        \n",
    "        # Fit the preprocessor to determine feature indices\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "        \n",
    "        # Determine the number of output features after preprocessing\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        self.logger.debug(f\"Feature names after preprocessing: {feature_names}\")\n",
    "        \n",
    "        # Identify indices of all categorical features (from OneHotEncoder)\n",
    "        categorical_indices = []\n",
    "        start_idx = 0\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name.startswith('onehot_enc_'):\n",
    "                ohe = transformer.named_steps['onehot_encoder']\n",
    "                n_categories = len(ohe.categories_[0])\n",
    "                categorical_indices.extend(list(range(start_idx, start_idx + n_categories)))\n",
    "                self.logger.debug(f\"Feature '{name}' has {n_categories} categories; indices {list(range(start_idx, start_idx + n_categories))}.\")\n",
    "                start_idx += n_categories\n",
    "            elif name in ['num', 'ord']:\n",
    "                n_features = len(features)\n",
    "                self.logger.debug(f\"Feature '{name}' has {n_features} features; advancing start index by {n_features}.\")\n",
    "                start_idx += n_features\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transformer '{name}'. Skipping index calculation.\")\n",
    "        \n",
    "        self.logger.debug(f\"Categorical feature indices for SMOTENC: {categorical_indices}\")\n",
    "        \n",
    "        # Initialize SMOTENC with correct categorical feature indices\n",
    "        smote_nc = SMOTENC(\n",
    "            categorical_features=categorical_indices,\n",
    "            sampling_strategy='auto',\n",
    "            k_neighbors=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.logger.debug(\"SMOTENC initialized with correct categorical_features.\")\n",
    "        \n",
    "        # Create the full imblearn pipeline with preprocessing, SMOTENC, and the classifier\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('smote_nc', smote_nc),\n",
    "            ('classifier', self.classifier)\n",
    "        ])\n",
    "        \n",
    "        self.logger.debug(\"Full preprocessing, SMOTENC, and classifier pipeline created.\")\n",
    "        \n",
    "        return pipeline\n",
    "    \n",
    "    def inverse_transform_data(\n",
    "        self,\n",
    "        X_transformed: np.ndarray\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "        \n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame.\n",
    "        \n",
    "        Example:\n",
    "            X_inverse = pipeline.inverse_transform_data(X_transformed)\n",
    "        \"\"\"\n",
    "        preprocessor = self.pipeline.named_steps['preprocessor']\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        logger.debug(\"Starting inverse transformation.\")\n",
    "        \n",
    "        # Initialize dictionaries to hold inverse-transformed data\n",
    "        inverse_data = {}\n",
    "        \n",
    "        # Initialize index tracker\n",
    "        start_idx = 0\n",
    "        \n",
    "        # Iterate through each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                continue  # Skip any remainder features\n",
    "            # Extract the transformed data for current transformer\n",
    "            if name == 'num':\n",
    "                end_idx = start_idx + len(features)\n",
    "                numerical_data = X_transformed[:, start_idx:end_idx]\n",
    "                numerical_inverse = transformer.named_steps['scaler'].inverse_transform(\n",
    "                    numerical_data\n",
    "                )\n",
    "                inverse_data.update({feature: numerical_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                logger.debug(f\"Numerical features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name == 'ord':\n",
    "                end_idx = start_idx + len(features)\n",
    "                ordinal_data = X_transformed[:, start_idx:end_idx]\n",
    "                ordinal_inverse = transformer.named_steps['ordinal_encoder'].inverse_transform(\n",
    "                    ordinal_data\n",
    "                )\n",
    "                inverse_data.update({feature: ordinal_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                logger.debug(f\"Ordinal features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name.startswith('onehot_enc_'):\n",
    "                # For OneHotEncoder, need to inverse transform multiple columns\n",
    "                transformer_steps = transformer.named_steps\n",
    "                onehot_encoder = transformer_steps['onehot_encoder']\n",
    "                # Get number of categories for this feature\n",
    "                n_categories = len(onehot_encoder.categories_[0])\n",
    "                end_idx = start_idx + n_categories\n",
    "                nominal_data = X_transformed[:, start_idx:end_idx]\n",
    "                nominal_inverse = onehot_encoder.inverse_transform(nominal_data)\n",
    "                inverse_data.update({feature: nominal_inverse[:, 0] for feature in features})\n",
    "                logger.debug(f\"Nominal features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            else:\n",
    "                logger.warning(f\"Unknown transformer '{name}'. Skipping inversion.\")\n",
    "        \n",
    "        # Create the inverse-transformed DataFrame\n",
    "        inverse_df = pd.DataFrame(inverse_data)\n",
    "        \n",
    "        logger.debug(\"Inverse-transformed DataFrame constructed.\")\n",
    "        \n",
    "        logger.info(\"✅ Inverse transformation completed successfully.\")\n",
    "        \n",
    "        return inverse_df\n",
    "    \n",
    "    def predict(self, X_new: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Preprocess new data, perform predictions, and inverse transform for interpretability.\n",
    "        \n",
    "        Args:\n",
    "            X_new (pd.DataFrame): New data for prediction.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing preprocessed data and inverse-transformed DataFrame with predictions.\n",
    "        \n",
    "        Example:\n",
    "            prediction_output = pipeline.predict(X_new)\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting prediction process.\")\n",
    "        \n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Pipeline has not been trained. Please run the training process first.\")\n",
    "            raise AttributeError(\"Pipeline has not been trained. Please run the training process first.\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.pipeline.predict(X_new)\n",
    "        self.logger.info(\"✅ Predictions made on new data.\")\n",
    "        \n",
    "        # Transform and inverse transform the new data\n",
    "        X_preprocessed = self.pipeline.named_steps['preprocessor'].transform(X_new)\n",
    "        X_inverse = self.inverse_transform_data(\n",
    "            X_transformed=X_preprocessed\n",
    "        )\n",
    "        self.logger.debug(\"Inverse transformation applied to new data.\")\n",
    "        \n",
    "        # Attach predictions to the inverse-transformed DataFrame\n",
    "        X_inverse['predictions'] = y_pred\n",
    "        self.logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "        \n",
    "        # Prepare the output dictionary\n",
    "        output = {\n",
    "            'X_preprocessed': X_preprocessed,\n",
    "            'X_inverse': X_inverse\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def train(self, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "        \"\"\"\n",
    "        Train the preprocessing, resampling, and classification pipeline.\n",
    "        \n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training feature set.\n",
    "            y_train (pd.Series): Training target variable.\n",
    "            X_test (pd.DataFrame): Testing feature set.\n",
    "            y_test (pd.Series): Testing target variable.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing preprocessed and inverse-transformed datasets and performance metrics.\n",
    "        \n",
    "        Example:\n",
    "            training_output = pipeline.train(X_train, y_train, X_test, y_test)\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting training process.\")\n",
    "        \n",
    "        # Print class distribution in training set\n",
    "        print(\"Training Set Class Distribution:\")\n",
    "        print(y_train.value_counts())\n",
    "        \n",
    "        # Create and fit the preprocessing, resampling, and modeling pipeline\n",
    "        self.pipeline = self.create_preprocessing_pipeline(X_train)\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        self.logger.info(\"✅ Preprocessing, SMOTENC, and Logistic Regression pipeline fitted on training data.\")\n",
    "    \n",
    "        # Make predictions on the test set\n",
    "        y_pred = self.pipeline.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = cross_val_score(self.pipeline, X_train, y_train, scoring='precision', cv=5).mean()\n",
    "        recall = cross_val_score(self.pipeline, X_train, y_train, scoring='recall', cv=5).mean()\n",
    "        f1 = cross_val_score(self.pipeline, X_train, y_train, scoring='f1', cv=5).mean()\n",
    "        roc_auc = cross_val_score(self.pipeline, X_train, y_train, scoring='roc_auc', cv=5).mean()\n",
    "        \n",
    "        # Log performance metrics\n",
    "        self.logger.info(f\"✅ Model Accuracy on Test Set: {accuracy:.2f}\")\n",
    "        self.logger.info(f\"✅ Model Precision: {precision:.2f}\")\n",
    "        self.logger.info(f\"✅ Model Recall: {recall:.2f}\")\n",
    "        self.logger.info(f\"✅ Model F1-Score: {f1:.2f}\")\n",
    "        self.logger.info(f\"✅ Model ROC-AUC: {roc_auc:.2f}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Store performance metrics\n",
    "        performance_metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "        \n",
    "        # Transform the training and test sets\n",
    "        X_train_preprocessed = self.pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "        X_test_preprocessed = self.pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "        \n",
    "        # Inverse transform the test set for interpretability\n",
    "        X_test_inverse = self.inverse_transform_data(\n",
    "            X_transformed=X_test_preprocessed\n",
    "        )\n",
    "        self.logger.info(\"✅ Inverse transformation applied to test data.\")\n",
    "        \n",
    "        # Preserve the original indexing\n",
    "        X_test_inverse.index = X_test.index\n",
    "        self.logger.debug(\"Index preserved for inverse-transformed test data.\")\n",
    "        \n",
    "        # Optionally, save performance metrics to a file for monitoring\n",
    "        performance_df = pd.DataFrame([performance_metrics])\n",
    "        performance_df.to_csv('performance_metrics.csv', index=False)\n",
    "        self.logger.info(\"✅ Performance metrics saved to 'performance_metrics.csv'.\")\n",
    "        \n",
    "        # Prepare the output dictionary\n",
    "        output = {\n",
    "            'X_train_preprocessed': X_train_preprocessed,\n",
    "            'X_test_preprocessed': X_test_preprocessed,\n",
    "            'y_train_preprocessed': y_train,\n",
    "            'y_test_preprocessed': y_test,\n",
    "            'X_test_inverse': X_test_inverse,\n",
    "            'performance_metrics': performance_metrics\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ----------------------------\n",
    "# Step 7: Define the Main Execution Logic\n",
    "# ----------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate training and prediction using the PreprocessingPipeline.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Training Phase\n",
    "    # ----------------------------\n",
    "    numerical_features = ['age']  # Update as needed\n",
    "    ordinal_features = ['education_level', 'experience']\n",
    "    nominal_features = ['gender', 'city', 'department']\n",
    "    target_variable = [\"salary\"]\n",
    "    # Initialize the PreprocessingPipeline with configuration file\n",
    "    pipeline = PreprocessingPipeline(config_path='config.yaml'\n",
    "                                     ,numerical_features= numerical_features \n",
    "                                     ,ordinal_features = ordinal_features\n",
    "                                     ,nominal_features = nominal_features\n",
    "                                     ,target_variable = target_variable)\n",
    "    \n",
    "    # Sample Training Data; replace with your actual data\n",
    "    data = {\n",
    "        'age': [25, 32, 47, 51, 22, 33, 45, 52],\n",
    "        'salary': [0, 1, 1, 1, 0, 1, 1, 1],  # Binary target for Logistic Regression\n",
    "        'education_level': ['Bachelors', 'Masters', 'PhD', 'Bachelors', 'Masters', 'PhD', 'Bachelors', 'Masters'],\n",
    "        'experience': ['Junior', 'Mid', 'Senior', 'Senior', 'Mid', 'Senior', 'Junior', 'Mid'],\n",
    "        'gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "        'city': ['New York', 'Chicago', 'Los Angeles', 'Houston', 'Chicago', 'Houston', 'New York', 'Los Angeles'],\n",
    "        'department': ['Sales', 'Engineering', 'HR', 'Engineering', 'Sales', 'HR', 'Sales', 'Engineering']\n",
    "    }\n",
    "\n",
    "    # Expanding the dataset with more rows\n",
    "    additional_rows = 50\n",
    "\n",
    "    # Generate new rows\n",
    "    expanded_data = {\n",
    "        'age': np.random.randint(20, 60, additional_rows),\n",
    "        'salary': np.random.choice([0, 1], additional_rows),\n",
    "        'education_level': np.random.choice(['Bachelors', 'Masters', 'PhD'], additional_rows),\n",
    "        'experience': np.random.choice(['Junior', 'Mid', 'Senior'], additional_rows),\n",
    "        'gender': np.random.choice(['Male', 'Female'], additional_rows),\n",
    "        'city': np.random.choice(['New York', 'Chicago', 'Los Angeles', 'Houston'], additional_rows),\n",
    "        'department': np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing', 'Finance'], additional_rows),\n",
    "    }\n",
    "    # Create new DataFrame and append to existing data\n",
    "    df_pre_expansion = pd.DataFrame(data)\n",
    "\n",
    "    df = pd.concat([df_pre_expansion, pd.DataFrame(expanded_data)], ignore_index=True)\n",
    "\n",
    "    # Define target and features\n",
    "    X = df.drop('salary', axis=1)\n",
    "    y = df['salary']\n",
    "    \n",
    "\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.25,\n",
    "        random_state=42,\n",
    "        stratify=y  # Ensure proportional representation of classes\n",
    "    )\n",
    "    pipeline.logger.info(\"✅ Data split into training and testing sets successfully.\")\n",
    "    \n",
    "    # Train the pipeline and model\n",
    "    training_output = pipeline.train(X_train, y_train, X_test, y_test)\n",
    "    pipeline.logger.info(\"✅ Training completed.\")\n",
    "    \n",
    "    # Save the pipeline and model\n",
    "    joblib.dump(pipeline, 'preprocessing_pipeline.joblib')\n",
    "    pipeline.logger.info(\"✅ Preprocessing pipeline and model saved as 'preprocessing_pipeline.joblib'.\")\n",
    "    \n",
    "    # Save the inverse-transformed test set for interpretability\n",
    "    training_output['X_test_inverse'].to_csv('X_test_inverse.csv')\n",
    "    pipeline.logger.info(\"✅ Inverse-transformed test set saved as 'X_test_inverse.csv'.\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Prediction Phase\n",
    "    # ----------------------------\n",
    "    # Load the trained pipeline and model\n",
    "    loaded_pipeline = joblib.load('preprocessing_pipeline.joblib')\n",
    "    loaded_pipeline.logger.info(\"✅ Preprocessing pipeline and model loaded from 'preprocessing_pipeline.joblib'.\")\n",
    "    \n",
    "    # Example Prediction Data\n",
    "    prediction_data = {\n",
    "        'age': [28, 40],\n",
    "        'education_level': ['Masters', 'Bachelors'],\n",
    "        'experience': ['Mid', 'Senior'],\n",
    "        'gender': ['Female', 'Male'],\n",
    "        'city': ['Chicago', 'Houston'],\n",
    "        'department': ['Engineering', 'Sales']\n",
    "    }\n",
    "    \n",
    "    X_new = pd.DataFrame(prediction_data)\n",
    "    \n",
    "    # Categorize features and verify\n",
    "    loaded_pipeline.categorize_features(X_new)\n",
    "    \n",
    "    # Perform prediction preprocessing and inverse transformation\n",
    "    prediction_output = loaded_pipeline.predict(X_new)\n",
    "    loaded_pipeline.logger.info(\"✅ Prediction data preprocessed and predictions made successfully.\")\n",
    "    \n",
    "    # Save the inverse-transformed prediction data for interpretability\n",
    "    prediction_output['X_inverse'].to_csv('X_inverse_prediction.csv')\n",
    "    loaded_pipeline.logger.info(\"✅ Inverse-transformed prediction data saved as 'X_inverse_prediction.csv'.\")\n",
    "    \n",
    "    # Display the inverse-transformed prediction data\n",
    "    print(\"\\nInverse Transformed Prediction DataFrame with Predictions:\")\n",
    "    print(prediction_output['X_inverse'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
