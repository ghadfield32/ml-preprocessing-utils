{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data_preprocessor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "import logging\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import joblib  # For saving/loading transformers\n",
    "from inspect import signature  # For parameter validation in SMOTE\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        column_assets: Dict[str, List[str]],\n",
    "        mode: str,  # 'train', 'predict', 'clustering'\n",
    "        options: Optional[Dict] = None,\n",
    "        debug: bool = False,\n",
    "        normalize_debug: bool = False,\n",
    "        normalize_graphs_output: bool = False,\n",
    "        graphs_output_dir: str = './plots',\n",
    "        transformers_dir: str = './transformers'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DataPreprocessor with model type, column assets, and user-defined options.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the machine learning model (e.g., 'Logistic Regression').\n",
    "            column_assets (Dict[str, List[str]]): Dictionary containing lists of columns for different categories.\n",
    "            mode (str): Operational mode ('train', 'predict', 'clustering').\n",
    "            options (Optional[Dict]): User-defined options for preprocessing steps.\n",
    "            debug (bool): General debug flag to control overall verbosity.\n",
    "            normalize_debug (bool): Flag to display normalization plots.\n",
    "            normalize_graphs_output (bool): Flag to save normalization plots.\n",
    "            graphs_output_dir (str): Directory to save plots.\n",
    "            transformers_dir (str): Directory to save/load transformers.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.column_assets = column_assets\n",
    "        self.mode = mode.lower()\n",
    "        if self.mode not in ['train', 'predict', 'clustering']:\n",
    "            raise ValueError(\"Mode must be one of 'train', 'predict', or 'clustering'.\")\n",
    "        self.options = options or {}\n",
    "        self.debug = debug\n",
    "        self.normalize_debug = normalize_debug\n",
    "        self.normalize_graphs_output = normalize_graphs_output\n",
    "        self.graphs_output_dir = graphs_output_dir\n",
    "        self.transformers_dir = transformers_dir\n",
    "\n",
    "        # Define model categories for accurate processing\n",
    "        self.model_category = self.map_model_type_to_category()\n",
    "\n",
    "        if self.model_category == 'unknown':\n",
    "            self.logger = logging.getLogger(self.__class__.__name__)\n",
    "            self.logger.error(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "            raise ValueError(f\"Model category for '{self.model_type}' is unknown. Check your configuration.\")\n",
    "\n",
    "        # Initialize y_variable based on mode and model category\n",
    "        if self.mode in ['train', 'predict'] and self.model_category in ['classification', 'regression']:\n",
    "            self.y_variable = column_assets.get('y_variable', [])\n",
    "            if not self.y_variable:\n",
    "                if self.mode == 'train':\n",
    "                    raise ValueError(\"Target variable 'y_variable' must be specified for supervised models in train mode.\")\n",
    "                # In predict mode, y_variable might not be present\n",
    "        else:\n",
    "            # For 'clustering' mode or unsupervised prediction\n",
    "            self.y_variable = []\n",
    "\n",
    "        # Fetch feature lists\n",
    "        self.ordinal_categoricals = column_assets.get('ordinal_categoricals', [])\n",
    "        self.nominal_categoricals = column_assets.get('nominal_categoricals', [])\n",
    "        self.numericals = column_assets.get('numericals', [])\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.scaler = None\n",
    "        self.transformer = None\n",
    "        self.ordinal_encoder = None\n",
    "        self.nominal_encoder = None\n",
    "        self.preprocessor = None\n",
    "        self.smote = None\n",
    "        self.feature_reasons = {col: '' for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals}\n",
    "        self.preprocessing_steps = []\n",
    "        self.normality_results = {}\n",
    "        self.features_to_transform = []\n",
    "        self.nominal_encoded_feature_names = []\n",
    "        self.final_feature_order = []\n",
    "\n",
    "        # Initialize placeholders for clustering-specific transformers\n",
    "        self.cluster_transformers = {}\n",
    "        self.cluster_model = None\n",
    "        self.cluster_labels = None\n",
    "        self.silhouette_score = None\n",
    "\n",
    "        # Define default thresholds for SMOTE recommendations\n",
    "        self.imbalance_threshold = self.options.get('smote_recommendation', {}).get('imbalance_threshold', 0.1)\n",
    "        self.noise_threshold = self.options.get('smote_recommendation', {}).get('noise_threshold', 0.1)\n",
    "        self.overlap_threshold = self.options.get('smote_recommendation', {}).get('overlap_threshold', 0.1)\n",
    "        self.boundary_threshold = self.options.get('smote_recommendation', {}).get('boundary_threshold', 0.1)\n",
    "\n",
    "        self.pipeline = None  # Initialize pipeline\n",
    "\n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.setLevel(logging.DEBUG if self.debug else logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "    def get_debug_flag(self, flag_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Retrieve the value of a specific debug flag from the options.\n",
    "        Args:\n",
    "            flag_name (str): The name of the debug flag.\n",
    "        Returns:\n",
    "            bool: The value of the debug flag.\n",
    "        \"\"\"\n",
    "        return self.options.get(flag_name, False)\n",
    "\n",
    "    def _log(self, message: str, step: str, level: str = 'info'):\n",
    "        \"\"\"\n",
    "        Internal method to log messages based on the step-specific debug flags.\n",
    "        \n",
    "        Args:\n",
    "            message (str): The message to log.\n",
    "            step (str): The preprocessing step name.\n",
    "            level (str): The logging level ('info', 'debug', etc.).\n",
    "        \"\"\"\n",
    "        debug_flag = self.get_debug_flag(f'debug_{step}')\n",
    "        if debug_flag:\n",
    "            if level == 'debug':\n",
    "                self.logger.debug(message)\n",
    "            elif level == 'info':\n",
    "                self.logger.info(message)\n",
    "            elif level == 'warning':\n",
    "                self.logger.warning(message)\n",
    "            elif level == 'error':\n",
    "                self.logger.error(message)\n",
    "\n",
    "    def map_model_type_to_category(self) -> str:\n",
    "        \"\"\"\n",
    "        Map the model_type string to a predefined category.\n",
    "\n",
    "        Returns:\n",
    "            str: The model category ('classification', 'regression', 'clustering', etc.).\n",
    "        \"\"\"\n",
    "        classification_models = [\n",
    "            'Logistic Regression',\n",
    "            'Tree Based Classifier',\n",
    "            'k-NN Classifier',\n",
    "            'SVM Classifier',\n",
    "            'Neural Network Classifier'\n",
    "        ]\n",
    "\n",
    "        regression_models = [\n",
    "            'Linear Regression',\n",
    "            'Tree Based Regressor',\n",
    "            'k-NN Regressor',\n",
    "            'SVM Regressor',\n",
    "            'Neural Network Regressor'\n",
    "        ]\n",
    "\n",
    "        clustering_models = [\n",
    "            'K-Means', 'Hierarchical Clustering', 'DBSCAN', 'KModes', 'KPrototypes'\n",
    "        ]\n",
    "\n",
    "\n",
    "        if self.model_type in classification_models:\n",
    "            return 'classification'\n",
    "        elif self.model_type in regression_models:\n",
    "            return 'regression'\n",
    "        elif self.model_type in clustering_models:\n",
    "            return 'clustering'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "\n",
    "    def split_dataset(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: Optional[pd.Series] = None\n",
    "    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Split the dataset into training and testing sets while retaining original indices.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Features.\n",
    "            y (Optional[pd.Series]): Target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.Series], Optional[pd.Series]]: X_train, X_test, y_train, y_test\n",
    "        \"\"\"\n",
    "        step_name = \"split_dataset\"\n",
    "        self.logger.info(\"Step: Split Dataset into Train and Test\")\n",
    "\n",
    "        # Debugging Statements\n",
    "        self._log(f\"Before Split - X shape: {X.shape}\", step_name, 'debug')\n",
    "        if y is not None:\n",
    "            self._log(f\"Before Split - y shape: {y.shape}\", step_name, 'debug')\n",
    "        else:\n",
    "            self._log(\"Before Split - y is None\", step_name, 'debug')\n",
    "\n",
    "        # Determine splitting based on mode\n",
    "        if self.mode == 'train' and self.model_category in ['classification', 'regression']:\n",
    "            if self.model_category == 'classification':\n",
    "                stratify = y if self.options.get('split_dataset', {}).get('stratify_for_classification', False) else None\n",
    "                test_size = self.options.get('split_dataset', {}).get('test_size', 0.2)\n",
    "                random_state = self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=test_size,\n",
    "                    stratify=stratify, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "                self._log(\"Performed stratified split for classification.\", step_name, 'debug')\n",
    "            elif self.model_category == 'regression':\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, \n",
    "                    test_size=self.options.get('split_dataset', {}).get('test_size', 0.2),\n",
    "                    random_state=self.options.get('split_dataset', {}).get('random_state', 42)\n",
    "                )\n",
    "                self._log(\"Performed random split for regression.\", step_name, 'debug')\n",
    "        else:\n",
    "            # For 'predict' and 'clustering' modes or other categories\n",
    "            X_train = X.copy()\n",
    "            X_test = None\n",
    "            y_train = y.copy() if y is not None else None\n",
    "            y_test = None\n",
    "            self.logger.info(f\"No splitting performed for mode '{self.mode}' or model category '{self.model_category}'.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Split Dataset into Train and Test\")\n",
    "\n",
    "        # Keep Indices Aligned Through Each Step\n",
    "        if X_test is not None and y_test is not None:\n",
    "            # Sort both X_test and y_test by index\n",
    "            X_test = X_test.sort_index()\n",
    "            y_test = y_test.sort_index()\n",
    "            self.logger.debug(\"Sorted X_test and y_test by index for alignment.\")\n",
    "\n",
    "        # Debugging: Log post-split shapes and index alignment\n",
    "        self._log(f\"After Split - X_train shape: {X_train.shape}, X_test shape: {X_test.shape if X_test is not None else 'N/A'}\", step_name, 'debug')\n",
    "        if self.model_category == 'classification' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"Class distribution in y_train:\\n{y_train.value_counts(normalize=True)}\")\n",
    "            self.logger.debug(f\"Class distribution in y_test:\\n{y_test.value_counts(normalize=True)}\")\n",
    "        elif self.model_category == 'regression' and y_train is not None and y_test is not None:\n",
    "            self.logger.debug(f\"y_train statistics:\\n{y_train.describe()}\")\n",
    "            self.logger.debug(f\"y_test statistics:\\n{y_test.describe()}\")\n",
    "\n",
    "        # Check index alignment\n",
    "        if y_train is not None and X_train.index.equals(y_train.index):\n",
    "            self.logger.debug(\"X_train and y_train indices are aligned.\")\n",
    "        else:\n",
    "            self.logger.warning(\"X_train and y_train indices are misaligned.\")\n",
    "\n",
    "        if X_test is not None and y_test is not None and X_test.index.equals(y_test.index):\n",
    "            self.logger.debug(\"X_test and y_test indices are aligned.\")\n",
    "        elif X_test is not None and y_test is not None:\n",
    "            self.logger.warning(\"X_test and y_test indices are misaligned.\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "    def categorize_features(self, df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Categorize features into numerical, ordinal, and nominal based on provided lists or data types.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, List[str]]: Dictionary with keys 'numerical', 'ordinal', 'nominal' and lists of feature names.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting feature categorization.\")\n",
    "        feature_types = {'numerical': [], 'ordinal': [], 'nominal': []}\n",
    "\n",
    "        # Use provided numerical_features, or categorize automatically\n",
    "        if self.numericals:\n",
    "            feature_types['numerical'] = self.numericals\n",
    "            self.logger.debug(f\"Using provided numerical features: {self.numericals}\")\n",
    "        else:\n",
    "            numerical = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col != self.target_variable]\n",
    "            feature_types['numerical'] = numerical\n",
    "            self.logger.debug(f\"Automatically categorized numerical features: {numerical}\")\n",
    "\n",
    "        # Use provided ordinal_features\n",
    "        if self.ordinal_categoricals:\n",
    "            feature_types['ordinal'] = self.ordinal_categoricals\n",
    "            self.logger.debug(f\"Using provided ordinal features: {self.ordinal_categoricals}\")\n",
    "        else:\n",
    "            self.logger.warning(\"No ordinal categoricals provided. Skipping ordinal feature encoding.\")\n",
    "\n",
    "        # Use provided nominal_features, or categorize automatically\n",
    "        if self.nominal_categoricals:\n",
    "            feature_types['nominal'] = self.nominal_categoricals\n",
    "            self.logger.debug(f\"Using provided nominal features: {self.nominal_categoricals}\")\n",
    "        else:\n",
    "            # Nominal features are those not in numerical or ordinal\n",
    "            nominal = [col for col in df.columns if col not in self.numericals + self.ordinal_categoricals + [self.target_variable]]\n",
    "            feature_types['nominal'] = nominal\n",
    "            self.logger.debug(f\"Automatically categorized nominal features: {nominal}\")\n",
    "\n",
    "        self.numericals = feature_types['numerical']\n",
    "        self.ordinal_features = feature_types['ordinal']\n",
    "        self.nominal_features = feature_types['nominal']\n",
    "\n",
    "        # Log the categorized features\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Categorized Numerical Features: {self.numericals}\")\n",
    "            self.logger.debug(f\"Categorized Ordinal Features: {self.ordinal_features}\")\n",
    "            self.logger.debug(f\"Categorized Nominal Features: {self.nominal_features}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Features categorized: Numerical={len(self.numericals)}, \"\n",
    "                            f\"Ordinal={len(self.ordinal_features)}, Nominal={len(self.nominal_features)}.\")\n",
    "\n",
    "        self.logger.debug(f\"Categorized Numerical Features: {self.numericals}\")\n",
    "        self.logger.debug(f\"Categorized Ordinal Features: {self.ordinal_categoricals}\")\n",
    "        self.logger.debug(f\"Categorized Nominal Features: {self.nominal_categoricals}\")\n",
    "\n",
    "        # Warn if any category is empty\n",
    "        if not self.numericals:\n",
    "            self.logger.warning(\"No numerical features detected.\")\n",
    "        if not self.ordinal_categoricals:\n",
    "            self.logger.warning(\"No ordinal categorical features detected.\")\n",
    "        if not self.nominal_categoricals:\n",
    "            self.logger.warning(\"No nominal categorical features detected.\")\n",
    "\n",
    "        return feature_types\n",
    "\n",
    "\n",
    "\n",
    "    def handle_missing_values(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Handle missing values for numerical and categorical features based on user options.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_missing_values\"\n",
    "        self.logger.info(\"Step: Handle Missing Values\")\n",
    "\n",
    "        # Fetch user-defined imputation options or set defaults\n",
    "        impute_options = self.options.get('handle_missing_values', {})\n",
    "        numerical_strategy = impute_options.get('numerical_strategy', {})\n",
    "        categorical_strategy = impute_options.get('categorical_strategy', {})\n",
    "\n",
    "        # Numerical Imputation\n",
    "        numerical_imputer = None\n",
    "        new_columns = []\n",
    "        if self.numericals:\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                default_num_strategy = 'mean'  # For clustering, mean imputation is acceptable\n",
    "            else:\n",
    "                default_num_strategy = 'median'\n",
    "            num_strategy = numerical_strategy.get('strategy', default_num_strategy)\n",
    "            num_imputer_type = numerical_strategy.get('imputer', 'SimpleImputer')  # Can be 'SimpleImputer', 'KNNImputer', etc.\n",
    "\n",
    "            self._log(f\"Numerical Imputation Strategy: {num_strategy.capitalize()}, Imputer Type: {num_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize numerical imputer based on user option\n",
    "            if num_imputer_type == 'SimpleImputer':\n",
    "                numerical_imputer = SimpleImputer(strategy=num_strategy)\n",
    "            elif num_imputer_type == 'KNNImputer':\n",
    "                knn_neighbors = numerical_strategy.get('knn_neighbors', 5)\n",
    "                numerical_imputer = KNNImputer(n_neighbors=knn_neighbors)\n",
    "            else:\n",
    "                self.logger.error(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Numerical imputer type '{num_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[self.numericals] = numerical_imputer.fit_transform(X_train[self.numericals])\n",
    "            self.numerical_imputer = numerical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({col: self.feature_reasons.get(col, '') + f'Numerical: {num_strategy.capitalize()} Imputation | ' for col in self.numericals})\n",
    "            new_columns.extend(self.numericals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[self.numericals] = numerical_imputer.transform(X_test[self.numericals])\n",
    "\n",
    "        # Categorical Imputation\n",
    "        categorical_imputer = None\n",
    "        all_categoricals = self.ordinal_categoricals + self.nominal_categoricals\n",
    "        if all_categoricals:\n",
    "            default_cat_strategy = 'most_frequent'\n",
    "            cat_strategy = categorical_strategy.get('strategy', default_cat_strategy)\n",
    "            cat_imputer_type = categorical_strategy.get('imputer', 'SimpleImputer')\n",
    "\n",
    "            self._log(f\"Categorical Imputation Strategy: {cat_strategy.capitalize()}, Imputer Type: {cat_imputer_type}\", step_name, 'debug')\n",
    "\n",
    "            # Initialize categorical imputer based on user option\n",
    "            if cat_imputer_type == 'SimpleImputer':\n",
    "                categorical_imputer = SimpleImputer(strategy=cat_strategy)\n",
    "            elif cat_imputer_type == 'ConstantImputer':\n",
    "                fill_value = categorical_strategy.get('fill_value', 'Missing')\n",
    "                categorical_imputer = SimpleImputer(strategy='constant', fill_value=fill_value)\n",
    "            else:\n",
    "                self.logger.error(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "                raise ValueError(f\"Categorical imputer type '{cat_imputer_type}' is not supported.\")\n",
    "\n",
    "            # Fit and transform ONLY on X_train\n",
    "            X_train[all_categoricals] = categorical_imputer.fit_transform(X_train[all_categoricals])\n",
    "            self.categorical_imputer = categorical_imputer  # Assign to self for saving\n",
    "            self.feature_reasons.update({\n",
    "                col: self.feature_reasons.get(col, '') + (f'Categorical: Constant Imputation (Value={categorical_strategy.get(\"fill_value\", \"Missing\")}) | ' if cat_imputer_type == 'ConstantImputer' else f'Categorical: {cat_strategy.capitalize()} Imputation | ')\n",
    "                for col in all_categoricals\n",
    "            })\n",
    "            new_columns.extend(all_categoricals)\n",
    "\n",
    "            if X_test is not None:\n",
    "                # Transform ONLY on X_test without fitting\n",
    "                X_test[all_categoricals] = categorical_imputer.transform(X_test[all_categoricals])\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Missing Values\")\n",
    "\n",
    "        # Debugging: Log post-imputation shapes and missing values\n",
    "        self._log(f\"Completed: Handle Missing Values. Dataset shape after imputation: {X_train.shape}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after imputation in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"New columns handled: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "\n",
    "    def handle_outliers(self, X_train: pd.DataFrame, y_train: Optional[pd.Series] = None) -> Tuple[pd.DataFrame, Optional[pd.Series]]:\n",
    "        \"\"\"\n",
    "        Handle outliers based on the model's sensitivity and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series, optional): Training target.\n",
    "\n",
    "        Returns:\n",
    "            tuple: X_train without outliers and corresponding y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"handle_outliers\"\n",
    "        self.logger.info(\"Step: Handle Outliers\")\n",
    "        self._log(\"Starting outlier handling.\", step_name, 'debug')\n",
    "\n",
    "        debug_flag = self.get_debug_flag('debug_handle_outliers')\n",
    "        initial_shape = X_train.shape[0]\n",
    "        new_columns = []\n",
    "\n",
    "        # Fetch user-defined outlier handling options or set defaults\n",
    "        outlier_options = self.options.get('handle_outliers', {})\n",
    "        zscore_threshold = outlier_options.get('zscore_threshold', 3)\n",
    "        iqr_multiplier = outlier_options.get('iqr_multiplier', 1.5)\n",
    "        winsor_limits = outlier_options.get('winsor_limits', [0.05, 0.05])\n",
    "        isolation_contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "\n",
    "        # Check for target leakage: Ensure y_train is not used in transformations\n",
    "        if self.mode == 'train' and y_train is not None:\n",
    "            self._log(\"y_train is present. Confirming it's not used in outlier handling.\", step_name, 'debug')\n",
    "            # Add any specific checks if transformations accidentally use y_train\n",
    "            # For example, ensure that no columns derived from y_train are being modified\n",
    "\n",
    "        for col in self.numericals:\n",
    "            if self.model_category in ['regression', 'classification']:\n",
    "                # Z-Score Filtering\n",
    "                apply_zscore = outlier_options.get('apply_zscore', True)\n",
    "                if apply_zscore:\n",
    "                    z_scores = np.abs((X_train[col] - X_train[col].mean()) / X_train[col].std())\n",
    "                    mask_z = z_scores < zscore_threshold\n",
    "                    removed_z = (~mask_z).sum()\n",
    "                    X_train = X_train[mask_z]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with Z-Score Filtering (threshold={zscore_threshold}) | '\n",
    "                    self._log(f\"Removed {removed_z} outliers from '{col}' using Z-Score Filtering\", step_name, 'debug')\n",
    "\n",
    "                # IQR Filtering\n",
    "                apply_iqr = outlier_options.get('apply_iqr', True)\n",
    "                if apply_iqr:\n",
    "                    Q1 = X_train[col].quantile(0.25)\n",
    "                    Q3 = X_train[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - iqr_multiplier * IQR\n",
    "                    upper_bound = Q3 + iqr_multiplier * IQR\n",
    "                    mask_iqr = (X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)\n",
    "                    removed_iqr = (~mask_iqr).sum()\n",
    "                    X_train = X_train[mask_iqr]\n",
    "                    if y_train is not None:\n",
    "                        y_train = y_train.loc[X_train.index]\n",
    "                    self.feature_reasons[col] += f'Outliers handled with IQR Filtering (multiplier={iqr_multiplier}) | '\n",
    "                    self._log(f\"Removed {removed_iqr} outliers from '{col}' using IQR Filtering\", step_name, 'debug')\n",
    "\n",
    "            elif self.model_category == 'clustering':\n",
    "                # For clustering, apply IsolationForest for outlier detection\n",
    "                contamination = outlier_options.get('isolation_contamination', 0.05)\n",
    "                iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "                preds = iso_forest.fit_predict(X_train[[col]])\n",
    "                mask_iso = preds != -1\n",
    "                removed_iso = (preds == -1).sum()\n",
    "                X_train = X_train[mask_iso]\n",
    "                self.feature_reasons[col] += f'Outliers handled with IsolationForest (contamination={contamination}) | '\n",
    "                self._log(f\"Removed {removed_iso} outliers from '{col}' using IsolationForest\", step_name, 'debug')\n",
    "\n",
    "            else:\n",
    "                self.logger.warning(f\"Model category '{self.model_category}' not recognized for outlier handling.\")\n",
    "\n",
    "        self.preprocessing_steps.append(\"Handle Outliers\")\n",
    "\n",
    "        # Completion Logging\n",
    "        self._log(f\"Completed: Handle Outliers. Initial samples: {initial_shape}, Final samples: {X_train.shape[0]}\", step_name, 'debug')\n",
    "        self._log(f\"Missing values after outlier handling in X_train:\\n{X_train.isnull().sum()}\", step_name, 'debug')\n",
    "        self._log(f\"Outlier handling applied on columns: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "        return X_train, y_train\n",
    "    \n",
    "    def test_normality(self, X_train: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Test normality for numerical features based on normality tests and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict]: Dictionary with normality test results for each numerical feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Test for Normality\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_test_normality')\n",
    "        normality_results = {}\n",
    "\n",
    "        # Fetch user-defined normality test options or set defaults\n",
    "        normality_options = self.options.get('test_normality', {})\n",
    "        p_value_threshold = normality_options.get('p_value_threshold', 0.05)\n",
    "        skewness_threshold = normality_options.get('skewness_threshold', 1.0)\n",
    "        additional_tests = normality_options.get('additional_tests', [])  # e.g., ['anderson-darling']\n",
    "\n",
    "        for col in self.numericals:\n",
    "            data = X_train[col].dropna()\n",
    "            skewness = data.skew()\n",
    "            kurtosis = data.kurtosis()\n",
    "\n",
    "            # Determine which normality test to use based on sample size and user options\n",
    "            test_used = 'Shapiro-Wilk'\n",
    "            p_value = 0.0\n",
    "\n",
    "            if len(data) <= 5000:\n",
    "                from scipy.stats import shapiro\n",
    "                stat, p_val = shapiro(data)\n",
    "                test_used = 'Shapiro-Wilk'\n",
    "                p_value = p_val\n",
    "            else:\n",
    "                from scipy.stats import anderson\n",
    "                result = anderson(data)\n",
    "                test_used = 'Anderson-Darling'\n",
    "                # Determine p-value based on critical values\n",
    "                p_value = 0.0  # Default to 0\n",
    "                for cv, sig in zip(result.critical_values, result.significance_level):\n",
    "                    if result.statistic < cv:\n",
    "                        p_value = sig / 100\n",
    "                        break\n",
    "\n",
    "            # Apply user-defined or default criteria\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # Linear, Logistic Regression, and Clustering: Use p-value and skewness\n",
    "                needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "            else:\n",
    "                # Other models: Use skewness, and optionally p-values based on options\n",
    "                use_p_value = normality_options.get('use_p_value_other_models', False)\n",
    "                if use_p_value:\n",
    "                    needs_transform = (p_value < p_value_threshold) or (abs(skewness) > skewness_threshold)\n",
    "                else:\n",
    "                    needs_transform = abs(skewness) > skewness_threshold\n",
    "\n",
    "            normality_results[col] = {\n",
    "                'skewness': skewness,\n",
    "                'kurtosis': kurtosis,\n",
    "                'p_value': p_value,\n",
    "                'test_used': test_used,\n",
    "                'needs_transform': needs_transform\n",
    "            }\n",
    "\n",
    "            # Conditional Detailed Logging\n",
    "            if debug_flag:\n",
    "                self._log(f\"Feature '{col}': p-value={p_value:.4f}, skewness={skewness:.4f}, needs_transform={needs_transform}\", step_name, 'debug')\n",
    "\n",
    "        self.normality_results = normality_results\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Normality results computed.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Normality results computed.\")\n",
    "\n",
    "        return normality_results\n",
    "\n",
    "\n",
    "    def encode_categorical_variables(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Encode categorical variables using user-specified encoding strategies.\n",
    "        \"\"\"\n",
    "        step_name = \"encode_categorical_variables\"\n",
    "        self.logger.info(\"Step: Encode Categorical Variables\")\n",
    "        self._log(\"Starting categorical variable encoding.\", step_name, 'debug')\n",
    "\n",
    "        # Fetch user-defined encoding options or set defaults\n",
    "        encoding_options = self.options.get('encode_categoricals', {})\n",
    "        ordinal_encoding = encoding_options.get('ordinal_encoding', 'OrdinalEncoder')  # Options: 'OrdinalEncoder', 'None'\n",
    "        nominal_encoding = encoding_options.get('nominal_encoding', 'OrdinalEncoder')  # Changed from 'OneHotEncoder' to 'OrdinalEncoder'\n",
    "        handle_unknown = encoding_options.get('handle_unknown', 'use_encoded_value')  # Adjusted for OrdinalEncoder\n",
    "\n",
    "        # Determine if SMOTENC is being used\n",
    "        smote_variant = self.options.get('implement_smote', {}).get('variant', None)\n",
    "        if smote_variant == 'SMOTENC':\n",
    "            nominal_encoding = 'OrdinalEncoder'  # Ensure compatibility\n",
    "\n",
    "        transformers = []\n",
    "        new_columns = []\n",
    "        if self.ordinal_categoricals and ordinal_encoding != 'None':\n",
    "            if ordinal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('ordinal', OrdinalEncoder(), self.ordinal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.ordinal_categoricals}\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Ordinal encoding method '{ordinal_encoding}' is not supported.\")\n",
    "        if self.nominal_categoricals and nominal_encoding != 'None':\n",
    "            if nominal_encoding == 'OrdinalEncoder':\n",
    "                transformers.append(\n",
    "                    ('nominal', OrdinalEncoder(handle_unknown=handle_unknown), self.nominal_categoricals)\n",
    "                )\n",
    "                self._log(f\"Added OrdinalEncoder for features: {self.nominal_categoricals}\", step_name, 'debug')\n",
    "            elif nominal_encoding == 'FrequencyEncoder':\n",
    "                # Custom Frequency Encoding\n",
    "                for col in self.nominal_categoricals:\n",
    "                    freq = X_train[col].value_counts(normalize=True)\n",
    "                    X_train[col] = X_train[col].map(freq)\n",
    "                    if X_test is not None:\n",
    "                        X_test[col] = X_test[col].map(freq).fillna(0)\n",
    "                    self.feature_reasons[col] += 'Encoded with Frequency Encoding | '\n",
    "                    self._log(f\"Applied Frequency Encoding to '{col}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.error(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "                raise ValueError(f\"Nominal encoding method '{nominal_encoding}' is not supported.\")\n",
    "\n",
    "        if not transformers and 'FrequencyEncoder' not in nominal_encoding:\n",
    "            self.logger.info(\"No categorical variables to encode.\")\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. No encoding was applied.\", step_name, 'debug')\n",
    "            return X_train, X_test\n",
    "\n",
    "        if transformers:\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=transformers,\n",
    "                remainder='passthrough'  # Keep other columns unchanged\n",
    "            )\n",
    "\n",
    "            # Fit and transform training data\n",
    "            X_train_encoded = self.preprocessor.fit_transform(X_train)\n",
    "            self._log(\"Fitted and transformed X_train with ColumnTransformer.\", step_name, 'debug')\n",
    "\n",
    "            # Transform testing data\n",
    "            if X_test is not None:\n",
    "                X_test_encoded = self.preprocessor.transform(X_test)\n",
    "                self._log(\"Transformed X_test with fitted ColumnTransformer.\", step_name, 'debug')\n",
    "            else:\n",
    "                X_test_encoded = None\n",
    "\n",
    "            # Retrieve feature names after encoding\n",
    "            encoded_feature_names = []\n",
    "            if self.ordinal_categoricals and ordinal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.ordinal_categoricals\n",
    "            if self.nominal_categoricals and nominal_encoding == 'OrdinalEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            elif self.nominal_categoricals and nominal_encoding == 'FrequencyEncoder':\n",
    "                encoded_feature_names += self.nominal_categoricals\n",
    "            passthrough_features = [col for col in X_train.columns if col not in self.ordinal_categoricals + self.nominal_categoricals]\n",
    "            encoded_feature_names += passthrough_features\n",
    "            new_columns.extend(encoded_feature_names)\n",
    "\n",
    "            # Convert numpy arrays back to DataFrames\n",
    "            X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)\n",
    "            if X_test_encoded is not None:\n",
    "                X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_feature_names, index=X_test.index)\n",
    "            else:\n",
    "                X_test_encoded_df = None\n",
    "\n",
    "            # Store encoders for inverse transformation\n",
    "            self.ordinal_encoder = self.preprocessor.named_transformers_['ordinal'] if 'ordinal' in self.preprocessor.named_transformers_ else None\n",
    "            self.nominal_encoder = self.preprocessor.named_transformers_['nominal'] if 'nominal' in self.preprocessor.named_transformers_ else None\n",
    "\n",
    "            self.preprocessing_steps.append(\"Encode Categorical Variables\")\n",
    "            self._log(f\"Completed: Encode Categorical Variables. X_train_encoded shape: {X_train_encoded_df.shape}\", step_name, 'debug')\n",
    "            self._log(f\"Columns after encoding: {encoded_feature_names}\", step_name, 'debug')\n",
    "            self._log(f\"Sample of encoded X_train:\\n{X_train_encoded_df.head()}\", step_name, 'debug')\n",
    "            self._log(f\"New columns added: {new_columns}\", step_name, 'debug')\n",
    "\n",
    "            return X_train_encoded_df, X_test_encoded_df\n",
    "\n",
    "    def generate_recommendations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a table of preprocessing recommendations based on the model type, data, and user options.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing recommendations for each feature.\n",
    "        \"\"\"\n",
    "        step_name = \"Generate Preprocessor Recommendations\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_generate_recommendations')\n",
    "\n",
    "        # Generate recommendations based on feature reasons\n",
    "        recommendations = {}\n",
    "        for col in self.ordinal_categoricals + self.nominal_categoricals + self.numericals:\n",
    "            reasons = self.feature_reasons.get(col, '').strip(' | ')\n",
    "            recommendations[col] = reasons\n",
    "\n",
    "        recommendations_table = pd.DataFrame.from_dict(\n",
    "            recommendations, \n",
    "            orient='index', \n",
    "            columns=['Preprocessing Reason']\n",
    "        )\n",
    "        if debug_flag:\n",
    "            self.logger.debug(f\"Preprocessing Recommendations:\\n{recommendations_table}\")\n",
    "        else:\n",
    "            self.logger.info(\"Preprocessing Recommendations generated.\")\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "        # Completion Logging\n",
    "        if debug_flag:\n",
    "            self._log(f\"Completed: {step_name}. Recommendations generated.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Step '{step_name}' completed: Recommendations generated.\")\n",
    "\n",
    "        return recommendations_table\n",
    "\n",
    "\n",
    "    def save_transformers(self):\n",
    "        \"\"\"\n",
    "        Save fitted transformers to disk for future use during prediction.\n",
    "        \"\"\"\n",
    "        step_name = \"Save Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_save_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "        \n",
    "        transformers = {\n",
    "            'numerical_imputer': getattr(self, 'numerical_imputer', None),\n",
    "            'categorical_imputer': getattr(self, 'categorical_imputer', None),\n",
    "            'preprocessor': self.pipeline,  # **Include the pipeline**\n",
    "            # 'scaler': self.scaler,  # **Removed**\n",
    "            'smote': self.smote,\n",
    "            'final_feature_order': self.final_feature_order,\n",
    "            'categorical_indices': self.categorical_indices  # **Added**\n",
    "        }\n",
    "        try:\n",
    "            joblib.dump(transformers, transformers_path)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Transformers saved at '{transformers_path}'.\", step_name, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Transformers saved at '{transformers_path}'.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to save transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "\n",
    "    def load_transformers(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load fitted transformers from disk for use during prediction.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing all necessary transformers.\n",
    "        \"\"\"\n",
    "        step_name = \"Load Transformers\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_load_transformers')  # Assuming a step-specific debug flag\n",
    "        transformers_path = os.path.join(self.transformers_dir, 'transformers.pkl')\n",
    "        \n",
    "        if not os.path.exists(transformers_path):\n",
    "            self.logger.error(f\"❌ Transformers file not found at '{transformers_path}'. Cannot proceed with prediction.\")\n",
    "            raise FileNotFoundError(f\"Transformers file not found at '{transformers_path}'.\")\n",
    "        \n",
    "        try:\n",
    "            transformers = joblib.load(transformers_path)\n",
    "            \n",
    "            # Extract transformers\n",
    "            numerical_imputer = transformers.get('numerical_imputer')\n",
    "            categorical_imputer = transformers.get('categorical_imputer')\n",
    "            preprocessor = transformers.get('preprocessor')\n",
    "            # scaler = transformers.get('scaler')  # **Removed**\n",
    "            self.pipeline = transformers.get('preprocessor')  # **Load the pipeline**\n",
    "            smote = transformers.get('smote', None)\n",
    "            final_feature_order = transformers.get('final_feature_order', [])\n",
    "            categorical_indices = transformers.get('categorical_indices', [])\n",
    "            self.categorical_indices = categorical_indices  # **Set the attribute**\n",
    "\n",
    "            # **Post-Loading Debugging:**\n",
    "            for name, transformer, features in preprocessor.transformers_:\n",
    "                if name == 'ord':\n",
    "                    ordinal_encoder = transformer.named_steps.get('ordinal_encoder', None)\n",
    "                    if ordinal_encoder:\n",
    "                        if hasattr(ordinal_encoder, 'categories_'):\n",
    "                            self.logger.debug(f\"✅ OrdinalEncoder for features {features} is fitted.\")\n",
    "                        else:\n",
    "                            self.logger.error(f\"❌ OrdinalEncoder for features {features} is NOT fitted.\")\n",
    "                    else:\n",
    "                        self.logger.error(f\"❌ 'ordinal_encoder' not found in transformer '{name}'.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "            raise\n",
    "\n",
    "        self.preprocessing_steps.append(step_name)\n",
    "        \n",
    "        # Additional checks\n",
    "        if preprocessor is None:\n",
    "            self.logger.error(\"❌ Preprocessor is not loaded.\")\n",
    "\n",
    "        if debug_flag:\n",
    "            self._log(f\"Transformers loaded successfully from '{transformers_path}'.\", step_name, 'debug')\n",
    "        else:\n",
    "            self.logger.info(f\"Transformers loaded successfully from '{transformers_path}'.\")\n",
    "\n",
    "        # Return the transformers as a dictionary\n",
    "        return {\n",
    "            'numerical_imputer': numerical_imputer,\n",
    "            'categorical_imputer': categorical_imputer,\n",
    "            'preprocessor': preprocessor,\n",
    "            # 'scaler': scaler,  # **Removed**\n",
    "            'smote': smote,\n",
    "            'final_feature_order': final_feature_order,\n",
    "            'categorical_indices': categorical_indices\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def apply_scaling(self, X_train: pd.DataFrame, X_test: Optional[pd.DataFrame] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Apply scaling based on the model type and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            X_test (Optional[pd.DataFrame]): Testing features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, Optional[pd.DataFrame]]: Scaled X_train and X_test.\n",
    "        \"\"\"\n",
    "        step_name = \"Apply Scaling (If Needed by Model)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "        debug_flag = self.get_debug_flag('debug_apply_scaling')\n",
    "\n",
    "        # Fetch user-defined scaling options or set defaults\n",
    "        scaling_options = self.options.get('apply_scaling', {})\n",
    "        scaling_method = scaling_options.get('method', None)  # 'StandardScaler', 'MinMaxScaler', 'RobustScaler', 'None'\n",
    "        features_to_scale = scaling_options.get('features', self.numericals)\n",
    "\n",
    "        scaler = None\n",
    "        scaling_type = 'None'\n",
    "\n",
    "        if scaling_method is None:\n",
    "            # Default scaling based on model category\n",
    "            if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "                # For clustering, MinMaxScaler is generally preferred\n",
    "                if self.model_category == 'clustering':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaling_type = 'MinMaxScaler'\n",
    "                else:\n",
    "                    scaler = StandardScaler()\n",
    "                    scaling_type = 'StandardScaler'\n",
    "            else:\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "        else:\n",
    "            # User-specified scaling method\n",
    "            if scaling_method == 'StandardScaler':\n",
    "                scaler = StandardScaler()\n",
    "                scaling_type = 'StandardScaler'\n",
    "            elif scaling_method == 'MinMaxScaler':\n",
    "                scaler = MinMaxScaler()\n",
    "                scaling_type = 'MinMaxScaler'\n",
    "            elif scaling_method == 'RobustScaler':\n",
    "                scaler = RobustScaler()\n",
    "                scaling_type = 'RobustScaler'\n",
    "            elif scaling_method == 'None':\n",
    "                scaler = None\n",
    "                scaling_type = 'None'\n",
    "            else:\n",
    "                self.logger.error(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "                raise ValueError(f\"Scaling method '{scaling_method}' is not supported.\")\n",
    "\n",
    "        # Apply scaling if scaler is defined\n",
    "        if scaler is not None and features_to_scale:\n",
    "            self.scaler = scaler\n",
    "            if debug_flag:\n",
    "                self._log(f\"Features to scale: {features_to_scale}\", debug_flag, 'debug')\n",
    "\n",
    "            # Check if features exist in the dataset\n",
    "            missing_features = [feat for feat in features_to_scale if feat not in X_train.columns]\n",
    "            if missing_features:\n",
    "                self.logger.error(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "                raise KeyError(f\"The following features specified for scaling are missing in the dataset: {missing_features}\")\n",
    "\n",
    "            X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "            if X_test is not None:\n",
    "                X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "            for col in features_to_scale:\n",
    "                self.feature_reasons[col] += f'Scaling Applied: {scaling_type} | '\n",
    "\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Applied {scaling_type} to features: {features_to_scale}\", debug_flag, 'debug')\n",
    "                if hasattr(scaler, 'mean_'):\n",
    "                    self._log(f\"Scaler Parameters: mean={scaler.mean_}\", debug_flag, 'debug')\n",
    "                if hasattr(scaler, 'scale_'):\n",
    "                    self._log(f\"Scaler Parameters: scale={scaler.scale_}\", debug_flag, 'debug')\n",
    "                self._log(f\"Sample of scaled X_train:\\n{X_train[features_to_scale].head()}\", debug_flag, 'debug')\n",
    "                if X_test is not None:\n",
    "                    self._log(f\"Sample of scaled X_test:\\n{X_test[features_to_scale].head()}\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: Applied {scaling_type} to features: {features_to_scale}\")\n",
    "        else:\n",
    "            self.logger.info(\"No scaling applied based on user options or no features specified.\")\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            if debug_flag:\n",
    "                self._log(f\"Completed: {step_name}. No scaling was applied.\", debug_flag, 'debug')\n",
    "            else:\n",
    "                self.logger.info(f\"Step '{step_name}' completed: No scaling was applied.\")\n",
    "\n",
    "        return X_train, X_test\n",
    "\n",
    "    def implement_smote(self, X_train: pd.DataFrame, y_train: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Implement SMOTE or its variants based on class imbalance, dataset characteristics, and user options.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "            y_train (pd.Series): Training target.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Resampled X_train and y_train.\n",
    "        \"\"\"\n",
    "        step_name = \"Implement SMOTE (Train Only)\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        # Check if classification\n",
    "        if self.model_category != 'classification':\n",
    "            self.logger.info(\"SMOTE not applicable: Not a classification model.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "\n",
    "        # Fetch user-defined SMOTE options or set defaults\n",
    "        smote_options = self.options.get('implement_smote', {})\n",
    "        user_smote_variant = smote_options.get('variant', None)\n",
    "        smote_params = smote_options.get('params', {})\n",
    "\n",
    "        # Calculate class distribution\n",
    "        class_counts = y_train.value_counts()\n",
    "        if len(class_counts) < 2:\n",
    "            self.logger.warning(\"SMOTE not applicable: Only one class present.\")\n",
    "            self.preprocessing_steps.append(\"SMOTE Skipped\")\n",
    "            return X_train, y_train\n",
    "        majority_class = class_counts.idxmax()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        majority_count = class_counts.max()\n",
    "        minority_count = class_counts.min()\n",
    "        imbalance_ratio = minority_count / majority_count\n",
    "        self.logger.info(f\"Class Distribution before SMOTE: {class_counts.to_dict()}\")\n",
    "        self.logger.info(f\"Imbalance Ratio (Minority/Majority): {imbalance_ratio:.4f}\")\n",
    "\n",
    "        # Determine dataset composition\n",
    "        has_numericals = len(self.numericals) > 0\n",
    "        has_categoricals = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        # Initialize variant_name as None\n",
    "        variant_name = None\n",
    "\n",
    "        # Determine SMOTE variant based on dataset composition and user preference\n",
    "        if has_numericals and not has_categoricals:\n",
    "            # Numerical-only dataset\n",
    "            if user_smote_variant:\n",
    "                variant_name = user_smote_variant\n",
    "                self.logger.info(f\"User-specified SMOTE variant: {variant_name}\")\n",
    "            else:\n",
    "                self.logger.info(\"Dataset contains only numerical features. Analyzing to recommend SMOTE variants...\")\n",
    "                smote_recommendation = self.smote_numerics_criteria(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    imbalance_threshold=self.imbalance_threshold,\n",
    "                    noise_threshold=self.noise_threshold,\n",
    "                    overlap_threshold=self.overlap_threshold,\n",
    "                    boundary_threshold=self.boundary_threshold,\n",
    "                    debug=self.get_debug_flag('debug_implement_smote')\n",
    "                )\n",
    "                if smote_recommendation:\n",
    "                    variant_name = smote_recommendation[0]\n",
    "                    self.logger.info(f\"Recommended SMOTE variant: {variant_name}\")\n",
    "                else:\n",
    "                    variant_name = 'SMOTE'\n",
    "                    self.logger.info(\"No specific recommendation from criteria. Using default SMOTE.\")\n",
    "        elif has_numericals and has_categoricals:\n",
    "            # Mixed dataset\n",
    "            variant_name = 'SMOTENC'\n",
    "            self.logger.info(\"Dataset contains numerical and categorical features. Using SMOTENC.\")\n",
    "        elif not has_numericals and has_categoricals:\n",
    "            # Categorical-only dataset\n",
    "            variant_name = 'SMOTEN'\n",
    "            self.logger.info(\"Dataset contains only categorical features. Using SMOTEN.\")\n",
    "        else:\n",
    "            # Fallback\n",
    "            variant_name = 'SMOTE'\n",
    "            self.logger.info(\"Dataset composition not recognized. Using SMOTE as default.\")\n",
    "\n",
    "        # Initialize SMOTE variant\n",
    "        try:\n",
    "            self.logger.debug(f\"Initializing SMOTE Variant '{variant_name}' with parameters: {smote_params}\")\n",
    "\n",
    "            if variant_name == 'SMOTENC':\n",
    "                # Combine ordinal and nominal categorical features\n",
    "                categorical_cols = self.ordinal_categoricals + self.nominal_categoricals\n",
    "                categorical_features = [X_train.columns.get_loc(col) for col in categorical_cols]\n",
    "                smote = SMOTENC(\n",
    "                    categorical_features=categorical_features,\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            elif variant_name == 'SMOTEN':\n",
    "                smote = SMOTEN(\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            elif variant_name in ['SMOTE', 'ADASYN', 'BorderlineSMOTE', 'SMOTETomek', 'SMOTEENN']:\n",
    "                smote_class = {\n",
    "                    'SMOTE': SMOTE,\n",
    "                    'ADASYN': ADASYN,\n",
    "                    'BorderlineSMOTE': BorderlineSMOTE,\n",
    "                    'SMOTETomek': SMOTETomek,\n",
    "                    'SMOTEENN': SMOTEENN\n",
    "                }.get(variant_name, SMOTE)  # Default to SMOTE if not found\n",
    "                smote = smote_class(\n",
    "                    random_state=42,\n",
    "                    **smote_params\n",
    "                )\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown SMOTE variant '{variant_name}'. Falling back to SMOTE.\")\n",
    "                smote = SMOTE(random_state=42, **smote_params)\n",
    "\n",
    "            # Validate parameters using inspect\n",
    "            smote_signature = signature(smote.__class__)\n",
    "            valid_params = smote_signature.parameters.keys()\n",
    "            invalid_params = set(smote_params.keys()) - set(valid_params)\n",
    "            if invalid_params:\n",
    "                self.logger.warning(f\"Invalid parameters for SMOTE variant '{variant_name}': {invalid_params}. These will be ignored.\")\n",
    "\n",
    "        except TypeError as e:\n",
    "            self.logger.error(f\"Error initializing SMOTE variant '{variant_name}': {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error during SMOTE initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Validate that all features are numerical before SMOTENC\n",
    "        if variant_name == 'SMOTENC':\n",
    "            # All features should be numerical after encoding\n",
    "            if not np.all([np.issubdtype(dtype, np.number) for dtype in X_train.dtypes]):\n",
    "                non_numeric_cols = X_train.columns[~X_train.dtypes.apply(np.issubdtype, args=(np.number,))]\n",
    "                self.logger.error(f\"SMOTENC requires all features to be numerical. Non-numeric columns found: {list(non_numeric_cols)}\")\n",
    "                raise ValueError(f\"SMOTENC requires all features to be numerical. Non-numeric columns found: {list(non_numeric_cols)}\")\n",
    "\n",
    "        # Apply SMOTE variant\n",
    "        try:\n",
    "            self.logger.debug(\"Applying SMOTE variant...\")\n",
    "            X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "            self.smote = smote\n",
    "            self.preprocessing_steps.append(step_name)\n",
    "            self.logger.info(f\"Applied SMOTE Variant '{variant_name}'. Resampled X_train shape: {X_res.shape}, y_train shape: {y_res.shape}\")\n",
    "            self.logger.debug(f\"Class Distribution after SMOTE: {y_res.value_counts().to_dict()}\")\n",
    "            return X_res, y_res\n",
    "        except ValueError as ve:\n",
    "            self.logger.error(f\"ValueError during SMOTE application: {ve}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error during SMOTE application: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(\"Starting inverse transformation.\")\n",
    "\n",
    "        # Initialize dictionaries to hold inverse-transformed data\n",
    "        inverse_data = {}\n",
    "\n",
    "        # Initialize index tracker\n",
    "        start_idx = 0\n",
    "\n",
    "        # Iterate through each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                continue  # Skip any remainder features\n",
    "            # Extract the transformed data for current transformer\n",
    "            if name == 'num':\n",
    "                end_idx = start_idx + len(features)\n",
    "                numerical_data = X_transformed[:, start_idx:end_idx]\n",
    "                numerical_inverse = transformer.named_steps['scaler'].inverse_transform(numerical_data)\n",
    "                inverse_data.update({feature: numerical_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                logger.debug(f\"Numerical features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name == 'ord':\n",
    "                ordinal_encoder = transformer.named_steps.get('ordinal_encoder', None)\n",
    "                if ordinal_encoder:\n",
    "                    if hasattr(ordinal_encoder, 'categories_'):\n",
    "                        end_idx = start_idx + len(features)\n",
    "                        ordinal_data = X_transformed[:, start_idx:end_idx]\n",
    "                        ordinal_inverse = ordinal_encoder.inverse_transform(ordinal_data)\n",
    "                        inverse_data.update({feature: ordinal_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                        logger.debug(f\"Ordinal features {features} inverse transformed.\")\n",
    "                        start_idx = end_idx\n",
    "                    else:\n",
    "                        logger.error(f\"OrdinalEncoder for features {features} is NOT fitted.\")\n",
    "                        raise AttributeError(f\"OrdinalEncoder for features {features} is NOT fitted.\")\n",
    "                else:\n",
    "                    logger.error(f\"'ordinal_encoder' not found in transformer '{name}'.\")\n",
    "                    raise AttributeError(f\"'ordinal_encoder' not found in transformer '{name}'.\")\n",
    "            elif name.startswith('onehot_enc_'):\n",
    "                # For OneHotEncoder, need to inverse transform multiple columns\n",
    "                transformer_steps = transformer.named_steps\n",
    "                onehot_encoder = transformer_steps['onehot_encoder']\n",
    "                # Get number of categories for this feature\n",
    "                n_categories = len(onehot_encoder.categories_[0])\n",
    "                end_idx = start_idx + n_categories\n",
    "                nominal_data = X_transformed[:, start_idx:end_idx]\n",
    "                nominal_inverse = onehot_encoder.inverse_transform(nominal_data)\n",
    "                inverse_data.update({feature: nominal_inverse[:, 0] for feature in features})\n",
    "                logger.debug(f\"Nominal features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            else:\n",
    "                logger.warning(f\"Unknown transformer '{name}'. Skipping inversion.\")\n",
    "\n",
    "        # Create the inverse-transformed DataFrame\n",
    "        inverse_df = pd.DataFrame(inverse_data)\n",
    "\n",
    "        logger.debug(\"Inverse-transformed DataFrame constructed.\")\n",
    "        logger.debug(f\"Inverse-transformed DataFrame shape: {inverse_df.shape}\")\n",
    "\n",
    "        logger.info(\"✅ Inverse transformation completed successfully.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "\n",
    "\n",
    "    def build_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        transformers = []\n",
    "\n",
    "        if self.numericals:\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())  # Include scaler here\n",
    "            ])\n",
    "            transformers.append(('num', numerical_transformer, self.numericals))\n",
    "            self.logger.debug(\"Numerical transformer (Imputer + Scaler) added to pipeline.\")\n",
    "\n",
    "        if self.ordinal_categoricals:\n",
    "            ordinal_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinal_encoder', OrdinalEncoder())\n",
    "            ])\n",
    "            transformers.append(('ord', ordinal_transformer, self.ordinal_categoricals))\n",
    "            self.logger.debug(\"Ordinal transformer added to pipeline.\")\n",
    "\n",
    "        if self.nominal_categoricals:\n",
    "            for feature in self.nominal_categoricals:\n",
    "                transformer = Pipeline(steps=[\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "                ])\n",
    "                transformers.append((f'onehot_enc_{feature}', transformer, [feature]))\n",
    "                self.logger.debug(f\"Nominal transformer for '{feature}' added to pipeline.\")\n",
    "\n",
    "        if not transformers:\n",
    "            self.logger.error(\"No transformers to add to the pipeline. Check feature categorization.\")\n",
    "            raise ValueError(\"No transformers to add to the pipeline. Check feature categorization.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "        self.logger.debug(\"ColumnTransformer constructed with the following transformers:\")\n",
    "        for t in transformers:\n",
    "            self.logger.debug(t)\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC\n",
    "        categorical_indices = []\n",
    "        current_index = 0\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name.startswith('onehot_enc_'):\n",
    "                ohe = transformer.named_steps['onehot_encoder']\n",
    "                n_categories = len(ohe.categories_[0])\n",
    "                categorical_indices.extend(list(range(current_index, current_index + n_categories)))\n",
    "                self.logger.debug(f\"Feature '{name}' has {n_categories} categories; indices {list(range(current_index, current_index + n_categories))}.\")\n",
    "                current_index += n_categories\n",
    "            elif name in ['num', 'ord']:\n",
    "                n_features = len(features)\n",
    "                current_index += n_features\n",
    "                self.logger.debug(f\"Feature '{name}' has {n_features} features; advancing start index by {n_features}.\")\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transformer '{name}'. Skipping index calculation.\")\n",
    "\n",
    "        self.categorical_indices = categorical_indices\n",
    "        self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        # Validate that encoders are fitted\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'ord':\n",
    "                ordinal_encoder = transformer.named_steps.get('ordinal_encoder', None)\n",
    "                if ordinal_encoder and hasattr(ordinal_encoder, 'categories_'):\n",
    "                    self.logger.debug(f\"✅ OrdinalEncoder for features {features} is fitted.\")\n",
    "                else:\n",
    "                    self.logger.error(f\"❌ OrdinalEncoder for features {features} is NOT fitted.\")\n",
    "                    raise AttributeError(f\"OrdinalEncoder for features {features} is NOT fitted.\")\n",
    "        return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_train(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess data for training mode.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "                - X_train: Transformed training features.\n",
    "                - X_test: Transformed test features.\n",
    "                - y_train: Training target.\n",
    "                - y_test: Test target.\n",
    "                - recommendations: Preprocessing recommendations.\n",
    "                - X_test_inverse: Inverse-transformed test features (optional).\n",
    "        \"\"\"\n",
    "        # Step 1: Split Dataset\n",
    "        X_train_original, X_test_original, y_train_original, y_test = self.split_dataset(X, y)\n",
    "\n",
    "        # Debugging: Log shapes after splitting\n",
    "        self.logger.debug(f\"After splitting: X_train_original.shape={X_train_original.shape}, X_test_original.shape={X_test_original.shape}\")\n",
    "\n",
    "        # Step 2: Handle Missing Values\n",
    "        X_train_missing_values, X_test_missing_values = self.handle_missing_values(X_train_original, X_test_original)\n",
    "\n",
    "        # Debugging: Log shapes after handling missing values\n",
    "        self.logger.debug(f\"After handling missing values: X_train_missing_values.shape={X_train_missing_values.shape}, X_test_missing_values.shape={X_test_missing_values.shape}\")\n",
    "\n",
    "        # Step 3: Test for Normality\n",
    "        if self.model_category in ['regression', 'classification', 'clustering']:\n",
    "            self.test_normality(X_train_missing_values)\n",
    "\n",
    "        # Step 4: Handle Outliers\n",
    "        X_train_outliers_handled, y_train_outliers_handled = self.handle_outliers(X_train_missing_values, y_train_original)\n",
    "\n",
    "        # Debugging: Log shapes after handling outliers\n",
    "        self.logger.debug(f\"After handling outliers: X_train_outliers_handled.shape={X_train_outliers_handled.shape}\")\n",
    "\n",
    "        # Retain a copy of X_test without outliers for reference\n",
    "        X_test_outliers_handled = X_test_missing_values.copy() if X_test_missing_values is not None else None\n",
    "\n",
    "        # Step 5: Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Step 6: Build and Fit the Pipeline (Modified)\n",
    "        self.pipeline = self.build_pipeline(X_train_outliers_handled)\n",
    "\n",
    "        # Fit and transform training data using the pipeline\n",
    "        X_train_preprocessed = self.pipeline.transform(X_train_outliers_handled)  # Already fitted in build_pipeline\n",
    "\n",
    "        # Transform test data\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test_outliers_handled) if X_test_outliers_handled is not None else None\n",
    "        if X_test_preprocessed is not None:\n",
    "            self.logger.debug(f\"After pipeline transform: X_test_preprocessed.shape={X_test_preprocessed.shape}\")\n",
    "\n",
    "        self.logger.info(\"✅ Training and test data preprocessed.\")\n",
    "\n",
    "        # Step 7: Implement SMOTENC (Train Only for Classification)\n",
    "        if self.model_category == 'classification':\n",
    "            smotenc = SMOTENC(\n",
    "                categorical_features=self.categorical_indices,  # Now set correctly\n",
    "                sampling_strategy='auto',\n",
    "                random_state=42,\n",
    "                k_neighbors=5\n",
    "            )\n",
    "            X_train_smoted, y_train_smoted = smotenc.fit_resample(X_train_preprocessed, y_train_outliers_handled)\n",
    "            self.smote = smotenc\n",
    "            self.logger.info(\"✅ SMOTENC applied to training data.\")\n",
    "            self.logger.debug(f\"After SMOTENC: X_train_smoted.shape={X_train_smoted.shape}, y_train_smoted.shape={y_train_smoted.shape}\")\n",
    "\n",
    "            # **Validation:** Ensure SMOTENC did not alter the test set\n",
    "            if X_test_preprocessed is not None:\n",
    "                expected_test_shape = (X_test_original.shape[0], X_test_preprocessed.shape[1])\n",
    "                actual_test_shape = X_test_preprocessed.shape\n",
    "                if actual_test_shape != expected_test_shape:\n",
    "                    self.logger.error(f\"❌ Test set shape mismatch: Expected {expected_test_shape}, Got {actual_test_shape}\")\n",
    "                    raise ValueError(f\"Test set shape mismatch: Expected {expected_test_shape}, Got {actual_test_shape}\")\n",
    "        else:\n",
    "            X_train_smoted, y_train_smoted = X_train_preprocessed, y_train_outliers_handled\n",
    "            self.logger.info(\"⚠️ SMOTENC not applied: Not a classification model.\")\n",
    "\n",
    "        # Step 8: Save Transformers (Including the Pipeline)\n",
    "        self.final_feature_order = list(self.pipeline.get_feature_names_out())\n",
    "        X_train_final = pd.DataFrame(X_train_smoted, columns=self.final_feature_order)  # Removed index assignment\n",
    "        X_test_final = pd.DataFrame(X_test_preprocessed, columns=self.final_feature_order, index=X_test_original.index) if X_test_preprocessed is not None else None\n",
    "\n",
    "        self.logger.debug(f\"Final Training DataFrame shape: {X_train_final.shape}\")\n",
    "        if X_test_final is not None:\n",
    "            self.logger.debug(f\"Final Test DataFrame shape: {X_test_final.shape}\")\n",
    "\n",
    "        self.save_transformers()\n",
    "\n",
    "        # Confirm Indices Before Inverse Transform (Debugging Step)\n",
    "        self._log(\"Indices in X_test_final after transformations:\", \"preprocess_train\", 'debug')\n",
    "        if X_test_final is not None:\n",
    "            self._log(X_test_final.index, \"preprocess_train\", 'debug')\n",
    "            self._log(\"Indices in X_test_original:\", \"preprocess_train\", 'debug')\n",
    "            self._log(X_test_original.index, \"preprocess_train\", 'debug')\n",
    "\n",
    "        # Inverse transformations (optional, for interpretability)\n",
    "        try:\n",
    "            # Use the final test dataset (fully transformed) for inverse transformations\n",
    "            if X_test_final is not None:\n",
    "                X_test_inverse = self.inverse_transform_data(X_test_final.values)\n",
    "                self.logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "                self.logger.debug(f\"Inverse-transformed X_test_inverse.shape={X_test_inverse.shape}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "            X_test_inverse = None\n",
    "\n",
    "        # Return processed datasets\n",
    "        return X_train_final, X_test_final, y_train_smoted, y_test, recommendations, X_test_inverse\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Call fit_transform first.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Call fit_transform first.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def preprocess_predict(self, X: pd.DataFrame, transformers: dict) -> Tuple[np.ndarray, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Preprocess data for prediction mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data for prediction.\n",
    "            transformers (dict): Loaded transformers from save_transformers.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "                - X_processed: Preprocessed features.\n",
    "                - recommendations: Preprocessing recommendations.\n",
    "                - X_inverse: Inverse-transformed features (optional).\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Predict\"\n",
    "        self.logger.info(f\"Starting preprocessing in '{self.mode}' mode.\")\n",
    "        \n",
    "        # Load transformers\n",
    "        self.load_transformers()\n",
    "\n",
    "        # Apply the pipeline to the new data\n",
    "        try:\n",
    "            X_processed = self.pipeline.transform(X)\n",
    "            self.logger.info(\"✅ New data preprocessed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Preprocessing failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Generate recommendations if needed (optional)\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Inverse transform for interpretability\n",
    "        try:\n",
    "            X_inverse = self.inverse_transform_data(X_processed)\n",
    "            self.logger.info(\"✅ Inverse transformation applied successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "            X_inverse = None  # Proceed without inverse transform if it fails\n",
    "\n",
    "        return X_processed, recommendations, X_inverse\n",
    "\n",
    "    def preprocess_clustering(self, X: pd.DataFrame, debug: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Preprocess data for clustering mode.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Input features for clustering.\n",
    "            debug (bool): Flag to control debug outputs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: X_processed, recommendations.\n",
    "        \"\"\"\n",
    "        step_name = \"Preprocess Clustering\"\n",
    "        self.logger.info(f\"Step: {step_name}\")\n",
    "\n",
    "        if debug:\n",
    "            self.logger.debug(\"Starting preprocessing for clustering.\")\n",
    "\n",
    "        # Handle Missing Values\n",
    "        X_missing, _ = self.handle_missing_values(X, None, debug=debug)\n",
    "\n",
    "        # Handle Outliers\n",
    "        X_outliers_handled, _ = self.handle_outliers(X_missing, None, debug=debug)\n",
    "\n",
    "        # Generate Preprocessing Recommendations\n",
    "        recommendations = self.generate_recommendations()\n",
    "\n",
    "        # Build and Fit the Pipeline\n",
    "        self.pipeline = self.build_pipeline(X_outliers_handled, debug=debug)\n",
    "\n",
    "        # Transform the data\n",
    "        X_processed = self.pipeline.transform(X_outliers_handled)\n",
    "        if debug:\n",
    "            self.logger.debug(f\"After pipeline transform: X_processed.shape={X_processed.shape}\")\n",
    "\n",
    "        self.logger.info(\"✅ Clustering data preprocessed successfully.\")\n",
    "\n",
    "        return X_processed, recommendations\n",
    "\n",
    "\n",
    "    def final_preprocessing(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> Tuple:\n",
    "        \"\"\"\n",
    "        Execute the full preprocessing pipeline based on the mode.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): Input dataset containing features and possibly the target variable.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Depending on mode:\n",
    "                - 'train': X_train, X_test, y_train, y_test, recommendations, X_test_inverse\n",
    "                - 'predict': X_processed, recommendations, X_inverse\n",
    "                - 'clustering': X_processed, recommendations\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting: Final Preprocessing Pipeline in '{self.mode}' mode.\")\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # Ensure y_variable is present in the data\n",
    "            if not all(col in data.columns for col in self.y_variable):\n",
    "                missing_y = [col for col in self.y_variable if col not in data.columns]\n",
    "                raise ValueError(f\"Target variable(s) {missing_y} not found in the dataset.\")\n",
    "\n",
    "            # Separate X and y\n",
    "            X = data.drop(self.y_variable, axis=1)\n",
    "            y = data[self.y_variable].iloc[:, 0] if len(self.y_variable) == 1 else data[self.y_variable]\n",
    "\n",
    "            if y is None:\n",
    "                raise ValueError(\"Target variable 'y' must be provided in train mode.\")\n",
    "            return self.preprocess_train(X, y)\n",
    "\n",
    "        elif self.mode == 'predict':\n",
    "            # Predict mode: Use all data as X; y is not required\n",
    "            X = data.copy()\n",
    "            \n",
    "            # Load transformers explicitly\n",
    "            transformers = self.load_transformers()\n",
    "            \n",
    "            return self.preprocess_predict(X, transformers)\n",
    "\n",
    "        elif self.mode == 'clustering':\n",
    "            # Clustering mode: Use all data as X; y is not used\n",
    "            X = data.copy()\n",
    "            return self.preprocess_clustering(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Mode '{self.mode}' is not implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, implement a method to display column info for debugging\n",
    "    def _debug_column_info(self, df: pd.DataFrame, step: str = \"Debug Column Info\"):\n",
    "        \"\"\"\n",
    "        Display information about DataFrame columns for debugging purposes.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to inspect.\n",
    "            step (str, optional): Description of the current step. Defaults to \"Debug Column Info\".\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"\\n📊 {step}: Column Information\")\n",
    "        for col in df.columns:\n",
    "            self.logger.debug(f\"Column '{col}': {df[col].dtype}, Unique Values: {df[col].nunique()}\")\n",
    "        self.logger.debug(\"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "\n",
    "# Assuming DataPreprocessor is defined/imported correctly\n",
    "# from data_preprocessor import DataPreprocessor\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and parse the YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the preprocessor_config.yaml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed configuration.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = '../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml'  # Path to your preprocessor_config.yaml\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "        logger_config = config.get('logging', {})\n",
    "        logger_level = logger_config.get('level', 'INFO').upper()\n",
    "        logger_format = logger_config.get('format', '%(asctime)s [%(levelname)s] %(message)s')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Configure Logging\n",
    "    # ----------------------------\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else getattr(logging, logger_level, logging.INFO),\n",
    "        format=logger_format,\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('main_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Extract Feature Assets\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Extract Execution Parameters for Training\n",
    "    # ----------------------------\n",
    "    execution_train = config.get('execution', {}).get('train', {})\n",
    "    train_mode = 'train'\n",
    "\n",
    "    train_input_path = execution_train.get('input_path', '')\n",
    "    output_dir = execution_train.get('output_dir', './processed_data')\n",
    "    transformers_dir = execution_train.get('save_transformers_path', './transformers')\n",
    "    normalize_debug = execution_train.get('normalize_debug', False)\n",
    "    normalize_graphs_output = execution_train.get('normalize_graphs_output', False)\n",
    "\n",
    "    # Validate essential paths\n",
    "    if not train_input_path:\n",
    "        logger.error(\"❌ 'input_path' for training mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(train_input_path):\n",
    "        logger.error(f\"❌ Training input dataset not found at {train_input_path}.\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Extract Model Configurations\n",
    "    # ----------------------------\n",
    "    model_name = config.get('model_type', 'Tree Based Classifier')  # Fetch 'model_type' from config\n",
    "    model_config = config.get('models', {}).get(model_name, {})\n",
    "    if not model_config:\n",
    "        logger.error(f\"❌ Model configuration for '{model_name}' not found.\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Initialize DataPreprocessor\n",
    "    # ----------------------------\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=model_name,  # Use the actual model name\n",
    "        column_assets=column_assets,\n",
    "        mode=train_mode,\n",
    "        options=model_config,\n",
    "        debug=debug_flag,\n",
    "        normalize_debug=normalize_debug,\n",
    "        normalize_graphs_output=normalize_graphs_output,\n",
    "        graphs_output_dir=config.get('execution', {}).get('shared', {}).get('plot_output_dir', './plots'),\n",
    "        transformers_dir=transformers_dir\n",
    "    )\n",
    "    # ----------------------------\n",
    "    # Step 3: Initialize FeatureManager\n",
    "    # ----------------------------\n",
    "    save_path = config.get('execution', {}).get('features_metadata_path', '../../ml-preprocessing-utils/data/dataset/test/features_info/features_metadata.pkl')\n",
    "    feature_manager = FeatureManager(save_path=save_path)\n",
    "    # ----------------------------\n",
    "    # Step 7: Load Training Dataset\n",
    "    # ----------------------------\n",
    "    # Load features and dataset\n",
    "    try:\n",
    "        filtered_df, column_assets = feature_manager.load_features_and_dataset(\n",
    "            debug=True  # Set to False to reduce verbosity\n",
    "        )\n",
    "        logger.info(\"✅ Features loaded and dataset filtered successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load features and dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 8: Execute Preprocessing\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test, recommendations, X_test_inverse = preprocessor.final_preprocessing(filtered_df)\n",
    "        logger.info(\"✅ Preprocessing completed successfully in train mode.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing failed in train mode: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 9: Save Preprocessed Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        X_train.to_csv(os.path.join(output_dir, 'X_train.csv'), index=False)\n",
    "        y_train.to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)\n",
    "        X_test.to_csv(os.path.join(output_dir, 'X_test.csv'), index=False)\n",
    "        y_test.to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)\n",
    "        recommendations.to_csv(os.path.join(output_dir, 'preprocessing_recommendations.csv'), index=False)\n",
    "        logger.info(f\"✅ Preprocessed data saved to '{output_dir}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save preprocessed data: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Optional: Visualize Inverse Transformations\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        if X_test_inverse is not None:\n",
    "            print(\"Inverse Transformed Test Data:\")\n",
    "            print(X_test_inverse.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during visualization: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"✅ All preprocessing tasks completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    \n",
    "# predict_main.py\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Assuming DataPreprocessor is defined/imported correctly\n",
    "# from data_preprocessor import DataPreprocessor\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_config(config_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load and parse the YAML configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_path (str): Path to the preprocessor_config.yaml file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed configuration.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at {config_path}\")\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Main function for Predict Mode.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Step 1: Load Configuration\n",
    "    # ----------------------------\n",
    "    config_path = '../../ml-preprocessing-utils/data/dataset/test/preprocessor_config/preprocessor_config.yaml'  # Path to your preprocessor_config.yaml\n",
    "    try:\n",
    "        config = load_config(config_path)\n",
    "        logger_config = config.get('logging', {})\n",
    "        logger_level = logger_config.get('level', 'INFO').upper()\n",
    "        logger_format = logger_config.get('format', '%(asctime)s [%(levelname)s] %(message)s')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load configuration: {e}\")\n",
    "        return  # Exit if config loading fails\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 2: Configure Logging\n",
    "    # ----------------------------\n",
    "    debug_flag = config.get('logging', {}).get('debug', False)\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG if debug_flag else getattr(logging, logger_level, logging.INFO),\n",
    "        format=logger_format,\n",
    "        handlers=[\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('predict_preprocessing')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 3: Extract Feature Assets and Execution Parameters for Predict\n",
    "    # ----------------------------\n",
    "    features_config = config.get('features', {})\n",
    "    column_assets = {\n",
    "        'y_variable': features_config.get('y_variable', []),  # Not used in predict, but kept for consistency\n",
    "        'ordinal_categoricals': features_config.get('ordinal_categoricals', []),\n",
    "        'nominal_categoricals': features_config.get('nominal_categoricals', []),\n",
    "        'numericals': features_config.get('numericals', [])\n",
    "    }\n",
    "\n",
    "    execution_predict = config.get('execution', {}).get('predict', {})\n",
    "    predict_mode = 'predict'\n",
    "\n",
    "    prediction_input_path = execution_predict.get('prediction_input_path', '')\n",
    "    load_transformers_path = execution_predict.get('load_transformers_path', '')\n",
    "    trained_model_path = execution_predict.get('trained_model_path', '')\n",
    "    predictions_output_path = execution_predict.get('predictions_output_path', './predictions')\n",
    "    normalize_debug = execution_predict.get('normalize_debug', False)\n",
    "    normalize_graphs_output = execution_predict.get('normalize_graphs_output', False)\n",
    "\n",
    "    # Validate essential paths\n",
    "    if not prediction_input_path:\n",
    "        logger.error(\"❌ 'prediction_input_path' for predict mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(prediction_input_path):\n",
    "        logger.error(f\"❌ Prediction input dataset not found at {prediction_input_path}.\")\n",
    "        return\n",
    "    if not load_transformers_path:\n",
    "        logger.error(\"❌ 'load_transformers_path' for predict mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(load_transformers_path):\n",
    "        logger.error(f\"❌ Transformers file not found at {load_transformers_path}.\")\n",
    "        return\n",
    "    if not trained_model_path:\n",
    "        logger.error(\"❌ 'trained_model_path' for predict mode is not specified in the configuration.\")\n",
    "        return\n",
    "    if not os.path.exists(trained_model_path):\n",
    "        logger.error(f\"❌ Trained model not found at {trained_model_path}.\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 4: Initialize DataPreprocessor in Predict Mode\n",
    "    # ----------------------------\n",
    "    preprocessor = DataPreprocessor(\n",
    "        model_type=config.get('model_type', 'Tree Based Classifier'),  # Use the actual model name\n",
    "        column_assets=column_assets,\n",
    "        mode=predict_mode,\n",
    "        options=config.get('models', {}).get(config.get('model_type', 'Tree Based Classifier'), {}),\n",
    "        debug=debug_flag,\n",
    "        normalize_debug=normalize_debug,\n",
    "        normalize_graphs_output=normalize_graphs_output,\n",
    "        graphs_output_dir=config.get('execution', {}).get('shared', {}).get('plot_output_dir', './plots'),\n",
    "        transformers_dir=os.path.dirname(load_transformers_path)\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 5: Load Trained Transformers\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        transformers = preprocessor.load_transformers()\n",
    "        logger.info(\"✅ Transformers loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load transformers: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 6: Load Trained Model\n",
    "    # ----------------------------\n",
    "    # try:\n",
    "    #     model = joblib.load(trained_model_path)\n",
    "    #     logger.info(f\"✅ Trained model loaded from '{trained_model_path}'.\")\n",
    "    # except Exception as e:\n",
    "    #     logger.error(f\"❌ Failed to load trained model: {e}\")\n",
    "    #     return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 7: Load Prediction Input Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new = load_dataset(prediction_input_path)\n",
    "        logger.info(f\"✅ Prediction input data loaded from '{prediction_input_path}'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to load prediction input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 8: Preprocess the New Data\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        X_new_preprocessed = preprocessor.transform(X_new)\n",
    "        logger.info(\"✅ New data preprocessed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Preprocessing of new data failed: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        X_new_inverse = preprocessor.inverse_transform_data(X_new_preprocessed)\n",
    "        logger.info(\"✅ Inverse transformations applied successfully.\")\n",
    "        logger.info(f\"Inverse-transformed X_new_inverse.shape={X_new_inverse.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Inverse transformations failed: {e}\")\n",
    "        X_new_inverse = pd.DataFrame({'predictions': y_new_pred})  # Correct variable assignment\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 9: Make Predictions\n",
    "    # ----------------------------\n",
    "    # try:\n",
    "    #     y_new_pred = model.predict(X_new_preprocessed)\n",
    "    #     logger.info(\"✅ Predictions made successfully on new data.\")\n",
    "    # except Exception as e:\n",
    "    #     logger.error(f\"❌ Failed to make predictions: {e}\")\n",
    "    #     return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 10: Inverse Transform the Data for Interpretability\n",
    "    # ----------------------------\n",
    "    y_new_pred = np.random.choice(['1', '0'], size=X_new_preprocessed.shape[0])  # Example for binary predictions\n",
    "    \n",
    "    try:\n",
    "        if X_new_inverse is not None:\n",
    "            X_new_inverse['predictions'] = y_new_pred\n",
    "            logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "        else:\n",
    "            # If inverse transformation failed, create a DataFrame with predictions only\n",
    "            X_new_inverse = pd.DataFrame({'predictions': y_new_pred})\n",
    "            logger.warning(\"Inverse transformation was not applied. Proceeding with predictions only.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Inverse transformation failed: {e}\")\n",
    "        X_new_inverse = pd.DataFrame({'predictions': y_new_pred})  # Assign to X_new_inverse\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 11: Save Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        os.makedirs(predictions_output_path, exist_ok=True)\n",
    "        X_new_inverse.to_csv(os.path.join(predictions_output_path, 'predictions.csv'), index=False)\n",
    "        logger.info(f\"✅ Predictions saved to '{predictions_output_path}/predictions.csv'.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to save predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------\n",
    "    # Step 12: Display Predictions\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        print(\"\\nInverse Transformed Prediction DataFrame with Predictions:\")\n",
    "        print(X_new_inverse.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error during displaying predictions: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"✅ Predict mode executed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
