{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTENC, SMOTEN, SMOTE\n",
    "from typing import List, Optional, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Configure Logging\n",
    "# ----------------------------\n",
    "def configure_logging(debug: bool = False, logger_name: str = 'PreprocessingPipeline') -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure logging settings.\n",
    "\n",
    "    Args:\n",
    "        debug (bool): Flag to enable detailed debugging.\n",
    "        logger_name (str): Name of the logger.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    log_level = logging.DEBUG if debug else logging.INFO\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(log_level)\n",
    "        formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(name)s: %(message)s')\n",
    "        # File Handler\n",
    "        fh = logging.FileHandler(f\"{logger_name.lower()}.log\")\n",
    "        fh.setLevel(log_level)\n",
    "        fh.setFormatter(formatter)\n",
    "        # Stream Handler\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setLevel(log_level)\n",
    "        sh.setFormatter(formatter)\n",
    "        # Add Handlers\n",
    "        logger.addHandler(fh)\n",
    "        logger.addHandler(sh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Define the PreprocessingPipeline Class\n",
    "# ----------------------------\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        numericals: List[str],\n",
    "        ordinal_categoricals: List[str],\n",
    "        nominal_categoricals: List[str],\n",
    "        y_variable: str,\n",
    "        smote_params: Optional[Dict] = None,\n",
    "        debug: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PreprocessingPipeline with specified columns.\n",
    "\n",
    "        Args:\n",
    "            numericals (List[str]): List of numerical feature names.\n",
    "            ordinal_categoricals (List[str]): List of ordinal categorical feature names.\n",
    "            nominal_categoricals (List[str]): List of nominal categorical feature names.\n",
    "            y_variable (str): Name of the target variable.\n",
    "            smote_params (Dict, optional): Parameters for the SMOTE technique.\n",
    "            debug (bool): Flag to enable detailed debugging.\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self.logger = configure_logging(debug=self.debug, logger_name='PreprocessingPipeline')\n",
    "        self.pipeline = None\n",
    "        self.categorical_indices = []\n",
    "        self.numericals = numericals\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.y_variable = y_variable\n",
    "        self.smote_params = smote_params or {}\n",
    "        self.logger.debug(f\"Initialized PreprocessingPipeline with numericals: {self.numericals}, \"\n",
    "                          f\"ordinal_categoricals: {self.ordinal_categoricals}, \"\n",
    "                          f\"nominal_categoricals: {self.nominal_categoricals}, \"\n",
    "                          f\"y_variable: {self.y_variable}\")\n",
    "\n",
    "    def categorize_features(self, df: pd.DataFrame, include_target: bool = True) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Ensure that the DataFrame contains only the specified columns.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "            include_target (bool): Whether to include the target variable in the check.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[str]]: Dictionary categorizing the features.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting feature categorization based on provided column lists.\")\n",
    "\n",
    "        # Determine expected columns based on context\n",
    "        if include_target:\n",
    "            expected_columns = self.numericals + self.ordinal_categoricals + self.nominal_categoricals + [self.y_variable]\n",
    "        else:\n",
    "            expected_columns = self.numericals + self.ordinal_categoricals + self.nominal_categoricals\n",
    "\n",
    "        missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            self.logger.error(f\"The following specified columns are missing in the DataFrame: {missing_columns}\")\n",
    "            raise ValueError(f\"The following specified columns are missing in the DataFrame: {missing_columns}\")\n",
    "\n",
    "        feature_types = {\n",
    "            'numerical': self.numericals,\n",
    "            'ordinal': self.ordinal_categoricals,\n",
    "            'nominal': self.nominal_categoricals\n",
    "        }\n",
    "\n",
    "        # Debug Output\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Categorized Numerical Features: {feature_types['numerical']}\")\n",
    "            self.logger.debug(f\"Categorized Ordinal Features: {feature_types['ordinal']}\")\n",
    "            self.logger.debug(f\"Categorized Nominal Features: {feature_types['nominal']}\")\n",
    "            if include_target:\n",
    "                self.logger.debug(f\"Target Variable: {self.y_variable}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Features categorized: Numerical={len(feature_types['numerical'])}, \"\n",
    "                             f\"Ordinal={len(feature_types['ordinal'])}, Nominal={len(feature_types['nominal'])}.\")\n",
    "\n",
    "        return feature_types\n",
    "\n",
    "    def create_preprocessing_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        \"\"\"\n",
    "        Create and fit the preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training features.\n",
    "\n",
    "        Returns:\n",
    "            ColumnTransformer: Fitted preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Creating preprocessing pipeline with specified transformers.\")\n",
    "\n",
    "        feature_types = self.categorize_features(X_train, include_target=False)\n",
    "        \n",
    "        # Confirm the number of features\n",
    "        total_features = len(feature_types['numerical']) + len(feature_types['ordinal']) + len(feature_types['nominal'])\n",
    "        self.logger.debug(f\"Pipeline will be fitted on {total_features} features: {feature_types}\")\n",
    "\n",
    "        transformers = []\n",
    "\n",
    "        # Numerical Transformer\n",
    "        if feature_types['numerical']:\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "            transformers.append(('num', numerical_transformer, feature_types['numerical']))\n",
    "            self.logger.debug(\"Numerical transformer added to pipeline.\")\n",
    "        else:\n",
    "            self.logger.debug(\"No numerical features to transform.\")\n",
    "\n",
    "        # Ordinal Categorical Transformer\n",
    "        if feature_types['ordinal']:\n",
    "            ordinal_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('ordinal_encoder', OrdinalEncoder())\n",
    "            ])\n",
    "            transformers.append(('ord', ordinal_transformer, feature_types['ordinal']))\n",
    "            self.logger.debug(\"Ordinal categorical transformer added to pipeline.\")\n",
    "        else:\n",
    "            self.logger.debug(\"No ordinal categorical features to transform.\")\n",
    "\n",
    "        # Nominal Categorical Transformers\n",
    "        for feature in feature_types['nominal']:\n",
    "            transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ])\n",
    "            transformers.append((f'onehot_enc_{feature}', transformer, [feature]))\n",
    "            self.logger.debug(f\"Nominal categorical transformer for '{feature}' added to pipeline.\")\n",
    "\n",
    "        # Validate that at least one transformer is present\n",
    "        if not transformers:\n",
    "            self.logger.error(\"No transformers added to the pipeline. Please check the feature lists.\")\n",
    "            raise ValueError(\"No transformers added to the pipeline. Please check the feature lists.\")\n",
    "\n",
    "        # Combine Transformers\n",
    "        preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessing pipeline created and fitted on training data.\")\n",
    "\n",
    "        # Determine categorical feature indices for SMOTENC\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        self.logger.debug(f\"Feature names after preprocessing: {feature_names}\")\n",
    "\n",
    "        categorical_indices = []\n",
    "        start_idx = 0\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name.startswith('onehot_enc_'):\n",
    "                ohe = transformer.named_steps['onehot_encoder']\n",
    "                n_categories = len(ohe.categories_[0])\n",
    "                categorical_indices.extend(list(range(start_idx, start_idx + n_categories)))\n",
    "                self.logger.debug(f\"OneHotEncoder for '{features[0]}' has {n_categories} categories; indices {list(range(start_idx, start_idx + n_categories))}.\")\n",
    "                start_idx += n_categories\n",
    "            elif name in ['num', 'ord']:\n",
    "                n_features = len(features)\n",
    "                self.logger.debug(f\"Transformer '{name}' processes {n_features} features; advancing start index by {n_features}.\")\n",
    "                start_idx += n_features\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transformer '{name}'. Skipping index calculation.\")\n",
    "\n",
    "        self.categorical_indices = categorical_indices\n",
    "        self.logger.debug(f\"Categorical feature indices for SMOTENC: {self.categorical_indices}\")\n",
    "\n",
    "        self.pipeline = preprocessor\n",
    "        return preprocessor\n",
    "\n",
    "    def select_smote_technique(self) -> Any:\n",
    "        \"\"\"\n",
    "        Select the appropriate SMOTE technique based on feature types.\n",
    "\n",
    "        Returns:\n",
    "            An instance of SMOTE, SMOTENC, or SMOTEN.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Selecting SMOTE technique based on feature types.\")\n",
    "\n",
    "        has_numerical = len(self.numericals) > 0\n",
    "        has_categorical = len(self.ordinal_categoricals) + len(self.nominal_categoricals) > 0\n",
    "\n",
    "        if has_numerical and has_categorical:\n",
    "            self.logger.debug(\"Both numerical and categorical features present. Using SMOTENC.\")\n",
    "            smote = SMOTENC(categorical_features=self.categorical_indices, **self.smote_params)\n",
    "            selected_smote = 'SMOTENC'\n",
    "        elif has_categorical:\n",
    "            self.logger.debug(\"Only categorical features present. Using SMOTEN.\")\n",
    "            smote = SMOTEN(**self.smote_params)\n",
    "            selected_smote = 'SMOTEN'\n",
    "        elif has_numerical:\n",
    "            self.logger.debug(\"Only numerical features present. Using SMOTE.\")\n",
    "            smote = SMOTE(**self.smote_params)\n",
    "            selected_smote = 'SMOTE'\n",
    "        else:\n",
    "            self.logger.error(\"No features available for SMOTE.\")\n",
    "            raise ValueError(\"At least one feature type must be present for SMOTE.\")\n",
    "\n",
    "        self.logger.info(f\"✅ Selected SMOTE Technique: {selected_smote}\")\n",
    "        return smote\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fit the preprocessing pipeline, apply SMOTE, and split the data.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The entire dataset including features and target.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Contains preprocessed training and testing data.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting fit_transform process.\")\n",
    "        feature_types = self.categorize_features(df, include_target=True)\n",
    "\n",
    "        # Separate features and target\n",
    "        X = df.drop(columns=[self.y_variable])\n",
    "        y = df[self.y_variable]\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=0.25,\n",
    "            random_state=42,\n",
    "            stratify=y\n",
    "        )\n",
    "        self.logger.debug(f\"Data split into training ({X_train.shape}) and testing ({X_test.shape}) sets.\")\n",
    "\n",
    "        # Debug Output\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Training set shape: {X_train.shape}\")\n",
    "            self.logger.debug(f\"Testing set shape: {X_test.shape}\")\n",
    "            self.logger.debug(f\"Training target distribution: {Counter(y_train)}\")\n",
    "        else:\n",
    "            self.logger.info(\"✅ Data split into training and testing sets.\")\n",
    "\n",
    "        # Create and fit preprocessing pipeline\n",
    "        self.create_preprocessing_pipeline(X_train)\n",
    "\n",
    "        # Transform training data\n",
    "        X_train_preprocessed = self.pipeline.transform(X_train)\n",
    "        self.logger.info(\"✅ Training data preprocessed.\")\n",
    "\n",
    "        # Apply SMOTE\n",
    "        smote = self.select_smote_technique()\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "        self.logger.info(\"✅ SMOTE applied to training data.\")\n",
    "\n",
    "        # Debug Output\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Resampled training set shape: {X_train_resampled.shape}\")\n",
    "            self.logger.debug(f\"Resampled training target distribution: {Counter(y_train_resampled)}\")\n",
    "        else:\n",
    "            self.logger.info(\"✅ Training data resampled.\")\n",
    "\n",
    "        # Transform testing data\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test)\n",
    "        self.logger.info(\"✅ Testing data preprocessed.\")\n",
    "\n",
    "        # Inverse transform testing data\n",
    "        X_test_inverse = self.inverse_transform_data(X_test_preprocessed)\n",
    "        self.logger.info(\"✅ Inverse transformation applied to testing data.\")\n",
    "\n",
    "        # Prepare outputs\n",
    "        outputs = {\n",
    "            'X_train_preprocessed': X_train_resampled,\n",
    "            'X_test_preprocessed': X_test_preprocessed,\n",
    "            'y_train': y_train_resampled,\n",
    "            'y_test': y_test,\n",
    "            'X_test_inverse': X_test_inverse,\n",
    "            'selected_smote': smote.__class__.__name__\n",
    "        }\n",
    "\n",
    "        # Debug Output\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Outputs: {list(outputs.keys())}\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def transform_new_data(self, df_new: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Transform new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            df_new (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Contains preprocessed and inverse-transformed new data.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting transformation of new data for prediction.\")\n",
    "        feature_types = self.categorize_features(df_new, include_target=False)\n",
    "\n",
    "        # Preprocess new data\n",
    "        X_preprocessed = self.pipeline.transform(df_new)\n",
    "        self.logger.info(\"✅ New data preprocessed.\")\n",
    "\n",
    "        # Inverse transform\n",
    "        X_inversed = self.inverse_transform_data(X_preprocessed)\n",
    "        self.logger.info(\"✅ Inverse transformation applied to new data.\")\n",
    "\n",
    "        # Generate recommendations (if applicable)\n",
    "        recommendations = []\n",
    "\n",
    "        outputs = {\n",
    "            'X_preprocessed': X_preprocessed,\n",
    "            'recommendations': recommendations,  # Ensure this method exists and returns relevant data\n",
    "            'X_inversed': X_inversed\n",
    "        }\n",
    "\n",
    "        # Debug Output\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Outputs: {list(outputs.keys())}\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting inverse transformation.\")\n",
    "        preprocessor = self.pipeline\n",
    "        inverse_data = {}\n",
    "        start_idx = 0\n",
    "\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                continue  # Skip remainder\n",
    "\n",
    "            if not features:\n",
    "                self.logger.debug(f\"Transformer '{name}' has no features. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            if name == 'num':\n",
    "                end_idx = start_idx + len(features)\n",
    "                numerical_data = X_transformed[:, start_idx:end_idx]\n",
    "                numerical_inverse = transformer.named_steps['scaler'].inverse_transform(numerical_data)\n",
    "                for idx, feature in enumerate(features):\n",
    "                    inverse_data[feature] = numerical_inverse[:, idx]\n",
    "                self.logger.debug(f\"Numerical features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name == 'ord':\n",
    "                end_idx = start_idx + len(features)\n",
    "                ordinal_data = X_transformed[:, start_idx:end_idx]\n",
    "                ordinal_inverse = transformer.named_steps['ordinal_encoder'].inverse_transform(ordinal_data)\n",
    "                for idx, feature in enumerate(features):\n",
    "                    inverse_data[feature] = ordinal_inverse[:, idx]\n",
    "                self.logger.debug(f\"Ordinal features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name.startswith('onehot_enc_'):\n",
    "                ohe = transformer.named_steps['onehot_encoder']\n",
    "                n_categories = len(ohe.categories_[0])\n",
    "                end_idx = start_idx + n_categories\n",
    "                nominal_data = X_transformed[:, start_idx:end_idx]\n",
    "                nominal_inverse = ohe.inverse_transform(nominal_data)\n",
    "                feature = features[0]\n",
    "                inverse_data[feature] = nominal_inverse[:, 0]\n",
    "                self.logger.debug(f\"Nominal feature '{feature}' inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transformer '{name}'. Skipping inverse transformation.\")\n",
    "\n",
    "        inverse_df = pd.DataFrame(inverse_data)\n",
    "        self.logger.debug(\"Inverse-transformed DataFrame constructed.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "    def save_pipeline(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save the fitted preprocessing pipeline to disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to save the pipeline.\n",
    "        \"\"\"\n",
    "        joblib.dump(self.pipeline, filepath)\n",
    "        self.logger.info(f\"✅ Preprocessing pipeline saved to '{filepath}'.\")\n",
    "\n",
    "    def load_pipeline(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load a fitted preprocessing pipeline from disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to load the pipeline from.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            self.logger.error(f\"Preprocessing pipeline file '{filepath}' does not exist.\")\n",
    "            raise FileNotFoundError(f\"Preprocessing pipeline file '{filepath}' does not exist.\")\n",
    "        self.pipeline = joblib.load(filepath)\n",
    "        self.logger.info(f\"✅ Preprocessing pipeline loaded from '{filepath}'.\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Define the TrainingModule Class\n",
    "# ----------------------------\n",
    "class TrainingModule:\n",
    "    def __init__(self, model_type: str = 'LogisticRegression', model_params: Optional[Dict] = None, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the TrainingModule with the specified model.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the classifier ('LogisticRegression', 'RandomForestClassifier', etc.).\n",
    "            model_params (dict, optional): Parameters for the classifier.\n",
    "            debug (bool): Flag to enable detailed debugging.\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self.logger = configure_logging(debug=self.debug, logger_name='TrainingModule')\n",
    "        self.model_type = model_type\n",
    "        self.model_params = model_params or {}\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the classifier based on the specified type and parameters.\n",
    "\n",
    "        Returns:\n",
    "            classifier: An instance of the specified classifier.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model_type == 'LogisticRegression':\n",
    "                model = LogisticRegression(**self.model_params)\n",
    "            elif self.model_type == 'RandomForestClassifier':\n",
    "                model = RandomForestClassifier(**self.model_params)\n",
    "            elif self.model_type == 'GradientBoostingClassifier':\n",
    "                model = GradientBoostingClassifier(**self.model_params)\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported model type: {self.model_type}\")\n",
    "                raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
    "            self.logger.debug(f\"Initialized {self.model_type} with parameters: {self.model_params}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: pd.Series):\n",
    "        \"\"\"\n",
    "        Train the classifier on the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Preprocessed training features.\n",
    "            y_train (pd.Series): Training target variable.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting model training.\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.logger.info(\"✅ Model trained successfully.\")\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test data.\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray): Preprocessed test features.\n",
    "            y_test (pd.Series): Test target variable.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting model evaluation.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = cross_val_score(self.model, X_test, y_test, scoring='precision', cv=5).mean()\n",
    "        recall = cross_val_score(self.model, X_test, y_test, scoring='recall', cv=5).mean()\n",
    "        f1 = cross_val_score(self.model, X_test, y_test, scoring='f1', cv=5).mean()\n",
    "        roc_auc = cross_val_score(self.model, X_test, y_test, scoring='roc_auc', cv=5).mean()\n",
    "\n",
    "        # Log performance metrics\n",
    "        self.logger.info(f\"✅ Model Accuracy on Test Set: {accuracy:.2f}\")\n",
    "        self.logger.info(f\"✅ Model Precision: {precision:.2f}\")\n",
    "        self.logger.info(f\"✅ Model Recall: {recall:.2f}\")\n",
    "        self.logger.info(f\"✅ Model F1-Score: {f1:.2f}\")\n",
    "        self.logger.info(f\"✅ Model ROC-AUC: {roc_auc:.2f}\")\n",
    "\n",
    "        # Classification Report\n",
    "        self.logger.debug(\"Classification Report:\")\n",
    "        self.logger.debug(f\"\\n{classification_report(y_test, y_pred, zero_division=0)}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Store performance metrics\n",
    "        performance_metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "\n",
    "        # Save performance metrics\n",
    "        metrics_filename = f'performance_metrics_{self.model_type}.csv'\n",
    "        performance_df = pd.DataFrame([performance_metrics])\n",
    "        performance_df.to_csv(metrics_filename, index=False)\n",
    "        self.logger.info(f\"✅ Performance metrics saved to '{metrics_filename}'.\")\n",
    "\n",
    "        return performance_metrics\n",
    "\n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save the trained model to disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        joblib.dump(self.model, filepath)\n",
    "        self.logger.info(f\"✅ Model saved to '{filepath}'.\")\n",
    "\n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load a trained model from disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to load the model from.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            self.logger.error(f\"Model file '{filepath}' does not exist.\")\n",
    "            raise FileNotFoundError(f\"Model file '{filepath}' does not exist.\")\n",
    "        self.model = joblib.load(filepath)\n",
    "        self.logger.info(f\"✅ Model loaded from '{filepath}'.\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Preprocessed feature data.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted labels.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Making predictions.\")\n",
    "        predictions = self.model.predict(X)\n",
    "        self.logger.info(\"✅ Predictions made successfully.\")\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Define Utility Functions\n",
    "# ----------------------------\n",
    "def construct_filepath(base_name: str, dataset_key: str, extension: str = '.joblib') -> str:\n",
    "    \"\"\"\n",
    "    Construct a standardized filepath based on base name, dataset key, and extension.\n",
    "\n",
    "    Args:\n",
    "        base_name (str): Base name of the file (e.g., 'trained_model').\n",
    "        dataset_key (str): Identifier for the dataset.\n",
    "        extension (str): File extension (default is '.joblib').\n",
    "\n",
    "    Returns:\n",
    "        str: Constructed filepath.\n",
    "    \"\"\"\n",
    "    return f\"{base_name}_{dataset_key}{extension}\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Define the Main Execution Logic\n",
    "# ----------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate training and prediction using PreprocessingPipeline and TrainingModule.\n",
    "    Processes three datasets: numerical-only, categorical-only, and mixed.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Define Column Lists\n",
    "    # ----------------------------\n",
    "    # For demonstration, we define column lists manually.\n",
    "    # In practice, these can be dynamically determined or passed as arguments.\n",
    "    column_definitions = {\n",
    "        'numerical_only': {\n",
    "            'numericals': ['age', 'income'],\n",
    "            'ordinal_categoricals': [],\n",
    "            'nominal_categoricals': [],\n",
    "            'y_variable': 'salary'\n",
    "        },\n",
    "        'categorical_only': {\n",
    "            'numericals': [],\n",
    "            'ordinal_categoricals': ['education_level', 'experience'],\n",
    "            'nominal_categoricals': ['gender', 'city', 'department'],\n",
    "            'y_variable': 'salary'\n",
    "        },\n",
    "        'mixed': {\n",
    "            'numericals': ['age', 'income'],\n",
    "            'ordinal_categoricals': ['education_level', 'experience'],\n",
    "            'nominal_categoricals': ['gender', 'city', 'department'],\n",
    "            'y_variable': 'salary'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # Define Debug Flag\n",
    "    # ----------------------------\n",
    "    # Set debug=True for detailed outputs, debug=False for minimal outputs\n",
    "    debug = True  # Change to False for production\n",
    "\n",
    "    # ----------------------------\n",
    "    # Define Dataset Keys\n",
    "    # ----------------------------\n",
    "    dataset_keys = ['numerical_only', 'categorical_only', 'mixed']\n",
    "\n",
    "    # ----------------------------\n",
    "    # Iterate Through Each Dataset for Training\n",
    "    # ----------------------------\n",
    "    for dataset_key in dataset_keys:\n",
    "        print(f\"\\n--- Training on Dataset: {dataset_key} ---\")\n",
    "        cols = column_definitions[dataset_key]\n",
    "\n",
    "        # Initialize PreprocessingPipeline with dynamic columns\n",
    "        pipeline = PreprocessingPipeline(\n",
    "            numericals=cols['numericals'],\n",
    "            ordinal_categoricals=cols['ordinal_categoricals'],\n",
    "            nominal_categoricals=cols['nominal_categoricals'],\n",
    "            y_variable=cols['y_variable'],\n",
    "            smote_params={},  # Add any specific SMOTE parameters if needed\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # Data Generation\n",
    "        # ----------------------------\n",
    "        additional_rows = 300\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "\n",
    "        if dataset_key == 'numerical_only':\n",
    "            expanded_data = {\n",
    "                'age': np.random.randint(20, 60, additional_rows),\n",
    "                'income': np.random.randint(30000, 100000, additional_rows),\n",
    "                'salary': np.random.choice([0, 1], additional_rows, p=[0.7, 0.3])  # Imbalanced target\n",
    "            }\n",
    "        elif dataset_key == 'categorical_only':\n",
    "            expanded_data = {\n",
    "                'gender': np.random.choice(['Male', 'Female'], additional_rows),\n",
    "                'city': np.random.choice(['New York', 'Chicago', 'Los Angeles', 'Houston'], additional_rows),\n",
    "                'department': np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing', 'Finance'], additional_rows),\n",
    "                'education_level': np.random.choice(['Bachelors', 'Masters', 'PhD'], additional_rows),\n",
    "                'experience': np.random.choice(['Junior', 'Mid', 'Senior'], additional_rows),\n",
    "                'salary': np.random.choice([0, 1], additional_rows, p=[0.7, 0.3])  # Imbalanced target\n",
    "            }\n",
    "        elif dataset_key == 'mixed':\n",
    "            expanded_data = {\n",
    "                'age': np.random.randint(20, 60, additional_rows),\n",
    "                'income': np.random.randint(30000, 100000, additional_rows),\n",
    "                'gender': np.random.choice(['Male', 'Female'], additional_rows),\n",
    "                'city': np.random.choice(['New York', 'Chicago', 'Los Angeles', 'Houston'], additional_rows),\n",
    "                'department': np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing', 'Finance'], additional_rows),\n",
    "                'education_level': np.random.choice(['Bachelors', 'Masters', 'PhD'], additional_rows),\n",
    "                'experience': np.random.choice(['Junior', 'Mid', 'Senior'], additional_rows),\n",
    "                'salary': np.random.choice([0, 1], additional_rows, p=[0.7, 0.3])  # Imbalanced target\n",
    "            }\n",
    "        else:\n",
    "            print(f\"❌ Unknown dataset key: {dataset_key}\")\n",
    "            continue\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(expanded_data)\n",
    "        pipeline.logger.info(f\"✅ DataFrame for dataset '{dataset_key}' with {additional_rows} rows created.\")\n",
    "\n",
    "        # ----------------------------\n",
    "        # Training Phase\n",
    "        # ----------------------------\n",
    "        preprocessed_data = pipeline.fit_transform(df)\n",
    "\n",
    "        X_train_preprocessed = preprocessed_data['X_train_preprocessed']\n",
    "        X_test_preprocessed = preprocessed_data['X_test_preprocessed']\n",
    "        y_train = preprocessed_data['y_train']\n",
    "        y_test = preprocessed_data['y_test']\n",
    "        X_test_inverse = preprocessed_data['X_test_inverse']\n",
    "        selected_smote = preprocessed_data['selected_smote']\n",
    "\n",
    "        # Save the inverse-transformed test set\n",
    "        inverse_filename = f'X_test_inverse_{dataset_key}.csv'\n",
    "        X_test_inverse.to_csv(inverse_filename, index=False)\n",
    "        pipeline.logger.info(f\"✅ Inverse-transformed test set saved as '{inverse_filename}'.\")\n",
    "\n",
    "        # Initialize TrainingModule\n",
    "        training_module = TrainingModule(\n",
    "            model_type='LogisticRegression',\n",
    "            model_params={'max_iter': 1000},\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        training_module.train(X_train_preprocessed, y_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        performance_metrics = training_module.evaluate(X_test_preprocessed, y_test)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_filepath = construct_filepath('trained_model', dataset_key)\n",
    "        training_module.save_model(model_filepath)\n",
    "\n",
    "        # Save the preprocessing pipeline\n",
    "        preprocessor_filepath = construct_filepath('preprocessor', dataset_key)\n",
    "        pipeline.save_pipeline(preprocessor_filepath)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Summary of Training Phase\n",
    "        # ----------------------------\n",
    "        print(f\"Selected SMOTE Technique for '{dataset_key}': {selected_smote}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Prediction Phase\n",
    "    # ----------------------------\n",
    "    print(\"\\n--- Prediction Phase ---\")\n",
    "    prediction_dataset_key = 'mixed'  # Choose the dataset for prediction\n",
    "\n",
    "    # Retrieve column definitions\n",
    "    cols = column_definitions[prediction_dataset_key]\n",
    "\n",
    "    # Initialize PreprocessingPipeline for prediction\n",
    "    pipeline = PreprocessingPipeline(\n",
    "        numericals=cols['numericals'],\n",
    "        ordinal_categoricals=cols['ordinal_categoricals'],\n",
    "        nominal_categoricals=cols['nominal_categoricals'],\n",
    "        y_variable=cols['y_variable'],\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Define the path to the saved preprocessing pipeline\n",
    "    preprocessor_filepath = construct_filepath('preprocessor', prediction_dataset_key)\n",
    "\n",
    "    # Load the preprocessing pipeline\n",
    "    try:\n",
    "        pipeline.load_pipeline(preprocessor_filepath)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize TrainingModule\n",
    "    training_module = TrainingModule(\n",
    "        model_type='LogisticRegression',\n",
    "        model_params={'max_iter': 1000},\n",
    "        debug=debug\n",
    "    )\n",
    "\n",
    "    # Define the path to the trained model\n",
    "    model_filepath = construct_filepath('trained_model', prediction_dataset_key)\n",
    "\n",
    "    # Load the trained model\n",
    "    try:\n",
    "        training_module.load_model(model_filepath)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ {e}\")\n",
    "        return\n",
    "\n",
    "    # Example Prediction Data\n",
    "    prediction_data = {\n",
    "        'age': [28, 40],\n",
    "        'income': [50000, 80000],\n",
    "        'gender': ['Female', 'Male'],\n",
    "        'city': ['Chicago', 'Houston'],\n",
    "        'department': ['Engineering', 'Sales'],\n",
    "        'education_level': ['Masters', 'Bachelors'],\n",
    "        'experience': ['Mid', 'Senior']\n",
    "    }\n",
    "\n",
    "    X_new = pd.DataFrame(prediction_data)\n",
    "    pipeline.logger.info(\"✅ New prediction data created.\")\n",
    "\n",
    "    # Preprocess the new data\n",
    "    preprocessed_new_data = pipeline.transform_new_data(X_new)\n",
    "\n",
    "    X_new_preprocessed = preprocessed_new_data['X_preprocessed']\n",
    "    X_new_inversed = preprocessed_new_data['X_inversed']\n",
    "\n",
    "    # Make predictions\n",
    "    y_new_pred = training_module.predict(X_new_preprocessed)\n",
    "\n",
    "    # Attach predictions to inverse-transformed data\n",
    "    X_new_inversed['predictions'] = y_new_pred\n",
    "    pipeline.logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "\n",
    "    # Save the inverse-transformed prediction data\n",
    "    prediction_inverse_filename = f'X_inverse_prediction_{prediction_dataset_key}.csv'\n",
    "    X_new_inversed.to_csv(prediction_inverse_filename, index=False)\n",
    "    pipeline.logger.info(f\"✅ Inverse-transformed prediction data saved as '{prediction_inverse_filename}'.\")\n",
    "\n",
    "    # Display the inverse-transformed prediction data\n",
    "    print(\"\\nInverse Transformed Prediction DataFrame with Predictions:\")\n",
    "    print(X_new_inversed)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
