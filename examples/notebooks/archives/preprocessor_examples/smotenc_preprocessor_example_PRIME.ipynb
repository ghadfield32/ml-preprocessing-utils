{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import yaml\n",
    "from typing import List, Optional, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Configure Logging\n",
    "# ----------------------------\n",
    "def configure_logging(debug: bool = False) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure logging settings.\n",
    "\n",
    "    Args:\n",
    "        debug (bool): Flag to enable detailed debugging.\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    log_level = logging.DEBUG if debug else logging.INFO\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"preprocessing_pipeline.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logger = logging.getLogger('PreprocessingPipeline')\n",
    "    return logger\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Define the PreprocessingPipeline Class\n",
    "# ----------------------------\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_path: Optional[str] = None,\n",
    "        target_variable: Optional[str] = None,\n",
    "        numerical_features: Optional[List[str]] = None,\n",
    "        ordinal_features: Optional[List[str]] = None,\n",
    "        nominal_features: Optional[List[str]] = None,\n",
    "        debug: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PreprocessingPipeline with optional debugging and configuration.\n",
    "\n",
    "        Args:\n",
    "            config_path (str, optional): Path to the YAML configuration file.\n",
    "            target_variable (str, optional): Name of the target variable.\n",
    "            numerical_features (List[str], optional): List of numerical feature names.\n",
    "            ordinal_features (List[str], optional): List of ordinal feature names.\n",
    "            nominal_features (List[str], optional): List of nominal feature names.\n",
    "            debug (bool): Flag to enable detailed debugging.\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self.logger = configure_logging(debug=self.debug)\n",
    "        self.pipeline = None\n",
    "        self.categorical_indices = []\n",
    "        self.target_variable = target_variable\n",
    "        self.numerical_features = numerical_features or []\n",
    "        self.ordinal_features = ordinal_features or []\n",
    "        self.nominal_features = nominal_features or []\n",
    "\n",
    "        if config_path:\n",
    "            try:\n",
    "                with open(config_path, 'r') as file:\n",
    "                    config = yaml.safe_load(file)\n",
    "                self.target_variable = config.get('target_variable', self.target_variable)\n",
    "                self.numerical_features = config.get('numerical_features', self.numerical_features)\n",
    "                self.ordinal_features = config.get('ordinal_features', self.ordinal_features)\n",
    "                self.nominal_features = config.get('nominal_features', self.nominal_features)\n",
    "                self.debug = config.get('debug', self.debug)\n",
    "                self.logger = configure_logging(debug=self.debug)\n",
    "                self.logger.debug(f\"Pipeline initialized with configuration from {config_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to load configuration file: {e}\")\n",
    "                raise e\n",
    "        else:\n",
    "            if not self.target_variable:\n",
    "                self.logger.error(\"No target variable provided and no configuration file specified.\")\n",
    "                raise ValueError(\"Target variable must be provided if no configuration file is used.\")\n",
    "            if not (self.numerical_features or self.ordinal_features or self.nominal_features):\n",
    "                self.logger.warning(\"No feature lists provided. The pipeline will attempt to categorize features automatically.\")\n",
    "            self.logger.debug(\"Pipeline initialized without configuration file.\")\n",
    "\n",
    "    def categorize_features(self, df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Categorize features into numerical, ordinal, and nominal based on provided lists or data types.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[str]]: Dictionary with keys 'numerical', 'ordinal', 'nominal' and lists of feature names.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting feature categorization.\")\n",
    "        feature_types = {'numerical': [], 'ordinal': [], 'nominal': []}\n",
    "\n",
    "        # Use provided numerical_features, or categorize automatically\n",
    "        if self.numerical_features:\n",
    "            feature_types['numerical'] = self.numerical_features\n",
    "            self.logger.debug(f\"Using provided numerical features: {self.numerical_features}\")\n",
    "        else:\n",
    "            numerical = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col != self.target_variable]\n",
    "            feature_types['numerical'] = numerical\n",
    "            self.logger.debug(f\"Automatically categorized numerical features: {numerical}\")\n",
    "\n",
    "        # Use provided ordinal_features\n",
    "        if self.ordinal_features:\n",
    "            feature_types['ordinal'] = self.ordinal_features\n",
    "            self.logger.debug(f\"Using provided ordinal features: {self.ordinal_features}\")\n",
    "\n",
    "        # Use provided nominal_features, or categorize automatically\n",
    "        if self.nominal_features:\n",
    "            feature_types['nominal'] = self.nominal_features\n",
    "            self.logger.debug(f\"Using provided nominal features: {self.nominal_features}\")\n",
    "        else:\n",
    "            # Nominal features are those not in numerical or ordinal\n",
    "            nominal = [col for col in df.columns if col not in feature_types['numerical'] + feature_types['ordinal'] + [self.target_variable]]\n",
    "            feature_types['nominal'] = nominal\n",
    "            self.logger.debug(f\"Automatically categorized nominal features: {nominal}\")\n",
    "\n",
    "        self.numerical_features = feature_types['numerical']\n",
    "        self.ordinal_features = feature_types['ordinal']\n",
    "        self.nominal_features = feature_types['nominal']\n",
    "\n",
    "        # Log the categorized features\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Categorized Numerical Features: {self.numerical_features}\")\n",
    "            self.logger.debug(f\"Categorized Ordinal Features: {self.ordinal_features}\")\n",
    "            self.logger.debug(f\"Categorized Nominal Features: {self.nominal_features}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Features categorized: Numerical={len(self.numerical_features)}, \"\n",
    "                             f\"Ordinal={len(self.ordinal_features)}, Nominal={len(self.nominal_features)}.\")\n",
    "\n",
    "        return feature_types\n",
    "\n",
    "    def create_preprocessing_pipeline(self, X_train: pd.DataFrame) -> ColumnTransformer:\n",
    "        \"\"\"\n",
    "        Create a preprocessing pipeline compatible with SMOTENC.\n",
    "\n",
    "        Args:\n",
    "            X_train (pd.DataFrame): Training feature set for fitting transformers.\n",
    "\n",
    "        Returns:\n",
    "            ColumnTransformer: A ColumnTransformer object with numerical, ordinal, and nominal transformers.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Creating preprocessing pipeline with numerical, ordinal, and nominal transformers.\")\n",
    "\n",
    "        # Numerical Transformer\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        self.logger.debug(\"Numerical transformer created.\")\n",
    "\n",
    "        # Ordinal Categorical Transformer\n",
    "        ordinal_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('ordinal_encoder', OrdinalEncoder())\n",
    "        ])\n",
    "        self.logger.debug(\"Ordinal transformer created.\")\n",
    "\n",
    "        # Nominal Categorical Transformers: Use OneHotEncoder\n",
    "        nominal_transformers = []\n",
    "        for feature in self.nominal_features:\n",
    "            transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot_encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ])\n",
    "            nominal_transformers.append((f'onehot_enc_{feature}', transformer, [feature]))\n",
    "            self.logger.debug(f\"Nominal transformer with OneHotEncoder for '{feature}' created.\")\n",
    "\n",
    "        # Combine Transformers using ColumnTransformer\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num', numerical_transformer, self.numerical_features),\n",
    "            ('ord', ordinal_transformer, self.ordinal_features),\n",
    "            *nominal_transformers  # Unpack nominal transformers\n",
    "        ], remainder='drop')\n",
    "\n",
    "        self.logger.debug(\"ColumnTransformer created with numerical, ordinal, and nominal transformers.\")\n",
    "\n",
    "        # Fit the preprocessor to determine feature indices\n",
    "        preprocessor.fit(X_train)\n",
    "        self.logger.info(\"✅ Preprocessor fitted on training data.\")\n",
    "\n",
    "        # Determine the number of output features after preprocessing\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        self.logger.debug(f\"Feature names after preprocessing: {feature_names}\")\n",
    "\n",
    "        # Identify indices of all categorical features (from OneHotEncoder)\n",
    "        categorical_indices = []\n",
    "        start_idx = 0\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name.startswith('onehot_enc_'):\n",
    "                ohe = transformer.named_steps['onehot_encoder']\n",
    "                n_categories = len(ohe.categories_[0])\n",
    "                categorical_indices.extend(list(range(start_idx, start_idx + n_categories)))\n",
    "                self.logger.debug(f\"Feature '{name}' has {n_categories} categories; indices {list(range(start_idx, start_idx + n_categories))}.\")\n",
    "                start_idx += n_categories\n",
    "            elif name in ['num', 'ord']:\n",
    "                n_features = len(features)\n",
    "                self.logger.debug(f\"Feature '{name}' has {n_features} features; advancing start index by {n_features}.\")\n",
    "                start_idx += n_features\n",
    "            else:\n",
    "                self.logger.warning(f\"Unknown transformer '{name}'. Skipping index calculation.\")\n",
    "\n",
    "        self.logger.debug(f\"Categorical feature indices for SMOTENC: {categorical_indices}\")\n",
    "        self.categorical_indices = categorical_indices\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Fit the preprocessing pipeline, apply SMOTENC, and transform the data.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The entire dataset including features and target.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, any]: Dictionary containing preprocessed training and testing data, and inverse-transformed test data.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting fit_transform on the entire dataset.\")\n",
    "        # Categorize features\n",
    "        feature_types = self.categorize_features(df)\n",
    "\n",
    "        # Split into features and target\n",
    "        X = df.drop(self.target_variable, axis=1)\n",
    "        y = df[self.target_variable]\n",
    "\n",
    "        self.logger.debug(\"Split data into features and target.\")\n",
    "\n",
    "        # Split into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=0.25,\n",
    "            random_state=42,\n",
    "            stratify=y  # Ensure proportional representation of classes\n",
    "        )\n",
    "        self.logger.info(\"✅ Data split into training and testing sets successfully.\")\n",
    "\n",
    "        # Log class distribution before SMOTENC\n",
    "        class_distribution_before = Counter(y_train)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Class distribution before SMOTENC: {class_distribution_before}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Class distribution before SMOTENC: {class_distribution_before}\")\n",
    "\n",
    "        # Fit and transform the training data\n",
    "        self.pipeline = self.create_preprocessing_pipeline(X_train)\n",
    "        X_train_preprocessed = self.pipeline.transform(X_train)\n",
    "        self.logger.info(\"✅ Training data preprocessed.\")\n",
    "\n",
    "        # Initialize SMOTENC\n",
    "        smotenc = SMOTENC(\n",
    "            categorical_features=self.categorical_indices,\n",
    "            sampling_strategy='auto',\n",
    "            random_state=42,\n",
    "            k_neighbors=5\n",
    "        )\n",
    "        self.logger.debug(\"SMOTENC initialized.\")\n",
    "\n",
    "        # Apply SMOTENC to the preprocessed training data\n",
    "        X_train_resampled, y_train_resampled = smotenc.fit_resample(X_train_preprocessed, y_train)\n",
    "        self.logger.info(\"✅ SMOTENC applied to training data.\")\n",
    "\n",
    "        # Log class distribution after SMOTENC\n",
    "        class_distribution_after = Counter(y_train_resampled)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Class distribution after SMOTENC: {class_distribution_after}\")\n",
    "        else:\n",
    "            self.logger.info(f\"Class distribution after SMOTENC: {class_distribution_after}\")\n",
    "\n",
    "        # Transform the test data\n",
    "        X_test_preprocessed = self.pipeline.transform(X_test)\n",
    "        self.logger.info(\"✅ Test data preprocessed.\")\n",
    "\n",
    "        # Inverse transform the test set for interpretability\n",
    "        X_test_inverse = self.inverse_transform_data(\n",
    "            X_transformed=X_test_preprocessed\n",
    "        )\n",
    "        self.logger.info(\"✅ Inverse transformation applied to test data.\")\n",
    "\n",
    "        # Preserve the original indexing\n",
    "        X_test_inverse.index = X_test.index\n",
    "        self.logger.debug(\"Index preserved for inverse-transformed test data.\")\n",
    "\n",
    "        return {\n",
    "            'X_train_preprocessed': X_train_resampled,\n",
    "            'X_test_preprocessed': X_test_preprocessed,\n",
    "            'y_train': y_train_resampled,\n",
    "            'y_test': y_test,\n",
    "            'X_test_inverse': X_test_inverse\n",
    "        }\n",
    "\n",
    "    def transform_new_data(self, df_new: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data for prediction.\n",
    "\n",
    "        Args:\n",
    "            df_new (pd.DataFrame): New data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data ready for prediction.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting transformation of new data for prediction.\")\n",
    "\n",
    "        # Transform the new data\n",
    "        X_new_preprocessed = self.transform(df_new)\n",
    "        self.logger.info(\"✅ New data transformed successfully.\")\n",
    "\n",
    "        return X_new_preprocessed\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform new data using the fitted preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Call fit_transform first.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Call fit_transform first.\")\n",
    "        self.logger.debug(\"Transforming new data.\")\n",
    "        X_preprocessed = self.pipeline.transform(X)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Transformed data shape: {X_preprocessed.shape}\")\n",
    "        else:\n",
    "            self.logger.info(\"Data transformed.\")\n",
    "        return X_preprocessed\n",
    "\n",
    "    def inverse_transform_data(self, X_transformed: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform inverse transformation on the transformed data to reconstruct original feature values.\n",
    "\n",
    "        Args:\n",
    "            X_transformed (np.ndarray): The transformed feature data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The inverse-transformed DataFrame.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self.logger.error(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "            raise AttributeError(\"Preprocessing pipeline has not been fitted. Cannot perform inverse transformation.\")\n",
    "\n",
    "        preprocessor = self.pipeline\n",
    "        logger = logging.getLogger('InverseTransform')\n",
    "        if self.debug:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.debug(\"Starting inverse transformation.\")\n",
    "\n",
    "        # Initialize dictionaries to hold inverse-transformed data\n",
    "        inverse_data = {}\n",
    "\n",
    "        # Initialize index tracker\n",
    "        start_idx = 0\n",
    "\n",
    "        # Iterate through each transformer in the ColumnTransformer\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'remainder':\n",
    "                continue  # Skip any remainder features\n",
    "            # Extract the transformed data for current transformer\n",
    "            if name == 'num':\n",
    "                end_idx = start_idx + len(features)\n",
    "                numerical_data = X_transformed[:, start_idx:end_idx]\n",
    "                numerical_inverse = transformer.named_steps['scaler'].inverse_transform(\n",
    "                    numerical_data\n",
    "                )\n",
    "                inverse_data.update({feature: numerical_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                logger.debug(f\"Numerical features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name == 'ord':\n",
    "                end_idx = start_idx + len(features)\n",
    "                ordinal_data = X_transformed[:, start_idx:end_idx]\n",
    "                ordinal_inverse = transformer.named_steps['ordinal_encoder'].inverse_transform(\n",
    "                    ordinal_data\n",
    "                )\n",
    "                inverse_data.update({feature: ordinal_inverse[:, idx] for idx, feature in enumerate(features)})\n",
    "                logger.debug(f\"Ordinal features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            elif name.startswith('onehot_enc_'):\n",
    "                # For OneHotEncoder, need to inverse transform multiple columns\n",
    "                transformer_steps = transformer.named_steps\n",
    "                onehot_encoder = transformer_steps['onehot_encoder']\n",
    "                # Get number of categories for this feature\n",
    "                n_categories = len(onehot_encoder.categories_[0])\n",
    "                end_idx = start_idx + n_categories\n",
    "                nominal_data = X_transformed[:, start_idx:end_idx]\n",
    "                nominal_inverse = onehot_encoder.inverse_transform(nominal_data)\n",
    "                inverse_data.update({feature: nominal_inverse[:, 0] for feature in features})\n",
    "                logger.debug(f\"Nominal features {features} inverse transformed.\")\n",
    "                start_idx = end_idx\n",
    "            else:\n",
    "                logger.warning(f\"Unknown transformer '{name}'. Skipping inversion.\")\n",
    "\n",
    "        # Create the inverse-transformed DataFrame\n",
    "        inverse_df = pd.DataFrame(inverse_data)\n",
    "\n",
    "        logger.debug(\"Inverse-transformed DataFrame constructed.\")\n",
    "\n",
    "        logger.info(\"✅ Inverse transformation completed successfully.\")\n",
    "\n",
    "        return inverse_df\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Define the TrainingModule Class\n",
    "# ----------------------------\n",
    "class TrainingModule:\n",
    "    def __init__(self, model_type: str = 'LogisticRegression', model_params: Optional[Dict] = None, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the TrainingModule with the specified model.\n",
    "\n",
    "        Args:\n",
    "            model_type (str): Type of the classifier ('LogisticRegression', 'RandomForestClassifier', etc.).\n",
    "            model_params (dict, optional): Parameters for the classifier.\n",
    "            debug (bool): Flag to enable detailed debugging.\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self.logger = configure_logging(debug=self.debug)\n",
    "        self.model_type = model_type\n",
    "        self.model_params = model_params or {}\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the classifier based on the specified type and parameters.\n",
    "\n",
    "        Returns:\n",
    "            classifier: An instance of the specified classifier.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.model_type == 'LogisticRegression':\n",
    "                model = LogisticRegression(**self.model_params)\n",
    "            elif self.model_type == 'RandomForestClassifier':\n",
    "                model = RandomForestClassifier(**self.model_params)\n",
    "            elif self.model_type == 'GradientBoostingClassifier':\n",
    "                model = GradientBoostingClassifier(**self.model_params)\n",
    "            else:\n",
    "                self.logger.error(f\"Unsupported model type: {self.model_type}\")\n",
    "                raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
    "            self.logger.debug(f\"Initialized {self.model_type} with parameters: {self.model_params}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: pd.Series, X_val: Optional[np.ndarray] = None, y_val: Optional[pd.Series] = None):\n",
    "        \"\"\"\n",
    "        Train the classifier on the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Preprocessed training features.\n",
    "            y_train (pd.Series): Training target variable.\n",
    "            X_val (np.ndarray, optional): Preprocessed validation features.\n",
    "            y_val (pd.Series, optional): Validation target variable.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting model training.\")\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.logger.info(\"✅ Model trained successfully.\")\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test data.\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray): Preprocessed test features.\n",
    "            y_test (pd.Series): Test target variable.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting model evaluation.\")\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = cross_val_score(self.model, X_test, y_test, scoring='precision', cv=5).mean()\n",
    "        recall = cross_val_score(self.model, X_test, y_test, scoring='recall', cv=5).mean()\n",
    "        f1 = cross_val_score(self.model, X_test, y_test, scoring='f1', cv=5).mean()\n",
    "        roc_auc = cross_val_score(self.model, X_test, y_test, scoring='roc_auc', cv=5).mean()\n",
    "\n",
    "        # Log performance metrics\n",
    "        self.logger.info(f\"✅ Model Accuracy on Test Set: {accuracy:.2f}\")\n",
    "        self.logger.info(f\"✅ Model Precision: {precision:.2f}\")\n",
    "        self.logger.info(f\"✅ Model Recall: {recall:.2f}\")\n",
    "        self.logger.info(f\"✅ Model F1-Score: {f1:.2f}\")\n",
    "        self.logger.info(f\"✅ Model ROC-AUC: {roc_auc:.2f}\")\n",
    "\n",
    "        # Print classification report\n",
    "        self.logger.debug(\"Classification Report:\")\n",
    "        self.logger.debug(f\"\\n{classification_report(y_test, y_pred, zero_division=0)}\")\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "        # Store performance metrics\n",
    "        performance_metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "\n",
    "        # Optionally, save performance metrics to a file for monitoring\n",
    "        performance_df = pd.DataFrame([performance_metrics])\n",
    "        performance_df.to_csv('performance_metrics.csv', index=False)\n",
    "        self.logger.info(\"✅ Performance metrics saved to 'performance_metrics.csv'.\")\n",
    "\n",
    "        return performance_metrics\n",
    "\n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Save the trained model to disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        joblib.dump(self.model, filepath)\n",
    "        self.logger.info(f\"✅ Model saved to '{filepath}'.\")\n",
    "\n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Load a trained model from disk.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to load the model from.\n",
    "        \"\"\"\n",
    "        self.model = joblib.load(filepath)\n",
    "        self.logger.info(f\"✅ Model loaded from '{filepath}'.\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Preprocessed feature data.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Predicted labels.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Making predictions.\")\n",
    "        predictions = self.model.predict(X)\n",
    "        self.logger.info(\"✅ Predictions made successfully.\")\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Define the Main Execution Logic\n",
    "# ----------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate preprocessing and training using the PreprocessingPipeline and TrainingModule.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Configuration and Initialization\n",
    "    # ----------------------------\n",
    "    # Path to configuration file (if any)\n",
    "    config_path = 'config.yaml'  # Ensure this file exists or set to None\n",
    "\n",
    "    # Initialize the PreprocessingPipeline\n",
    "    pipeline = PreprocessingPipeline(\n",
    "        config_path=config_path,\n",
    "        target_variable='salary',  # Specify the target variable\n",
    "        ordinal_features=['education_level', 'experience'],  # Specify ordinal features if known\n",
    "        debug=True  # Set to False for concise logs\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # Data Ingestion\n",
    "    # ----------------------------\n",
    "\n",
    "    # Expanding the dataset with more rows\n",
    "    additional_rows = 300\n",
    "\n",
    "    # Generate new rows\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    expanded_data = {\n",
    "        'age': np.random.randint(20, 60, additional_rows),\n",
    "        'salary': np.random.choice([0, 1], additional_rows),\n",
    "        'education_level': np.random.choice(['Bachelors', 'Masters', 'PhD'], additional_rows),\n",
    "        'experience': np.random.choice(['Junior', 'Mid', 'Senior'], additional_rows),\n",
    "        'gender': np.random.choice(['Male', 'Female'], additional_rows),\n",
    "        'city': np.random.choice(['New York', 'Chicago', 'Los Angeles', 'Houston'], additional_rows),\n",
    "        'department': np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing', 'Finance'], additional_rows),\n",
    "    }\n",
    "\n",
    "    # Create new DataFrame and append to existing data\n",
    "    df = pd.DataFrame(expanded_data)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Data Preprocessing\n",
    "    # ----------------------------\n",
    "    # Fit and transform the entire dataset\n",
    "    preprocessed_data = pipeline.fit_transform(df)\n",
    "\n",
    "    X_train_preprocessed = preprocessed_data['X_train_preprocessed']\n",
    "    X_test_preprocessed = preprocessed_data['X_test_preprocessed']\n",
    "    y_train = preprocessed_data['y_train']\n",
    "    y_test = preprocessed_data['y_test']\n",
    "    X_test_inverse = preprocessed_data['X_test_inverse']\n",
    "\n",
    "    # Save the inverse-transformed test set for interpretability\n",
    "    X_test_inverse.to_csv('X_test_inverse.csv')\n",
    "    pipeline.logger.info(\"✅ Inverse-transformed test set saved as 'X_test_inverse.csv'.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Initialize and Train the Model\n",
    "    # ----------------------------\n",
    "    # Initialize the TrainingModule\n",
    "    training_module = TrainingModule(\n",
    "        model_type='LogisticRegression',\n",
    "        model_params={'max_iter': 1000},\n",
    "        debug=False  # Set to False for concise logs\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    training_module.train(X_train_preprocessed, y_train)\n",
    "    pipeline.logger.info(\"✅ Model training completed.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    performance_metrics = training_module.evaluate(X_test_preprocessed, y_test)\n",
    "\n",
    "    # Save the trained model\n",
    "    training_module.save_model('trained_model.joblib')\n",
    "\n",
    "    # ----------------------------\n",
    "    # Prediction Phase\n",
    "    # ----------------------------\n",
    "    # Load the trained model (optional if already trained)\n",
    "    loaded_model = joblib.load('trained_model.joblib')\n",
    "    pipeline.logger.info(\"✅ Trained model loaded from 'trained_model.joblib'.\")\n",
    "\n",
    "    # Example Prediction Data\n",
    "    prediction_data = {\n",
    "        'age': [28, 40],\n",
    "        'education_level': ['Masters', 'Bachelors'],\n",
    "        'experience': ['Mid', 'Senior'],\n",
    "        'gender': ['Female', 'Male'],\n",
    "        'city': ['Chicago', 'Houston'],\n",
    "        'department': ['Engineering', 'Sales']\n",
    "    }\n",
    "\n",
    "    X_new = pd.DataFrame(prediction_data)\n",
    "\n",
    "    # Preprocess the new data\n",
    "    X_new_preprocessed = pipeline.transform(X_new)\n",
    "    pipeline.logger.info(\"✅ New data preprocessed.\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_new_pred = loaded_model.predict(X_new_preprocessed)\n",
    "    pipeline.logger.info(\"✅ Predictions made on new data.\")\n",
    "\n",
    "    # Inverse transform the new data for interpretability\n",
    "    X_new_inverse = pipeline.inverse_transform_data(\n",
    "        X_transformed=X_new_preprocessed\n",
    "    )\n",
    "    X_new_inverse['predictions'] = y_new_pred\n",
    "    pipeline.logger.debug(\"Predictions attached to inverse-transformed DataFrame.\")\n",
    "\n",
    "    # Save the inverse-transformed prediction data\n",
    "    X_new_inverse.to_csv('X_inverse_prediction.csv', index=False)\n",
    "    pipeline.logger.info(\"✅ Inverse-transformed prediction data saved as 'X_inverse_prediction.csv'.\")\n",
    "\n",
    "    # Display the inverse-transformed prediction data\n",
    "    print(\"\\nInverse Transformed Prediction DataFrame with Predictions:\")\n",
    "    print(X_new_inverse)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
